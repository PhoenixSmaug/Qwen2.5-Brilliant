{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A finetune of Qwen2.5-1.5B on a Brilliant.org Community dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "We start by importing all necessary libraries. It should be noted that the finetuning library Unsloth requires a GPU with CUDA support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 08:59:34.536302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737964774.548661 2736272 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737964774.552478 2736272 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 08:59:34.565284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Super with Max-Q Design'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datasets import Dataset\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the Qwen2.5 model, which is a state-of-the-art open weight LLM. It is developed by Alibaba and the technical background can be found [here](https://qwenlm.github.io/blog/qwen2.5-math/). Since this training process will be run on a laptop, the smallest available model with 1.5B parameters is chosen. A quantization of 4bits further helps to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2 patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design. Max memory: 7.781 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-1.5B-bnb-4bit\", # unsloth/Qwen2.5-Math-1.5B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model on a simple example problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\\n\\n### Input:\\nFind the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\\n\\n### Response:\\nTo find the value of $x$ that satisfies the equation $4x+5 = 6x+7$, we can follow these steps:\\n\\n1. Subtract $4x$ from both sides of the equation to isolate the variable $x$ on one side:\\n   $4x + 5 - 4x = 6x + 7 - 4x$\\n   This simplifies to:\\n   $5 = 2x + 7$\\n\\n2. Subtract $7$ from both sides of the equation to isolate the term with $x$:\\n   $5 - 7 = 2x + 7 - 7$\\n   This simplifies to:\\n   $-2 = 2x$\\n\\n3. Divide both sides of the equation by $2$ to solve for $x$:\\n   $-2 / 2 = 2x / 2$\\n   This simplifies to:\\n   $-1 = x$\\n\\nTherefore, the value of $x$ that satisfies the equation $4x+5 = 6x+7$ is $\\\\boxed{-1}$.<|endoftext|>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    qwen_prompt.format(\n",
    "        \"Please reason step by step, and put your final answer within \\\\boxed{}.\", # instruction\n",
    "        \"Find the value of $x$ that satisfies the equation $4x+5 = 6x+7$.\", # input\n",
    "        \"\", # output\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = max_seq_length, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset of problems will be loaded. It was created with `data-mining.py`, which processes the dump of the now defunct Brilliant.org community questions. The dump was published by the VP of Brilliant [here](https://www.reddit.com/r/DataHoarder/comments/o0qrey/comment/h1zerf6/) and has a size of around 10 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many different 5 letter sequences can be m...</td>\n",
       "      <td>Number of total words - Number of ‚ÄúBAD‚Äù words:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$\\\\large{ \\\\sum _{ n=1 }^{ \\\\infty  }{ \\\\frac ...</td>\n",
       "      <td>Using the following property  :‚àën=1‚àûFnxn=xx2‚àíx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements are true and...</td>\n",
       "      <td>Pick any values for S1 and S2.\\nUse S1 to obta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A cylinder of radius $R$ and length $l$ is flo...</td>\n",
       "      <td>Thus, the final answer is \\\\boxed{-1}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$\\\\large \\\\begin{cases} x = a(t-\\\\sin t) \\\\\\\\ ...</td>\n",
       "      <td>{x=a(t‚àísin‚Å°t)‚üπdxdt=a‚àíacos‚Å°ty=a(1‚àícos‚Å°t)‚üπdydt=a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45277</th>\n",
       "      <td>$\\\\log_{3} \\\\left (1 + \\\\dfrac13 \\\\right ) + \\...</td>\n",
       "      <td>log‚Å°3(1+13)+log‚Å°3(1+14)+log‚Å°3(1+15)+‚ãØ+log‚Å°3(1+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>Calculate $1^3 + 2^3 + 3^3 + ... + 10^3$ witho...</td>\n",
       "      <td>13+23=1+8=9=321^3 + 2^3 = 1 + 8 = 9 = 3^213+23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45279</th>\n",
       "      <td>In $\\\\triangle ABC$ , if $\\\\dfrac{\\\\cos A}{\\\\s...</td>\n",
       "      <td>To solve this problem, we need 3 inequalities,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45280</th>\n",
       "      <td>$\\\\lim_{x\\\\to0} \\\\dfrac{\\\\sin5x}{2x} = \\\\, ?$</td>\n",
       "      <td>lim‚Å°x‚Üí0sin‚Å°5x2x=52.lim‚Å°5x‚Üí0sin‚Å°5x5x=52\\\\lim_{x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45281</th>\n",
       "      <td>$$   $$ $ $$ $ $$   $$   $$   $$ $ $$ $ $$   $...</td>\n",
       "      <td>Thus, the final answer is \\\\boxed{3221221112}.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45282 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0      How many different 5 letter sequences can be m...   \n",
       "1      $\\\\large{ \\\\sum _{ n=1 }^{ \\\\infty  }{ \\\\frac ...   \n",
       "2      Which of the following statements are true and...   \n",
       "3      A cylinder of radius $R$ and length $l$ is flo...   \n",
       "4      $\\\\large \\\\begin{cases} x = a(t-\\\\sin t) \\\\\\\\ ...   \n",
       "...                                                  ...   \n",
       "45277  $\\\\log_{3} \\\\left (1 + \\\\dfrac13 \\\\right ) + \\...   \n",
       "45278  Calculate $1^3 + 2^3 + 3^3 + ... + 10^3$ witho...   \n",
       "45279  In $\\\\triangle ABC$ , if $\\\\dfrac{\\\\cos A}{\\\\s...   \n",
       "45280      $\\\\lim_{x\\\\to0} \\\\dfrac{\\\\sin5x}{2x} = \\\\, ?$   \n",
       "45281  $$   $$ $ $$ $ $$   $$   $$   $$ $ $$ $ $$   $...   \n",
       "\n",
       "                                                  answer  \n",
       "0      Number of total words - Number of ‚ÄúBAD‚Äù words:...  \n",
       "1      Using the following property  :‚àën=1‚àûFnxn=xx2‚àíx...  \n",
       "2      Pick any values for S1 and S2.\\nUse S1 to obta...  \n",
       "3                 Thus, the final answer is \\\\boxed{-1}.  \n",
       "4      {x=a(t‚àísin‚Å°t)‚üπdxdt=a‚àíacos‚Å°ty=a(1‚àícos‚Å°t)‚üπdydt=a...  \n",
       "...                                                  ...  \n",
       "45277  log‚Å°3(1+13)+log‚Å°3(1+14)+log‚Å°3(1+15)+‚ãØ+log‚Å°3(1+...  \n",
       "45278  13+23=1+8=9=321^3 + 2^3 = 1 + 8 = 9 = 3^213+23...  \n",
       "45279  To solve this problem, we need 3 inequalities,...  \n",
       "45280  lim‚Å°x‚Üí0sin‚Å°5x2x=52.lim‚Å°5x‚Üí0sin‚Å°5x5x=52\\\\lim_{x...  \n",
       "45281     Thus, the final answer is \\\\boxed{3221221112}.  \n",
       "\n",
       "[45282 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('brilliant-community.csv')\n",
    "df_sample = df.sample(n=50, random_state=42)\n",
    "df_train = df.drop(df_sample.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by benchmarking the unmodified model on a sample of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate if model found correct answer\n",
    "def evaluate(answer, model):\n",
    "    # Extract the last content inside \\boxed{}\n",
    "    answer_match = re.findall(r'\\\\boxed{([^{}]*)}', answer)\n",
    "    model_match = re.findall(r'\\\\boxed{([^{}]*)}', model)\n",
    "    \n",
    "    # If no boxed content found, return False\n",
    "    if not answer_match or not model_match:\n",
    "        return False\n",
    "    \n",
    "    # Compare the last boxed content\n",
    "    return answer_match[-1].strip() == model_match[-1].strip()\n",
    "\n",
    "def benchmark(df, model):\n",
    "    total_correct = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    progress_bar = tqdm(total=len(df), desc=\"Processing Questions\")\n",
    "\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Prepare input\n",
    "        inputs = tokenizer(\n",
    "            [qwen_prompt.format(\n",
    "                \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "                row['question'],  # input\n",
    "                \"\"  # output empty since model should generate its own\n",
    "            )], return_tensors = \"pt\").to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_new_tokens = max_seq_length, use_cache = True)\n",
    "        \n",
    "        decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "        print(decoded_output)\n",
    "        \n",
    "        is_correct = evaluate(row['answer'], decoded_output)\n",
    "        \n",
    "        total_processed += 1\n",
    "        if is_correct:\n",
    "            total_correct += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Accuracy': f'{total_correct/total_processed:.2%}'\n",
    "        })\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'question': row['question'],\n",
    "            'ground_truth': row['answer'],\n",
    "            'model_output': decoded_output,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    final_accuracy = total_correct / total_processed\n",
    "    print(f\"\\nFinal Accuracy: {final_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee06870f1c64c79ba59b9153249699f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Questions:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "If $(1,x,y)$ is a geometric sequence and $(x,y,3)$ is an arithmetic sequence then find the maximum value of $x+y$ .\n",
      "\n",
      "### Response:\n",
      "To find the maximum value of $x+y$, we need to use the properties of geometric and arithmetic sequences.\n",
      "\n",
      "First, let's recall the properties of geometric sequences. In a geometric sequence, the ratio between consecutive terms is constant. Let's denote this common ratio by $r$. Then, we have:\n",
      "\n",
      "$$x = r \\cdot 1$$\n",
      "$$y = r \\cdot x$$\n",
      "\n",
      "Now, let's consider the properties of arithmetic sequences. In an arithmetic sequence, the difference between consecutive terms is constant. Let's denote this common difference by $d$. Then, we have:\n",
      "\n",
      "$$y = a + d$$\n",
      "$$3 = a + 2d$$\n",
      "\n",
      "From the above equations, we can express $x$ and $y$ in terms of $r$ and $d$:\n",
      "\n",
      "$$x = r$$\n",
      "$$y = r \\cdot x = r \\cdot r = r^2$$\n",
      "\n",
      "Now, we want to maximize the value of $x + y$. Substituting the expressions for $x$ and $y$, we get:\n",
      "\n",
      "$$x + y = r + r^2$$\n",
      "\n",
      "To find the maximum value of $x + y$, we need to find the maximum value of the expression $r + r^2$. We can do this by taking the derivative of the expression with respect to $r$ and setting it equal to zero:\n",
      "\n",
      "$$\\frac{d}{dr}(r + r^2) = 1 + 2r = 0$$\n",
      "\n",
      "Solving for $r$, we get:\n",
      "\n",
      "$$r = -\\frac{1}{2}$$\n",
      "\n",
      "Substituting this value of $r$ back into the expression for $x + y$, we get:\n",
      "\n",
      "$$x + y = -\\frac{1}{2} + \\left(-\\frac{1}{2}\\right)^2 = -\\frac{1}{2} + \\frac{1}{4} = -\\frac{1}{4}$$\n",
      "\n",
      "Therefore, the maximum value of $x + y$ is $\\boxed{-\\frac{1}{4}}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Adam, Bob, Caleb, Dylan, Elaine, Francis, Gillian, and Hamilton all decided they wanted to play Mario Kart together. They played three races, and the winner was determined by a points system:\n",
      "‚Äã For getting first place in a single race, a player was awarded 8 points.‚Äã For getting second place in a single race, a player was awarded 7 points.‚Äã For getting third place in a single race, a player was awarded 6 points.‚Äã For getting fourth place in a single race, a player was awarded 5 points.‚Äã For getting fifth place in a single race, a player was awarded 4 points.‚Äã For getting sixth place in a single race, a player was awarded 3 points.‚Äã For getting seventh place in a single race, a player was awarded 2 points.‚Äã For getting eighth place in a single race, a player was awarded 1 point.‚Äã The First Race‚Äã Francis finished directly in front of Elaine, but finished directly behind Dylan.‚Äã Bob was neither in first nor last, and did better than Adam.‚Äã Gillian came in sixth place.‚Äã Elaine did better than both Bob and Caleb, but did not do better than Hamilton. ‚ÄãDylan came in second. Adam came in last. The Second Race Hamilton did better than Dylan (Dylan was not directly behind Hamilton), but was behind Bob (not directly). Francis did better than both Bob and Elaine (in that order respectively). Gillian did better than Dylan (Dylan was directly behind Gillian), but Gillian was directly behind Caleb. Adam came in last. The Third Race Dylan and Francis finished fourth and fifth (but not necessarily in that order). Caleb did better than both Elaine and Adam. Gillian did better than Bob, but did worse than Caleb. Elaine did better than Adam. Dylan did worse than Caleb, but did better than Hamilton and Gillian. Bob came in last. Hamilton came directly behind Francis, and came directly in front of Gillian. Based on the following information, figure out where everyone placed after three races.Each racer's first initial has a number assigned to it, and that is how the question is answered A=1 B=2 C=3 D=4 E=5 F=6 G=7 H=8, so someone's answer might look like, if they thought that Adam won: 1/////// (the / just represents the numbers that would go there). If there was a tie, put the initial that came first. I.e., if both Adam and Bob had 12 points at the end, then Adam's number would go first because the letter A comes before the letter B.\n",
      "\n",
      "### Response:\n",
      "Based on the information provided, we can determine the order of the players after three races. Let's break it down step by step:\n",
      "\n",
      "1. **First Race:**\n",
      "   - **Francis:** Finished directly in front of Elaine, but finished directly behind Dylan.\n",
      "   - **Bob:** Was neither in first nor last, and did better than Adam.\n",
      "   - **Gillian:** Came in sixth place.\n",
      "   - **Elaine:** Did better than both Bob and Caleb, but did not do better than Hamilton.\n",
      "   - **Dylan:** Finished second.\n",
      "   - **Adam:** Finished last.\n",
      "   - **Caleb:** Did better than both Elaine and Adam.\n",
      "   - **Hamilton:** Did better than Dylan, but was behind Bob.\n",
      "\n",
      "2. **Second Race:**\n",
      "   - **Hamilton:** Did better than Dylan, but was behind Bob.\n",
      "   - **Francis:** Did better than both Bob and Elaine, but did not do better than Hamilton.\n",
      "   - **Gillian:** Did better than Dylan, but did worse than Caleb.\n",
      "   - **Elaine:** Did better than both Bob and Caleb, but did not do better than Hamilton.\n",
      "   - **Dylan:** Finished fourth.\n",
      "   - **Adam:** Finished fifth.\n",
      "   - **Bob:** Was neither in first nor last, and did better than Adam.\n",
      "   - **Caleb:** Did better than both Elaine and Adam.\n",
      "   - **Gillian:** Did better than Bob, but did worse than Caleb.\n",
      "   - **Elaine:** Did better than Adam.\n",
      "\n",
      "3. **Third Race:**\n",
      "   - **Dylan:** Finished fourth.\n",
      "   - **Francis:** Finished fifth.\n",
      "   - **Caleb:** Did better than both Elaine and Adam.\n",
      "   - **Gillian:** Did better than Bob, but did worse than Caleb.\n",
      "   - **Elaine:** Did better than Adam.\n",
      "   - **Hamilton:** Did better than Dylan, but was behind Bob.\n",
      "   - **Bob:** Was neither in first nor last, and did better than Adam.\n",
      "   - **Adam:** Finished last.\n",
      "   - **Gillian:** Did better than Bob, but did worse than Caleb.\n",
      "\n",
      "After the third race, the order of the players is:\n",
      "- **Dylan:** 4th place\n",
      "- **Francis:** 5th place\n",
      "- **Caleb:** 6th place\n",
      "- **Gillian:** 7th place\n",
      "- **Elaine:** 8th place\n",
      "- **Hamilton:** 9th place\n",
      "- **Bob:** 10th place\n",
      "- **Adam:** 11th place\n",
      "\n",
      "So, the final order is:\n",
      "- **Dylan:** 4th place\n",
      "- **Francis:** 5th place\n",
      "- **Caleb:** 6th place\n",
      "- **Gillian:** 7th place\n",
      "- **Elaine:** 8th place\n",
      "- **Hamilton:** 9th place\n",
      "- **Bob:** 10th place\n",
      "- **Adam:** 11th place<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "tan 45 = ....\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to use the trigonometric identity for the tangent of a 45-degree angle. The tangent of 45 degrees is equal to 1. Therefore, the final answer is \\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A palindromic number is one which can be written the same forward as backward. Ex: 19391, 88388, 11, etc. How many 3-digit palindromic numbers exist?\n",
      "\n",
      "### Response:\n",
      "To find the number of 3-digit palindromic numbers, we need to consider the structure of a palindromic number. A 3-digit palindromic number has the form ABCBA, where A, B, and C are digits and A cannot be zero (since it is a 3-digit number).\n",
      "\n",
      "Let's break down the problem step by step:\n",
      "\n",
      "1. **Determine the range of A**: Since A cannot be zero, A can take any value from 1 to 9 (9 possible choices).\n",
      "2. **Determine the range of B**: B can take any value from 0 to 9 (10 possible choices).\n",
      "3. **Determine the range of C**: C can also take any value from 0 to 9 (10 possible choices).\n",
      "\n",
      "Now, let's calculate the total number of 3-digit palindromic numbers:\n",
      "\n",
      "- There are 9 choices for A.\n",
      "- There are 10 choices for B.\n",
      "- There are 10 choices for C.\n",
      "\n",
      "The total number of 3-digit palindromic numbers is the product of these choices:\n",
      "\n",
      "\\[ 9 \\times 10 \\times 10 = 900 \\]\n",
      "\n",
      "Therefore, there are \\(\\boxed{900}\\) 3-digit palindromic numbers.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\Large \\\\left( \\\\frac{81^{\\\\frac{1}{\\\\log_{5}9}}+3^{\\\\frac{3}{\\\\log_{\\\\sqrt{6}}3}}}{409}\\\\right) \\\\left((\\\\sqrt{7})^{\\\\frac{2}{\\\\log_{25}7}}-(125)^{\\\\log_{25}6}\\\\right)= \\\\, ?$\n",
      "\n",
      "### Response:\n",
      "To solve this expression, we need to simplify each term inside the parentheses separately. Let's start with the first term:\n",
      "\n",
      "1. Simplify $81^{\\frac{1}{\\log_{5}9}}$:\n",
      "   - We know that $81 = 3^4$, so $81^{\\frac{1}{\\log_{5}9}} = (3^4)^{\\frac{1}{\\log_{5}9}}$.\n",
      "   - Using the property of exponents, we get $3^{\\frac{4}{\\log_{5}9}}$.\n",
      "   - Since $\\log_{5}9 = \\log_{5}(3^2) = 2\\log_{5}3$, we can rewrite the expression as $3^{\\frac{4}{2\\log_{5}3}} = 3^{\\frac{2}{\\log_{5}3}}$.\n",
      "   - Using the property of logarithms, we get $3^{\\frac{2}{\\log_{5}3}} = 3^{\\log_{3}9} = 9$.\n",
      "\n",
      "2. Simplify $3^{\\frac{3}{\\log_{\\sqrt{6}}3}}$:\n",
      "   - We know that $\\log_{\\sqrt{6}}3 = \\log_{6^{1/2}}3 = \\frac{1}{2}\\log_{6}3$.\n",
      "   - So, $3^{\\frac{3}{\\log_{\\sqrt{6}}3}} = 3^{\\frac{3}{\\frac{1}{2}\\log_{6}3}} = 3^{\\frac{6}{\\log_{6}3}}$.\n",
      "   - Using the property of logarithms, we get $3^{\\frac{6}{\\log_{6}3}} = 3^{\\log_{3}27} = 27$.\n",
      "\n",
      "3. Combine the simplified terms:\n",
      "   - The first term is $9$ and the second term is $27$, so the expression becomes $\\frac{9 \\cdot 27}{409}$.\n",
      "\n",
      "4. Simplify the second term:\n",
      "   - We know that $(\\sqrt{7})^{\\frac{2}{\\log_{25}7}} = (7^{\\frac{1}{2}})^{\\frac{2}{\\log_{25}7}} = 7^{\\frac{1}{\\log_{25}7}}$.\n",
      "   - Using the property of logarithms, we get $7^{\\frac{1}{\\log_{25}7}} = 7^{\\log_{7}25} = 25$.\n",
      "   - The second term is $25$.\n",
      "\n",
      "5. Combine the simplified terms:\n",
      "   - The first term is $\\frac{9 \\cdot 27}{409}$ and the second term is $25$, so the expression becomes $\\frac{\\frac{9 \\cdot 27}{409} \\cdot 25}{409}$.\n",
      "\n",
      "6. Simplify the final expression:\n",
      "   - The expression simplifies to $\\frac{9 \\cdot 27 \\cdot 25}{409^2}$.\n",
      "\n",
      "Therefore, the final answer is $\\boxed{\\frac{9 \\cdot 27 \\cdot 25}{409^2}}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the units digit of 2011! + 2012! + 2013! + 2014! + 2015!.\n",
      "\n",
      "### Response:\n",
      "To find the units digit of the sum of factorials, we need to consider the units digit of each factorial separately.\n",
      "\n",
      "1. The units digit of 2011! is 0 because it is a multiple of 10.\n",
      "2. The units digit of 2012! is 0 because it is a multiple of 10.\n",
      "3. The units digit of 2013! is 0 because it is a multiple of 10.\n",
      "4. The units digit of 2014! is 0 because it is a multiple of 10.\n",
      "5. The units digit of 2015! is 0 because it is a multiple of 10.\n",
      "\n",
      "Since each factorial has a units digit of 0, the sum of the factorials will also have a units digit of 0.\n",
      "\n",
      "Therefore, the units digit of 2011! + 2012! + 2013! + 2014! + 2015! is \\boxed{0}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\frac{\\\\left(1^4 + \\\\frac14\\\\right)\\\\left(3^4 + \\\\frac14\\\\right)\\\\cdots \\\\left((2n-1)^4 + \\\\frac14\\\\right)}{\\\\left(2^4 + \\\\frac14\\\\right)\\\\left(4^4 + \\\\frac14\\\\right)\\\\cdots \\\\left((2n)^4 + \\\\frac14\\\\right)}$  Given that the expression above can be simplified to $(an^2+ bn + c)^{-1}$ for constants $a,b,c$ , find $21(a-b+c)$ .\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to simplify the given expression and then find the values of $a$, $b$, and $c$.\n",
      "\n",
      "First, let's rewrite the expression as follows:\n",
      "\n",
      "$$\\frac{\\left(1^4 + \\frac{1}{4}\\right)\\left(3^4 + \\frac{1}{4}\\right)\\cdots \\left((2n-1)^4 + \\frac{1}{4}\\right)}{\\left(2^4 + \\frac{1}{4}\\right)\\left(4^4 + \\frac{1}{4}\\right)\\cdots \\left((2n)^4 + \\frac{1}{4}\\right)}$$\n",
      "\n",
      "Now, let's simplify each term in the numerator and denominator:\n",
      "\n",
      "$$\\frac{\\left(1 + \\frac{1}{4}\\right)\\left(3 + \\frac{1}{4}\\right)\\cdots \\left((2n-1) + \\frac{1}{4}\\right)}{\\left(2 + \\frac{1}{4}\\right)\\left(4 + \\frac{1}{4}\\right)\\cdots \\left(2n + \\frac{1}{4}\\right)}$$\n",
      "\n",
      "$$\\frac{\\left(\\frac{5}{4}\\right)\\left(\\frac{13}{4}\\right)\\cdots \\left(\\frac{16n-3}{4}\\right)}{\\left(\\frac{9}{4}\\right)\\left(\\frac{17}{4}\\right)\\cdots \\left(\\frac{16n+1}{4}\\right)}$$\n",
      "\n",
      "Now, let's simplify the expression further:\n",
      "\n",
      "$$\\frac{\\left(\\frac{5}{4}\\right)\\left(\\frac{13}{4}\\right)\\cdots \\left(\\frac{16n-3}{4}\\right)}{\\left(\\frac{9}{4}\\right)\\left(\\frac{17}{4}\\right)\\cdots \\left(\\frac{16n+1}{4}\\right)} = \\frac{5 \\cdot 13 \\cdot \\cdots \\cdot (16n-3)}{9 \\cdot 17 \\cdot \\cdots \\cdot (16n+1)}$$\n",
      "\n",
      "Now, let's find the values of $a$, $b$, and $c$:\n",
      "\n",
      "$$a = 16n^2$$\n",
      "$$b = -16n$$\n",
      "$$c = 1$$\n",
      "\n",
      "Now, we can find $21(a-b+c)$:\n",
      "\n",
      "$$21(a-b+c) = 21(16n^2 - 16n + 1) = 336n^2 - 252n + 21$$\n",
      "\n",
      "Therefore, the final answer is:\n",
      "\n",
      "$$\\boxed{336n^2 - 252n + 21}$$<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\sin^2(1^\\\\circ)+\\\\sin^2(2^\\\\circ)+\\\\ldots+\\\\sin^2(359^\\\\circ)+\\\\sin^2(360^\\\\circ) = \\\\ ?$\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to understand the properties of the sine function and the symmetry of the unit circle. The sine function is periodic with a period of \\(2\\pi\\) radians, which is equivalent to \\(360^\\circ\\). This means that \\(\\sin(x) = \\sin(360^\\circ - x)\\).\n",
      "\n",
      "Given the sum \\(\\sin^2(1^\\circ) + \\sin^2(2^\\circ) + \\ldots + \\sin^2(359^\\circ) + \\sin^2(360^\\circ)\\), we can pair the terms as follows:\n",
      "\\[\n",
      "\\sin^2(1^\\circ) + \\sin^2(359^\\circ) = \\sin^2(1^\\circ) + \\sin^2(360^\\circ - 1^\\circ) = \\sin^2(1^\\circ) + \\sin^2(359^\\circ)\n",
      "\\]\n",
      "\\[\n",
      "\\sin^2(2^\\circ) + \\sin^2(358^\\circ) = \\sin^2(2^\\circ) + \\sin^2(360^\\circ - 2^\\circ) = \\sin^2(2^\\circ) + \\sin^2(358^\\circ)\n",
      "\\]\n",
      "\\[\n",
      "\\ldots\n",
      "\\]\n",
      "\\[\n",
      "\\sin^2(180^\\circ) + \\sin^2(180^\\circ) = \\sin^2(180^\\circ) + \\sin^2(180^\\circ)\n",
      "\\]\n",
      "\\[\n",
      "\\sin^2(181^\\circ) + \\sin^2(179^\\circ) = \\sin^2(181^\\circ) + \\sin^2(179^\\circ)\n",
      "\\]\n",
      "\\[\n",
      "\\ldots\n",
      "\\]\n",
      "\\[\n",
      "\\sin^2(359^\\circ) + \\sin^2(1^\\circ) = \\sin^2(359^\\circ) + \\sin^2(1^\\circ)\n",
      "\\]\n",
      "\n",
      "Since \\(\\sin(360^\\circ - x) = \\sin(x)\\), we can pair the terms symmetrically around \\(180^\\circ\\). Therefore, each pair \\(\\sin^2(x) + \\sin^2(360^\\circ - x)\\) will sum to \\(\\sin^2(x) + \\sin^2(x) = 2\\sin^2(x)\\).\n",
      "\n",
      "Now, we need to sum the terms from \\(1^\\circ\\) to \\(359^\\circ\\). Since there are 360 terms in total, we can pair them into 180 pairs. Each pair sums to \\(2\\sin^2(x)\\), so the total sum is:\n",
      "\\[\n",
      "180 \\times 2\\sin^2(x) = 360\\sin^2(x)\n",
      "\\]\n",
      "\n",
      "Since \\(\\sin^2(x)\\) is symmetric and the sum of \\(\\sin^2(x)\\) over one full period is \\(\\frac{1}{2}\\), we have:\n",
      "\\[\n",
      "\\sin^2(x) = \\frac{1}{2}\n",
      "\\]\n",
      "\n",
      "Thus, the final answer is:\n",
      "\\[\n",
      "\\boxed{360}\n",
      "\\]<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "In $\\\\Delta ABC$ , $s$ represents the semiperimeter and $r_{1}$ represents the radius of the excircle opposite to $A$ . If $3s^{2}=r_{1}^{2}$ , then enter the degree measure of $\\\\angle A$ . This problem is part of the setTrigonometry.\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to use the relationship between the semiperimeter, inradius, and exradius of a triangle. Let's break it down step by step:\n",
      "\n",
      "1. **Understand the given equation**: The equation provided is \\(3s^2 = r_1^2\\). We need to find the measure of \\(\\angle A\\).\n",
      "\n",
      "2. **Recall the formula for the semiperimeter**: The semiperimeter \\(s\\) of a triangle is given by \\(s = \\frac{a + b + c}{2}\\), where \\(a\\), \\(b\\), and \\(c\\) are the lengths of the sides of the triangle.\n",
      "\n",
      "3. **Recall the formula for the inradius**: The inradius \\(r\\) of a triangle is given by \\(r = \\frac{A}{s}\\), where \\(A\\) is the area of the triangle.\n",
      "\n",
      "4. **Recall the formula for the exradius**: The exradius \\(r_1\\) of a triangle is given by \\(r_1 = \\frac{A}{s - a}\\), where \\(A\\) is the area of the triangle and \\(a\\) is one of the sides of the triangle.\n",
      "\n",
      "5. **Use the given equation**: We are given that \\(3s^2 = r_1^2\\). Substitute the expression for \\(r_1\\) from the exradius formula into this equation:\n",
      "   \\[\n",
      "   3s^2 = \\left(\\frac{A}{s - a}\\right)^2\n",
      "   \\]\n",
      "\n",
      "6. **Simplify the equation**: Simplify the right-hand side of the equation:\n",
      "   \\[\n",
      "   3s^2 = \\frac{A^2}{(s - a)^2}\n",
      "   \\]\n",
      "\n",
      "7. **Cross-multiply to solve for \\(A\\)**:\n",
      "   \\[\n",
      "   3s^2 (s - a)^2 = A^2\n",
      "   \\]\n",
      "   \\[\n",
      "   A = \\sqrt{3s^2 (s - a)^2}\n",
      "   \\]\n",
      "\n",
      "8. **Use the area formula**: Recall that the area \\(A\\) of a triangle can also be expressed as \\(A = \\frac{1}{2}bc \\sin A\\). However, we don't need this information directly to solve for \\(\\angle A\\).\n",
      "\n",
      "9. **Relate the exradius to the inradius**: We know that \\(r_1 = \\frac{A}{s - a}\\). Substitute the expression for \\(A\\) from the previous step:\n",
      "   \\[\n",
      "   r_1 = \\frac{\\sqrt{3s^2 (s - a)^2}}{s - a}\n",
      "   \\]\n",
      "\n",
      "10. **Simplify the expression**: Simplify the expression for \\(r_1\\):\n",
      "    \\[\n",
      "    r_1 = \\sqrt{\\frac{3s^2 (s - a)^2}{(s - a)^2}} = \\sqrt{3s^2} = \\sqrt{3} s\n",
      "    \\]\n",
      "\n",
      "11. **Square both sides to solve for \\(r_1^2\\)**:\n",
      "    \\[\n",
      "    r_1^2 = 3s^2\n",
      "    \\]\n",
      "\n",
      "12. **Verify the relationship**: We are given that \\(3s^2 = r_1^2\\), which confirms our calculations.\n",
      "\n",
      "13. **Determine \\(\\angle A\\)**: Since \\(3s^2 = r_1^2\\), we can conclude that \\(\\angle A\\) is a specific angle that satisfies this relationship. Given the symmetry and properties of the triangle, we can deduce that \\(\\angle A\\) must be \\(60^\\circ\\).\n",
      "\n",
      "Therefore, the degree measure of \\(\\angle A\\) is \\(\\boxed{60}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the least number whose last digit is 7 and which becomes 5 times larger when this last digit is carried to the beginning  of the number.\n",
      "\n",
      "### Response:\n",
      "To find the least number whose last digit is 7 and which becomes 5 times larger when this last digit is carried to the beginning of the number, we can follow these steps:\n",
      "\n",
      "1. **Identify the number**: Let's denote the number as \\( N \\). Since the last digit of \\( N \\) is 7, we can express \\( N \\) as \\( N = 10a + 7 \\), where \\( a \\) is the number formed by the remaining digits.\n",
      "\n",
      "2. **Formulate the condition**: When the last digit 7 is moved to the beginning of the number, the new number becomes \\( 7N \\). According to the problem, this new number is 5 times the original number, so we have:\n",
      "   \\[\n",
      "   7N = 5N\n",
      "   \\]\n",
      "   Simplifying this equation, we get:\n",
      "   \\[\n",
      "   7N = 5N \\implies 2N = 0 \\implies N = 0\n",
      "   \\]\n",
      "\n",
      "3. **Check the condition**: Since \\( N = 0 \\) does not satisfy the condition that the new number is 5 times the original number, we need to find the smallest non-zero number that meets the criteria. We need to find the smallest \\( a \\) such that moving the last digit 7 to the beginning of the number results in a number that is 5 times the original number.\n",
      "\n",
      "4. **Iterate through possible values of \\( a \\)**: We will start with \\( a = 1 \\) and check each subsequent value of \\( a \\) until we find the correct number.\n",
      "\n",
      "   - For \\( a = 1 \\):\n",
      "     \\[\n",
      "     N = 10 + 7 = 17\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 71 \\), and \\( 71 \\times 5 = 355 \\), which is not equal to 17.\n",
      "\n",
      "   - For \\( a = 2 \\):\n",
      "     \\[\n",
      "     N = 20 + 7 = 27\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 72 \\), and \\( 72 \\times 5 = 360 \\), which is not equal to 27.\n",
      "\n",
      "   - For \\( a = 3 \\):\n",
      "     \\[\n",
      "     N = 30 + 7 = 37\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 73 \\), and \\( 73 \\times 5 = 365 \\), which is not equal to 37.\n",
      "\n",
      "   - For \\( a = 4 \\):\n",
      "     \\[\n",
      "     N = 40 + 7 = 47\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 74 \\), and \\( 74 \\times 5 = 370 \\), which is not equal to 47.\n",
      "\n",
      "   - For \\( a = 5 \\):\n",
      "     \\[\n",
      "     N = 50 + 7 = 57\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 75 \\), and \\( 75 \\times 5 = 375 \\), which is not equal to 57.\n",
      "\n",
      "   - For \\( a = 6 \\):\n",
      "     \\[\n",
      "     N = 60 + 7 = 67\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 76 \\), and \\( 76 \\times 5 = 380 \\), which is not equal to 67.\n",
      "\n",
      "   - For \\( a = 7 \\):\n",
      "     \\[\n",
      "     N = 70 + 7 = 77\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 77 \\), and \\( 77 \\times 5 = 385 \\), which is not equal to 77.\n",
      "\n",
      "   - For \\( a = 8 \\):\n",
      "     \\[\n",
      "     N = 80 + 7 = 87\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 78 \\), and \\( 78 \\times 5 = 390 \\), which is not equal to 87.\n",
      "\n",
      "   - For \\( a = 9 \\):\n",
      "     \\[\n",
      "     N = 90 + 7 = 97\n",
      "     \\]\n",
      "     Moving the last digit 7 to the beginning gives \\( 79 \\), and \\( 79 \\times 5 = 395 \\), which is not equal to 97.\n",
      "\n",
      "5. **Conclusion**: The smallest number that meets the criteria is \\( N = 147 \\). Moving the last digit 7 to the beginning gives \\( 714 \\), and \\( 714 \\times 5 = 3570 \\), which is not equal to 147. Therefore, the correct number is \\( N = 147 \\).\n",
      "\n",
      "The final answer is:\n",
      "\\boxed{147}<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\int_0^1 x^6 \\\\sqrt{1-x^2} \\\\, dx$  If the integral above can be expressed as $\\\\frac {a\\\\pi}b$ for coprime positive integers $a$ and $b$ , what is the value of $a+b$ ?\n",
      "\n",
      "### Response:\n",
      "To solve the given integral, we can use the substitution method. Let $u = \\sqrt{1-x^2}$. Then, $u^2 = 1-x^2$, and $2u \\, du = -2x \\, dx$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "\n",
      "$$\\int_0^1 x^6 \\sqrt{1-x^2} \\, dx = \\int_0^1 x^6 \\cdot u \\cdot \\frac{1}{2} \\cdot \\frac{1}{u^2} \\, du = \\int_0^1 x^6 \\cdot \\frac{1}{2} \\cdot \\frac{1}{u} \\, du$$\n",
      "\n",
      "Now, we can substitute back $u = \\sqrt{1-x^2}$:\n",
      "\n",
      "$$\\int_0^1 x^6 \\cdot \\frac{1}{2} \\cdot \\frac{1}{u} \\, du = \\int_0^1 x^6 \\cdot \\frac{1}{2} \\cdot \\frac{1}{\\sqrt{1-x^2}} \\, dx$$\n",
      "\n",
      "Now, we can integrate with respect to $x$:\n",
      "\n",
      "$$\\int_0^1 x^6 \\cdot \\frac{1}{2} \\cdot \\frac{1}{\\sqrt{1-x^2}} \\, dx = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{\\sqrt{1-x^2}} \\, dx$$\n",
      "\n",
      "Now, we can use the substitution method again. Let $v = \\sqrt{1-x^2}$. Then, $v^2 = 1-x^2$, and $2v \\, dv = -2x \\, dx$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{\\sqrt{1-x^2}} \\, dx = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{v} \\cdot \\frac{1}{v} \\, dv = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{v^2} \\, dv$$\n",
      "\n",
      "Now, we can substitute back $v = \\sqrt{1-x^2}$:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{v^2} \\, dv = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{(\\sqrt{1-x^2})^2} \\, dv = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{1-x^2} \\, dv$$\n",
      "\n",
      "Now, we can integrate with respect to $x$:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{1-x^2} \\, dv = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{1-x^2} \\, dx$$\n",
      "\n",
      "Now, we can use the substitution method again. Let $w = 1-x^2$. Then, $dw = -2x \\, dx$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{1-x^2} \\, dx = \\frac{1}{2} \\int_0^1 x^6 \\cdot \\frac{1}{1-w} \\cdot \\frac{1}{-2x} \\, dw = \\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{1-w} \\, dw$$\n",
      "\n",
      "Now, we can integrate with respect to $x$:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{1-w} \\, dw = \\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{1-w} \\, dx$$\n",
      "\n",
      "Now, we can use the substitution method again. Let $z = 1-w$. Then, $dz = -dw$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{1-w} \\, dx = \\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{z} \\cdot \\frac{1}{-dz} \\, dz = \\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{z} \\, dz$$\n",
      "\n",
      "Now, we can integrate with respect to $x$:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{z} \\, dz = \\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{z} \\, dx$$\n",
      "\n",
      "Now, we can use the substitution method again. Let $y = x^5$. Then, $dy = 5x^4 \\, dx$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 x^5 \\cdot \\frac{1}{z} \\, dx = \\frac{1}{2} \\int_0^1 y \\cdot \\frac{1}{z} \\cdot \\frac{1}{5x^4} \\, dy = \\frac{1}{2} \\int_0^1 y \\cdot \\frac{1}{z} \\cdot \\frac{1}{5y^{4/5}} \\, dy$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 y \\cdot \\frac{1}{z} \\cdot \\frac{1}{5y^{4/5}} \\, dy = \\frac{1}{2} \\int_0^1 \\frac{1}{5z} \\cdot \\frac{1}{y^{4/5}} \\, dy$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\frac{1}{2} \\int_0^1 \\frac{1}{5z} \\cdot \\frac{1}{y^{4/5}} \\, dy = \\frac{1}{2} \\cdot \\frac{1}{5z} \\int_0^1 y^{-4/5} \\, dy$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\frac{1}{2} \\cdot \\frac{1}{5z} \\int_0^1 y^{-4/5} \\, dy = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{-4/5+1} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1 = \\frac{1}{2} \\cdot \\frac{1}{5z} \\cdot \\frac{y^{-4/5+1}}{1/5} \\bigg|_0^1$$\n",
      "\n",
      "Now, we can integrate with respect to $y$:\n",
      "\n",
      "$$\\\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "(2+6)/4^100+(2+2√ó6)/4^99+(2+3√ó6)/4^98................+(2+99√ó6)/4^2+(2+100√ó6)/4 is equal to\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to simplify the expression step by step. Let's break it down:\n",
      "\n",
      "1. **Identify the pattern**: The given expression is a sum of terms where each term has a specific form. We can see that each term is of the form \\(\\frac{2 + k \\cdot 6}{4^{100-k}}\\), where \\(k\\) ranges from 1 to 100.\n",
      "\n",
      "2. **Simplify each term**: Let's simplify the general term \\(\\frac{2 + k \\cdot 6}{4^{100-k}}\\). Notice that \\(2 + k \\cdot 6 = 2(1 + 3k)\\). So, the term becomes \\(\\frac{2(1 + 3k)}{4^{100-k}} = \\frac{2}{4^{100-k}} \\cdot (1 + 3k)\\).\n",
      "\n",
      "3. **Sum the series**: Now, we need to sum all these terms from \\(k = 1\\) to \\(k = 100\\). This can be written as:\n",
      "   \\[\n",
      "   \\sum_{k=1}^{100} \\frac{2(1 + 3k)}{4^{100-k}}\n",
      "   \\]\n",
      "\n",
      "4. **Factor out the constant 2**: We can factor out the constant 2 from the sum:\n",
      "   \\[\n",
      "   2 \\sum_{k=1}^{100} \\frac{1 + 3k}{4^{100-k}}\n",
      "   \\]\n",
      "\n",
      "5. **Simplify the inner sum**: Let's simplify the inner sum \\(\\sum_{k=1}^{100} \\frac{1 + 3k}{4^{100-k}}\\). Notice that \\(\\frac{1 + 3k}{4^{100-k}} = \\frac{1}{4^{100-k}} + \\frac{3k}{4^{100-k}}\\). So, the sum becomes:\n",
      "   \\[\n",
      "   \\sum_{k=1}^{100} \\left( \\frac{1}{4^{100-k}} + \\frac{3k}{4^{100-k}} \\right)\n",
      "   \\]\n",
      "\n",
      "6. **Separate the sums**: We can separate the sums into two parts:\n",
      "   \\[\n",
      "   \\sum_{k=1}^{100} \\frac{1}{4^{100-k}} + \\sum_{k=1}^{100} \\frac{3k}{4^{100-k}}\n",
      "   \\]\n",
      "\n",
      "7. **Evaluate the first sum**: The first sum is a geometric series:\n",
      "   \\[\n",
      "   \\sum_{k=1}^{100} \\frac{1}{4^{100-k}} = \\frac{1}{4^0} + \\frac{1}{4^1} + \\frac{1}{4^2} + \\cdots + \\frac{1}{4^{99}}\n",
      "   \\]\n",
      "   This is a geometric series with the first term \\(a = 1\\) and common ratio \\(r = \\frac{1}{4}\\). The sum of the first 100 terms of a geometric series is given by:\n",
      "   \\[\n",
      "   S = \\frac{1 - \\left(\\frac{1}{4}\\right)^{100}}{1 - \\frac{1}{4}} = \\frac{1 - \\left(\\frac{1}{4}\\right)^{100}}{\\frac{3}{4}} = \\frac{4}{3} \\left(1 - \\left(\\frac{1}{4}\\right)^{100}\\right)\n",
      "   \\]\n",
      "\n",
      "8. **Evaluate the second sum**: The second sum is a series of the form \\(\\sum_{k=1}^{100} \\frac{3k}{4^{100-k}}\\). Notice that:\n",
      "   \\[\n",
      "   \\sum_{k=1}^{100} \\frac{3k}{4^{100-k}} = 3 \\sum_{k=1}^{100} \\frac{k}{4^{100-k}}\n",
      "   \\]\n",
      "   This is a series of the form \\(\\sum_{k=1}^{100} \\frac{k}{4^{100-k}}\\), which is a series of the form \\(\\sum_{k=1}^{100} \\frac{k}{4^{100-k}}\\). This is a series of the form \\(\\sum_{k=1}^{100} \\frac{k}{4^{100-k}}\\), which is a series of the form \\(\\sum_{k=1}^{100} \\frac{k}{4^{100-k}}\\).\n",
      "\n",
      "9. **Combine the results**: Combining the results, we get:\n",
      "   \\[\n",
      "   2 \\left( \\frac{4}{3} \\left(1 - \\left(\\frac{1}{4}\\right)^{100}\\right) + 3 \\sum_{k=1}^{100} \\frac{k}{4^{100-k}} \\right)\n",
      "   \\]\n",
      "\n",
      "10. **Simplify the final expression**: The final expression is:\n",
      "   \\[\n",
      "   \\boxed{2 \\left( \\frac{4}{3} \\left(1 - \\left(\\frac{1}{4}\\right)^{100}\\right) + 3 \\sum_{k=1}^{100} \\frac{k}{4^{100-k}} \\right)}\n",
      "   \\]<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\sum_{n=0}^{\\\\infty} \\\\frac{(-1)^n}{(n+1)(2n+1)}= \\\\ ?$  Give your answer to 2 decimal places.\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we can use the method of partial fractions and the properties of infinite series. Let's break it down step by step:\n",
      "\n",
      "1. **Identify the series**: The given series is $\\sum_{n=0}^{\\infty} \\frac{(-1)^n}{(n+1)(2n+1)}$.\n",
      "\n",
      "2. **Simplify the general term**: Notice that the general term $\\frac{(-1)^n}{(n+1)(2n+1)}$ can be rewritten as $\\frac{(-1)^n}{n+1} \\cdot \\frac{1}{2n+1}$.\n",
      "\n",
      "3. **Use the method of partial fractions**: We can express the general term as a sum of two simpler fractions. Let's try to decompose it into $\\frac{A}{n+1} + \\frac{B}{2n+1}$.\n",
      "\n",
      "4. **Determine the constants A and B**: By equating the numerators, we get $(-1)^n = A(2n+1) + B(n+1)$. By comparing coefficients, we find that $A = \\frac{1}{2}$ and $B = -\\frac{1}{2}$.\n",
      "\n",
      "5. **Rewrite the series**: Now, the series becomes $\\sum_{n=0}^{\\infty} \\left( \\frac{(-1)^n}{2(n+1)} - \\frac{(-1)^n}{2(2n+1)} \\right)$.\n",
      "\n",
      "6. **Simplify the series**: Notice that the series is telescoping. Most terms will cancel out, leaving us with $\\frac{1}{2} - \\frac{1}{2} + \\frac{1}{6} - \\frac{1}{10} + \\frac{1}{14} - \\frac{1}{18} + \\ldots$.\n",
      "\n",
      "7. **Sum the series**: The series simplifies to $\\frac{1}{2} - \\frac{1}{2} + \\frac{1}{6} - \\frac{1}{10} + \\frac{1}{14} - \\frac{1}{18} + \\ldots = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right)$.\n",
      "\n",
      "8. **Final answer**: The sum of the series is $\\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right) = \\frac{1}{2} \\left(1 - \\frac{1}{3} + \\frac{1}{3} - \\frac{1}{5}\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "I have 2016 identical marbles. I plan to distribute them equally into one or more identical containers. How many ways can this be done if I have an unlimited number of containers?\n",
      "\n",
      "### Response:\n",
      "To determine the number of ways to distribute 2016 identical marbles into one or more identical containers, we need to consider the concept of partitions of an integer. Specifically, we are looking for the number of partitions of 2016 into parts where each part is a positive integer.\n",
      "\n",
      "The number of partitions of an integer \\( n \\) is denoted by \\( p(n) \\). For small values of \\( n \\), we can calculate \\( p(n) \\) using the partition function. However, for large values like 2016, the exact value is not straightforward to compute without computational tools.\n",
      "\n",
      "For the sake of this problem, let's assume we have a computational tool or a reference that provides the partition function for large numbers. The partition function for 2016 is known to be a very large number, but for the purpose of this exercise, we will denote it as \\( p(2016) \\).\n",
      "\n",
      "Thus, the number of ways to distribute 2016 identical marbles into one or more identical containers is given by \\( p(2016) \\).\n",
      "\n",
      "\\boxed{p(2016)}<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\int_{-2}^5 \\\\big(|x+1|+|x+2|-|2x+3| \\\\big)\\\\ dx = \\\\ ?$   $$  Notation: $|\\\\cdot|$ denotes theabsolute value function.\n",
      "\n",
      "### Response:\n",
      "To solve the given integral, we need to consider the behavior of the absolute value function within the given limits. The absolute value function changes its behavior at the points where the expression inside the absolute value is zero. In this case, the expression inside the absolute value is zero at $x = -1$, $x = -2$, and $x = -\\frac{3}{2}$. We can break the integral into three parts based on these points:\n",
      "\n",
      "1. $x \\in (-\\infty, -2)$\n",
      "2. $x \\in [-2, -1)$\n",
      "3. $x \\in [-1, \\infty)$\n",
      "\n",
      "Let's evaluate the integral in each of these parts:\n",
      "\n",
      "1. For $x \\in (-\\infty, -2)$, the expression inside the absolute value is negative, so we can remove the absolute value signs:\n",
      "\n",
      "$$\\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (x+1) + (x+2) - (2x+3) \\, dx = \\int_{-\\infty}^{-2} (\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "What is the sum of the factors of $50! + 50$ between $1$ and $50$ inclusive? (Eg. The factors of 15 between 1 and 4 are 1 and 3)\n",
      "\n",
      "### Response:\n",
      "To find the sum of the factors of $50! + 50$ between $1$ and $50$ inclusive, we need to follow these steps:\n",
      "\n",
      "1. Calculate $50! + 50$.\n",
      "2. Find all the factors of the result from step 1.\n",
      "3. Sum up all the factors found in step 2.\n",
      "\n",
      "Let's start with step 1:\n",
      "\n",
      "Step 1: Calculate $50! + 50$.\n",
      "$50!$ is the factorial of 50, which is the product of all positive integers from 1 to 50. So, $50! = 1 \\times 2 \\times 3 \\times \\ldots \\times 49 \\times 50$.\n",
      "$50! + 50$ is the sum of $50!$ and 50.\n",
      "\n",
      "Now, let's move on to step 2:\n",
      "\n",
      "Step 2: Find all the factors of the result from step 1.\n",
      "We need to find all the numbers that divide the result without leaving a remainder.\n",
      "\n",
      "Finally, let's move on to step 3:\n",
      "\n",
      "Step 3: Sum up all the factors found in step 2.\n",
      "We need to add up all the factors found in step 2.\n",
      "\n",
      "After performing these steps, we get the sum of the factors of $50! + 50$ between $1$ and $50$ inclusive.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Let $f:\\\\mathbb{R} \\\\to \\\\mathbb{R}$ be a function defined by $f(x)=\\\\begin{cases} \\\\lfloor x \\\\rfloor \\\\ \\\\ , \\\\quad x \\\\leq 2 \\\\\\\\ 0 \\\\ \\\\ , \\\\quad x>2 \\\\end{cases}$ IfI=‚à´‚àí12xf(x2)2+f(x+1)dxI=\\\\int_{-1}^2 \\\\frac{xf(x^2)}{2+f(x+1)}\\\\, dxI=‚à´‚àí12‚Äã2+f(x+1)xf(x2)‚Äãdx, then find the value of $4I-1$ .\n",
      "\n",
      "### Response:\n",
      "To find the value of \\(4I - 1\\), we need to evaluate the integral \\(I\\). Let's break down the integral into two parts based on the definition of \\(f(x)\\):\n",
      "\n",
      "1. For \\(x \\leq 2\\), \\(f(x) = \\lfloor x \\rfloor\\). Therefore, \\(f(x^2) = \\lfloor x^2 \\rfloor\\).\n",
      "2. For \\(x > 2\\), \\(f(x) = 0\\), so \\(f(x+1) = 0\\).\n",
      "\n",
      "Now, let's evaluate the integral:\n",
      "\n",
      "\\[I = \\int_{-1}^2 \\frac{xf(x^2)}{2 + f(x+1)} \\, dx\\]\n",
      "\n",
      "Since \\(f(x) = 0\\) for \\(x > 2\\), the integral simplifies to:\n",
      "\n",
      "\\[I = \\int_{-1}^2 \\frac{xf(x^2)}{2} \\, dx\\]\n",
      "\n",
      "Now, we can split the integral into two parts:\n",
      "\n",
      "\\[I = \\int_{-1}^2 \\frac{x \\lfloor x \\rfloor}{2} \\, dx\\]\n",
      "\n",
      "Since \\(\\lfloor x \\rfloor\\) is a step function, we can split the integral into two parts:\n",
      "\n",
      "\\[I = \\int_{-1}^0 \\frac{x \\lfloor x \\rfloor}{2} \\, dx + \\int_0^2 \\frac{x \\lfloor x \\rfloor}{2} \\, dx\\]\n",
      "\n",
      "For \\(x \\leq 0\\), \\(\\lfloor x \\rfloor = -1\\), so:\n",
      "\n",
      "\\[I_1 = \\int_{-1}^0 \\frac{x(-1)}{2} \\, dx = \\int_{-1}^0 \\frac{-x}{2} \\, dx = \\left[-\\frac{x^2}{4}\\right]_{-1}^0 = 0 - \\left(-\\frac{1}{4}\\right) = \\frac{1}{4}\\]\n",
      "\n",
      "For \\(0 \\leq x \\leq 2\\), \\(\\lfloor x \\rfloor = 0\\) or \\(1\\), so:\n",
      "\n",
      "\\[I_2 = \\int_0^2 \\frac{x \\cdot 0}{2} \\, dx = \\int_0^2 0 \\, dx = 0\\]\n",
      "\n",
      "\\[I_2 = \\int_0^2 \\frac{x \\cdot 1}{2} \\, dx = \\int_0^2 \\frac{x}{2} \\, dx = \\left[\\frac{x^2}{4}\\right]_0^2 = \\frac{4}{4} - 0 = 1\\]\n",
      "\n",
      "So, the total integral \\(I\\) is:\n",
      "\n",
      "\\[I = I_1 + I_2 = \\frac{1}{4} + 1 = \\frac{5}{4}\\]\n",
      "\n",
      "Now, we need to find \\(4I - 1\\):\n",
      "\n",
      "\\[4I - 1 = 4 \\cdot \\frac{5}{4} - 1 = 5 - 1 = 4\\]\n",
      "\n",
      "Therefore, the value of \\(4I - 1\\) is \\(\\boxed{4}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the number of different ways in which the 10 letters of the word  \" P R O P O R T I O N \" can be arranged if exactly two of the O s are next to each other. You may use a calculator.\n",
      "\n",
      "### Response:\n",
      "To find the number of different ways in which the 10 letters of the word \"PROPORTION\" can be arranged if exactly two of the O's are next to each other, we can follow these steps:\n",
      "\n",
      "1. **Identify the total number of letters and their arrangement**: The word \"PROPORTION\" consists of 10 letters, where \"O\" appears 3 times, \"P\" appears 1 time, and \"R\" appears 1 time.\n",
      "\n",
      "2. **Calculate the total number of arrangements without any restrictions**: The total number of arrangements of these 10 letters is given by the factorial of the total number of letters, which is \\(10!\\).\n",
      "\n",
      "3. **Calculate the number of arrangements where exactly two \"O\"s are next to each other**: We can treat the two adjacent \"O\"s as a single \"super letter\". This reduces the problem to arranging 9 \"letters\" (8 individual letters + 1 \"super letter\"). The number of ways to arrange these 9 \"letters\" is \\(9!\\). However, within the \"super letter\", the two \"O\"s can be arranged in 2 different ways (either \"OO\" or \"Oo\"). Therefore, the total number of arrangements where exactly two \"O\"s are next to each other is \\(9! \\times 2\\).\n",
      "\n",
      "4. **Subtract the restricted arrangements from the total arrangements**: The number of valid arrangements is the total number of arrangements minus the number of arrangements where exactly two \"O\"s are next to each other.\n",
      "\n",
      "Let's calculate this step-by-step:\n",
      "\n",
      "1. Calculate \\(10!\\):\n",
      "\\[10! = 3628800\\]\n",
      "\n",
      "2. Calculate \\(9!\\):\n",
      "\\[9! = 362880\\]\n",
      "\n",
      "3. Calculate \\(9! \\times 2\\):\n",
      "\\[9! \\times 2 = 362880 \\times 2 = 725760\\]\n",
      "\n",
      "4. Subtract the restricted arrangements from the total arrangements:\n",
      "\\[3628800 - 725760 = 2903040\\]\n",
      "\n",
      "Therefore, the number of different ways in which the 10 letters of the word \"PROPORTION\" can be arranged if exactly two of the O's are next to each other is \\(\\boxed{2903040}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "In the diagram, $P$ is a point inside a regular hexagon $ABCDEF$ . The areas of triangles $ABP$ , $CDP$ , and $EFP$ are $\\\\SI{3}{\\\\centi\\\\meter\\\\squared}, \\\\SI{5}{\\\\centi\\\\meter\\\\squared},$ and $\\\\SI{8}{\\\\centi\\\\meter\\\\squared},$ respectively. Find the area of triangle $BCP$ in $\\\\si{\\\\centi\\\\meter\\\\squared}$ rounded off to the nearest thousandth.\n",
      "\n",
      "### Response:\n",
      "To find the area of triangle $BCP$, we can use the fact that the sum of the areas of triangles $ABP$, $CDP$, and $EFP$ is equal to the area of the hexagon. Let's denote the area of triangle $BCP$ as $x$. Then we have:\n",
      "\n",
      "$$\\SI{3}{\\centi\\meter\\squared} + \\SI{5}{\\centi\\meter\\squared} + \\SI{8}{\\centi\\meter\\squared} + x = \\text{Area of hexagon}$$\n",
      "\n",
      "We can rearrange this equation to solve for $x$:\n",
      "\n",
      "$$x = \\text{Area of hexagon} - \\SI{3}{\\centi\\meter\\squared} - \\SI{5}{\\centi\\meter\\squared} - \\SI{8}{\\centi\\meter\\squared}$$\n",
      "\n",
      "Now we need to find the area of the hexagon. We can do this by subtracting the areas of triangles $ABP$, $CDP$, and $EFP$ from the total area of the hexagon. Let's denote the total area of the hexagon as $A$. Then we have:\n",
      "\n",
      "$$A = \\text{Area of hexagon} = \\text{Area of } \\triangle ABC + \\text{Area of } \\triangle BCD + \\text{Area of } \\triangle CDE + \\text{Area of } \\triangle DEF + \\text{Area of } \\triangle GEF + \\text{Area of } \\triangle ABC$$\n",
      "\n",
      "We can simplify this equation by noting that the area of each triangle is equal to the area of the hexagon divided by 6. Therefore, we have:\n",
      "\n",
      "$$A = \\frac{1}{6} \\text{Area of hexagon} + \\frac{1}{6} \\text{Area of hexagon} + \\frac{1}{6} \\text{Area of hexagon} + \\frac{1}{6} \\text{Area of hexagon} + \\frac{1}{6} \\text{Area of hexagon} + \\frac{1}{6} \\text{Area of hexagon}$$\n",
      "\n",
      "Simplifying this equation, we get:\n",
      "\n",
      "$$A = \\frac{1}{6} \\text{Area of hexagon} \\times 6 = \\text{Area of hexagon}$$\n",
      "\n",
      "Therefore, the area of triangle $BCP$ is:\n",
      "\n",
      "$$\\boxed{1.25}$$<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Calculate the normalization constant $A$ if the wavefunction is  $\\\\Psi(x,t) = A\\\\text{exp}(-2\\\\pi |x|-i\\\\pi t).$\n",
      "\n",
      "### Response:\n",
      "To calculate the normalization constant $A$, we need to ensure that the integral of the probability density function over all space is equal to 1. The probability density function is given by the square of the wavefunction, so we need to integrate the square of the wavefunction over all space and set it equal to 1.\n",
      "\n",
      "The wavefunction is given by $\\Psi(x,t) = A\\text{exp}(-2\\pi |x| - i\\pi t)$. To find the probability density function, we need to square the wavefunction:\n",
      "\n",
      "$$|\\Psi(x,t)|^2 = A^2\\text{exp}(-4\\pi^2 x^2 - 2i\\pi^2 x t - \\pi^2 t^2)$$\n",
      "\n",
      "Now, we need to integrate this expression over all space and set it equal to 1:\n",
      "\n",
      "$$\\int_{-\\infty}^{\\infty} A^2\\text{exp}(-4\\pi^2 x^2 - 2i\\pi^2 x t - \\pi^2 t^2) dx = 1$$\n",
      "\n",
      "To solve this integral, we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation, which is a partial differential equation. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral. The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to the Schr√∂dinger equation to simplify the integral.\n",
      "\n",
      "The wavefunction is a solution to the Schr√∂dinger equation, so we can use the fact that the wavefunction is a solution to\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$729x^{6}-1458x^{5}-1215x^{4}-540x^{3}-135x^{2}-18x-1=0$  The above equation has a positive real root in the form  $\\\\frac13(a^{1/6}+b^{1/6}+c^{1/6}+d^{1/6}+e^{1/6}+f^{1/6}).$  with $a, b, c, d, e, f$ are all integers. Find $a+b+c+d+e+f+3$ .\n",
      "\n",
      "### Response:\n",
      "To solve the given equation, we can start by factoring out the common term $x^3$ from the equation:\n",
      "\n",
      "$729x^3(x^3 - 2x^2 - 3x - 1) = 0$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$\n",
      "\n",
      "Now, we can see that the equation has a positive real root in the form $\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6})$. We can rewrite this expression as:\n",
      "\n",
      "$\\frac{1}{3}(a^{1/6} + b^{1/6} + c^{1/6} + d^{1/6} + e^{1/6} + f^{1/6}) = \\frac{1\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A parallel plate capacitor with plate seperation 5mm is charged by a battery.it is found that on introducing a mica aheet 2 mm thick, while keeping the connections intact, capacitor draws 25 percnt. more charge than before.Find dielectric constant of mica.\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to use the concept of capacitance and the relationship between capacitance, charge, and voltage. \n",
      "\n",
      "First, let's define the variables:\n",
      "\n",
      "* C1: Capacitance before introducing the mica sheet\n",
      "* C2: Capacitance after introducing the mica sheet\n",
      "* Q1: Charge before introducing the mica sheet\n",
      "* Q2: Charge after introducing the mica sheet\n",
      "* Œµ0: Permittivity of free space\n",
      "* Œµr: Relative permittivity of the mica sheet\n",
      "* Œµm: Permittivity of the mica sheet\n",
      "* d: Plate separation\n",
      "* d1: Plate separation after introducing the mica sheet\n",
      "\n",
      "We are given that the capacitor draws 25% more charge after introducing the mica sheet, so:\n",
      "\n",
      "Q2 = 1.25 * Q1\n",
      "\n",
      "We are also given that the plate separation remains the same after introducing the mica sheet, so:\n",
      "\n",
      "C2 = C1\n",
      "\n",
      "The capacitance of a parallel plate capacitor is given by:\n",
      "\n",
      "C = Œµ0 * Œµr * A / d\n",
      "\n",
      "where A is the area of the plates and Œµr is the relative permittivity of the material between the plates.\n",
      "\n",
      "The charge on a capacitor is given by:\n",
      "\n",
      "Q = C * V\n",
      "\n",
      "where V is the voltage across the capacitor.\n",
      "\n",
      "We can use these equations to relate the capacitance before and after introducing the mica sheet:\n",
      "\n",
      "C1 = Œµ0 * Œµr * A / d\n",
      "C2 = Œµ0 * Œµr * A / d1\n",
      "\n",
      "Since C2 = C1, we can equate the two expressions:\n",
      "\n",
      "Œµ0 * Œµr * A / d = Œµ0 * Œµr * A / d1\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "d1 = d\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "where V2 is the voltage across the capacitor after introducing the mica sheet.\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "We can also express the voltage across the capacitor after introducing the mica sheet in terms of the voltage across the capacitor before introducing the mica sheet:\n",
      "\n",
      "V2 = V1 * (1 + 0.25)\n",
      "\n",
      "where V1 is the voltage across the capacitor before introducing the mica sheet.\n",
      "\n",
      "Substituting this expression into the previous equation, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * (1 + 0.25)\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Canceling out the 1.25 on both sides, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of the mica sheet:\n",
      "\n",
      "Q2 = C2 * V2\n",
      "\n",
      "Substituting the expressions for Q2 and C2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V2\n",
      "\n",
      "Substituting the expression for V2, we get:\n",
      "\n",
      "1.25 * Q1 = (Œµ0 * Œµr * A / d) * V1 * 1.25\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Q1 = Œµ0 * Œµr * A / d * V1\n",
      "\n",
      "Now, we can use the relationship between charge, capacitance, and voltage to find the relative permittivity of\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "There were 8 actors who were very close to getting the role of Peeta in the Hunger Games and about 12 actresses who almost got the coveted role of Katniss. How many possible pairings of Katniss and Peeta there could have been if we don't take the main actors into account?\n",
      "\n",
      "### Response:\n",
      "To determine the number of possible pairings of Katniss and Peeta, we need to consider the total number of actors and actresses who were considered for the roles. \n",
      "\n",
      "There were 8 actors who were very close to getting the role of Peeta, and 12 actresses who almost got the coveted role of Katniss. \n",
      "\n",
      "Since we are not taking the main actors into account, we can only consider the 12 actresses who almost got the role of Katniss. \n",
      "\n",
      "Therefore, there are 12 possible pairings of Katniss and Peeta. \n",
      "\n",
      "\\boxed{12}<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$L = \\\\lim_{n\\\\to \\\\infty} \\\\sum_{r=1}^{n} \\\\sum_{t=0}^{r-1} \\\\frac 1{5^n} \\\\binom nr \\\\binom rt 3^t$  Given the above, find the value of $L^{99} + 99^L$ . Notation: $\\\\dbinom NM = \\\\dfrac {N!}{M!(N-M)!}$ denotes thebinomial coefficient.\n",
      "\n",
      "### Response:\n",
      "To find the value of \\( L^{99} + 99^L \\), we first need to evaluate the limit \\( L \\). The given expression is:\n",
      "\n",
      "\\[ L = \\lim_{n \\to \\infty} \\sum_{r=1}^{n} \\sum_{t=0}^{r-1} \\frac{1}{5^n} \\binom{n}{r} \\binom{r}{t} 3^t \\]\n",
      "\n",
      "We can simplify the inner sum by recognizing that \\( \\binom{n}{r} \\binom{r}{t} = \\binom{n}{r} \\binom{r}{t} \\). The binomial coefficient \\( \\binom{n}{r} \\) is the number of ways to choose \\( r \\) elements from \\( n \\) elements, and \\( \\binom{r}{t} \\) is the number of ways to choose \\( t \\) elements from \\( r \\) elements. Therefore, the inner sum can be rewritten as:\n",
      "\n",
      "\\[ \\sum_{t=0}^{r-1} \\binom{r}{t} 3^t \\]\n",
      "\n",
      "This is a binomial expansion of \\( (1 + 3)^r \\) evaluated at \\( t \\):\n",
      "\n",
      "\\[ \\sum_{t=0}^{r-1} \\binom{r}{t} 3^t = (1 + 3)^r - \\binom{r}{r} 3^r = 4^r - 1 \\]\n",
      "\n",
      "So the original limit becomes:\n",
      "\n",
      "\\[ L = \\lim_{n \\to \\infty} \\sum_{r=1}^{n} \\frac{1}{5^n} (4^r - 1) \\]\n",
      "\n",
      "We can split this into two sums:\n",
      "\n",
      "\\[ L = \\lim_{n \\to \\infty} \\sum_{r=1}^{n} \\frac{1}{5^n} 4^r - \\sum_{r=1}^{n} \\frac{1}{5^n} \\]\n",
      "\n",
      "The first sum is a geometric series with the first term \\( \\frac{4}{5} \\) and common ratio \\( \\frac{4}{5} \\):\n",
      "\n",
      "\\[ \\sum_{r=1}^{n} \\frac{1}{5^n} 4^r = \\frac{4}{5} \\sum_{r=1}^{n} \\left( \\frac{4}{5} \\right)^{r-1} = \\frac{4}{5} \\cdot \\frac{1 - \\left( \\frac{4}{5} \\right)^n}{1 - \\frac{4}{5}} = \\frac{4}{5} \\cdot \\frac{1 - \\left( \\frac{4}{5} \\right)^n}{\\frac{1}{5}} = 4 \\left( 1 - \\left( \\frac{4}{5} \\right)^n \\right) \\]\n",
      "\n",
      "As \\( n \\to \\infty \\), \\( \\left( \\frac{4}{5} \\right)^n \\to 0 \\), so the first sum approaches 4.\n",
      "\n",
      "The second sum is a geometric series with the first term \\( \\frac{1}{5} \\) and common ratio \\( \\frac{4}{5} \\):\n",
      "\n",
      "\\[ \\sum_{r=1}^{n} \\frac{1}{5^n} = \\frac{1}{5} \\sum_{r=1}^{n} \\left( \\frac{4}{5} \\right)^{r-1} = \\frac{1}{5} \\cdot \\frac{1 - \\left( \\frac{4}{5} \\right)^n}{1 - \\frac{4}{5}} = \\frac{1}{5} \\cdot \\frac{1 - \\left( \\frac{4}{5} \\right)^n}{\\frac{1}{5}} = 1 - \\left( \\frac{4}{5} \\right)^n \\]\n",
      "\n",
      "As \\( n \\to \\infty \\), \\( \\left( \\frac{4}{5} \\right)^n \\to 0 \\), so the second sum approaches 1.\n",
      "\n",
      "Therefore, the limit \\( L \\) is:\n",
      "\n",
      "\\[ L = 4 - 1 = 3 \\]\n",
      "\n",
      "Now we need to find \\( L^{99} + 99^L \\):\n",
      "\n",
      "\\[ L^{99} + 99^L = 3^{99} + 99^3 \\]\n",
      "\n",
      "So the final answer is:\n",
      "\n",
      "\\boxed{3^{99} + 99^3}<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "The mid-points of side of a square of side 12 m are joined to form another square inside it.The mid-points of the inner square are further joined to form another square within it.This process goes on infinitely.Find the sum of areas of all such squares formed.\n",
      "\n",
      "### Response:\n",
      "To find the sum of areas of all such squares formed, we need to consider the area of each square and how it is formed.\n",
      "\n",
      "1. The first square has a side length of 12 m, so its area is 12^2 = 144 m^2.\n",
      "2. The second square is formed by joining the mid-points of the sides of the first square. The side length of the second square is half the side length of the first square, so its area is (12/2)^2 = 36 m^2.\n",
      "3. The third square is formed by joining the mid-points of the sides of the second square. The side length of the third square is half the side length of the second square, so its area is (12/4)^2 = 9 m^2.\n",
      "4. This process continues infinitely, so the areas of the squares form a geometric series with the first term a = 144 m^2 and the common ratio r = 1/4.\n",
      "\n",
      "The sum of an infinite geometric series is given by the formula:\n",
      "\n",
      "S = a / (1 - r)\n",
      "\n",
      "Plugging in the values, we get:\n",
      "\n",
      "S = 144 / (1 - 1/4) = 144 / (3/4) = 144 * (4/3) = 192 m^2\n",
      "\n",
      "So, the sum of areas of all such squares formed is 192 m^2.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A ray of light is incident at an angle of $60^\\\\circ$ on one face of a prism which has an apex angle of $30^\\\\circ.$ The ray emerging out of the prism makes an angle of $30^{\\\\circ}$ with the incident ray. Calculate the refractive index of the material of the prism.\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to use Snell's law and the geometry of the prism. Let's break it down step by step:\n",
      "\n",
      "1. **Identify the given information:**\n",
      "   - Incident angle (\\(\\theta_i\\)) = \\(60^\\circ\\)\n",
      "   - Apex angle (\\(\\theta_p\\)) = \\(30^\\circ\\)\n",
      "   - Angle of emergence (\\(\\theta_e\\)) = \\(30^\\circ\\)\n",
      "\n",
      "2. **Determine the angle of refraction (\\(\\theta_r\\)) using Snell's law:**\n",
      "   Snell's law states that \\(n_1 \\sin(\\theta_i) = n_2 \\sin(\\theta_r)\\), where \\(n_1\\) is the refractive index of the prism and \\(n_2\\) is the refractive index of the medium outside the prism.\n",
      "\n",
      "3. **Calculate the angle of incidence in the prism:**\n",
      "   Since the prism is a right-angled prism, the angle of incidence in the prism (\\(\\theta_i'\\)) can be found using the apex angle:\n",
      "   \\[\n",
      "   \\theta_i' = \\theta_i - \\theta_p = 60^\\circ - 30^\\circ = 30^\\circ\n",
      "   \\]\n",
      "\n",
      "4. **Apply Snell's law in the prism:**\n",
      "   \\[\n",
      "   n_1 \\sin(30^\\circ) = n_2 \\sin(\\theta_r)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_1 \\left(\\frac{1}{2}\\right) = n_2 \\sin(\\theta_r)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_1 = 2n_2 \\sin(\\theta_r)\n",
      "   \\]\n",
      "\n",
      "5. **Determine the angle of refraction in the prism:**\n",
      "   Since the angle of refraction in the prism is the same as the angle of incidence in the prism, \\(\\theta_r = 30^\\circ\\).\n",
      "\n",
      "6. **Apply Snell's law again in the prism:**\n",
      "   \\[\n",
      "   n_2 \\sin(30^\\circ) = n_1 \\sin(\\theta_r)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_2 \\left(\\frac{1}{2}\\right) = n_1 \\sin(30^\\circ)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_2 = 2n_1 \\sin(30^\\circ)\n",
      "   \\]\n",
      "\n",
      "7. **Calculate the refractive index of the prism:**\n",
      "   \\[\n",
      "   n_2 = 2n_1 \\sin(30^\\circ)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_2 = n_1\n",
      "   \\]\n",
      "\n",
      "8. **Determine the refractive index of the prism:**\n",
      "   Since the angle of refraction in the prism is the same as the angle of incidence in the prism, \\(\\theta_r = 30^\\circ\\).\n",
      "\n",
      "9. **Calculate the refractive index of the prism:**\n",
      "   \\[\n",
      "   n_2 = 2n_1 \\sin(30^\\circ)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "   \\]\n",
      "   \\[\n",
      "   n_2 = n_1\n",
      "   \\]\n",
      "\n",
      "10. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "11. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "12. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "13. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "14. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "15. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "16. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "17. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "18. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "19. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "20. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "21. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "22. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "23. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "24. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "25. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\right)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = n_1\n",
      "    \\]\n",
      "\n",
      "26. **Calculate the refractive index of the prism:**\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\sin(30^\\circ)\n",
      "    \\]\n",
      "    \\[\n",
      "    n_2 = 2n_1 \\left(\\frac{1}{2}\\\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "One day, reading One Piece and eating chips, you notice a circle that has a diameter of 5 inches. You notice a smaller circle of 4.951 inches inside the circle. If the next circle is 4.542, what is 5 circles after that? Hint: Use a Pattern\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to find the radius of each circle and then calculate the area of each circle. We can then sum up the areas of all the circles to find the total area.\n",
      "\n",
      "1. First, let's find the radius of the first circle. The diameter of the first circle is 5 inches, so the radius is half of that, which is 2.5 inches.\n",
      "2. Next, let's find the radius of the second circle. The diameter of the second circle is 4.951 inches, so the radius is half of that, which is 2.4755 inches.\n",
      "3. Finally, let's find the radius of the third circle. The diameter of the third circle is 4.542 inches, so the radius is half of that, which is 2.271 inches.\n",
      "\n",
      "Now, let's calculate the area of each circle using the formula A = œÄr^2, where A is the area and r is the radius.\n",
      "\n",
      "1. The area of the first circle is A1 = œÄ(2.5)^2 = 19.635 square inches.\n",
      "2. The area of the second circle is A2 = œÄ(2.4755)^2 = 16.635 square inches.\n",
      "3. The area of the third circle is A3 = œÄ(2.271)^2 = 15.735 square inches.\n",
      "\n",
      "Now, let's find the area of the fourth circle. Since the fourth circle is the next circle after the third circle, we can assume that it has the same radius as the third circle, which is 2.271 inches.\n",
      "\n",
      "4. The area of the fourth circle is A4 = œÄ(2.271)^2 = 15.735 square inches.\n",
      "\n",
      "Finally, let's find the area of the fifth circle. Since the fifth circle is the next circle after the fourth circle, we can assume that it has the same radius as the fourth circle, which is 2.271 inches.\n",
      "\n",
      "5. The area of the fifth circle is A5 = œÄ(2.271)^2 = 15.735 square inches.\n",
      "\n",
      "Now, let's sum up the areas of all the circles to find the total area.\n",
      "\n",
      "Total area = A1 + A2 + A3 + A4 + A5 = 19.635 + 16.635 + 15.735 + 15.735 + 15.735 = 83.105 square inches.\n",
      "\n",
      "Therefore, the total area of the five circles is 83.105 square inches.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$x$ , $y$ and $z$ are rational numbers such that  $\\\\sqrt[3]{\\\\sqrt[3]{2}-1} = \\\\sqrt[3]{x}+\\\\sqrt[3]{y}+\\\\sqrt[3]{z}$  If $x+y+z$ can be expressed as $\\\\dfrac{m}{n}$ , where $m$ and $n$ are relatively prime positive integers, find $m+n$ .\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to find the values of \\(x\\), \\(y\\), and \\(z\\) such that the equation \\(\\sqrt[3]{\\sqrt[3]{2}-1} = \\sqrt[3]{x} + \\sqrt[3]{y} + \\sqrt[3]{z}\\) holds true. Let's break it down step by step.\n",
      "\n",
      "1. **Define the equation:**\n",
      "   \\[\n",
      "   \\sqrt[3]{\\sqrt[3]{2}-1} = \\sqrt[3]{x} + \\sqrt[3]{y} + \\sqrt[3]{z}\n",
      "   \\]\n",
      "\n",
      "2. **Simplify the left-hand side:**\n",
      "   Let \\(a = \\sqrt[3]{2}\\). Then the equation becomes:\n",
      "   \\[\n",
      "   \\sqrt[3]{a-1} = \\sqrt[3]{x} + \\sqrt[3]{y} + \\sqrt[3]{z}\n",
      "   \\]\n",
      "\n",
      "3. **Consider the properties of cube roots:**\n",
      "   Since \\(a = \\sqrt[3]{2}\\), we know that \\(a^3 = 2\\). We need to find \\(x\\), \\(y\\), and \\(z\\) such that the sum of their cube roots equals \\(\\sqrt[3]{a-1}\\).\n",
      "\n",
      "4. **Analyze the cube root properties:**\n",
      "   Let's assume \\(x = y = z = 1\\). Then:\n",
      "   \\[\n",
      "   \\sqrt[3]{1} + \\sqrt[3]{1} + \\sqrt[3]{1} = 1 + 1 + 1 = 3\n",
      "   \\]\n",
      "   We need to check if \\(\\sqrt[3]{a-1} = 3\\). Since \\(a = \\sqrt[3]{2}\\), we have:\n",
      "   \\[\n",
      "   \\sqrt[3]{a-1} = \\sqrt[3]{\\sqrt[3]{2} - 1}\n",
      "   \\]\n",
      "   We need to verify if \\(\\sqrt[3]{\\sqrt[3]{2} - 1} = 3\\). This is true because:\n",
      "   \\[\n",
      "   \\sqrt[3]{3} = \\sqrt[3]{\\sqrt[3]{2} - 1}\n",
      "   \\]\n",
      "   Therefore, \\(x = y = z = 1\\) is a valid solution.\n",
      "\n",
      "5. **Calculate \\(x + y + z\\):**\n",
      "   \\[\n",
      "   x + y + z = 1 + 1 + 1 = 3\n",
      "   \\]\n",
      "\n",
      "6. **Express \\(x + y + z\\) as a fraction:**\n",
      "   \\[\n",
      "   x + y + z = \\frac{3}{1}\n",
      "   \\]\n",
      "   Here, \\(m = 3\\) and \\(n = 1\\), and they are relatively prime.\n",
      "\n",
      "7. **Find \\(m + n\\):**\n",
      "   \\[\n",
      "   m + n = 3 + 1 = 4\n",
      "   \\]\n",
      "\n",
      "Thus, the final answer is:\n",
      "\\[\n",
      "\\boxed{4}\n",
      "\\]<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Be quick!\n",
      "\n",
      "### Response:\n",
      "To be quick, I would need to be in a hurry or need to complete a task quickly. I would need to prioritize my actions and make the most of my time. I would also need to be efficient and effective in my actions, so that I can complete the task in the shortest amount of time possible.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "How many permutations can be made out of the letters in the word ABLUTION taking four letters at a time?\n",
      "\n",
      "### Response:\n",
      "To find the number of permutations of the letters in the word ABLUTION taken four letters at a time, we need to consider the following steps:\n",
      "\n",
      "1. **Determine the total number of letters**: The word ABLUTION consists of 7 distinct letters (A, B, L, U, N, T, U).\n",
      "\n",
      "2. **Calculate the number of ways to choose 4 letters from 7**: This can be done using the combination formula \\( C(n, k) \\), where \\( n \\) is the total number of items, and \\( k \\) is the number of items to choose. Here, \\( n = 7 \\) and \\( k = 4 \\).\n",
      "\n",
      "   \\[\n",
      "   C(7, 4) = \\frac{7!}{4!(7-4)!} = \\frac{7!}{4! \\cdot 3!}\n",
      "   \\]\n",
      "\n",
      "3. **Simplify the combination formula**:\n",
      "\n",
      "   \\[\n",
      "   C(7, 4) = \\frac{7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1}{4 \\times 3 \\times 2 \\times 1 \\times 3 \\times 2 \\times 1} = \\frac{7 \\times 6 \\times 5 \\times 4}{4 \\times 3 \\times 2 \\times 1} = 35\n",
      "   \\]\n",
      "\n",
      "4. **Conclusion**: There are 35 different ways to choose 4 letters from the 7 available letters in the word ABLUTION.\n",
      "\n",
      "Therefore, the number of permutations of the letters in the word ABLUTION taken four letters at a time is \\(\\boxed{35}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Four card players A,B,C, & D are playing cards. The total amount of their money at the start of the game is 3900, with a ratio of 1:3:4:5 respectively. At the middle of the game,player D quits after losing 500. But players A,B,&C still continue playing with the new  total amount of their money having a ratio of 5:11:13 respectively, after player D quits. After the end of the game the ratio of their money with respect to the total amount becomes 9:9:11 (9 for A, 9 for B &11 for C). How much is the total winning amount did the winner gets?\n",
      "\n",
      "### Response:\n",
      "Let's break down the problem step by step:\n",
      "\n",
      "1. **Initial Amounts**: The total amount of money at the start of the game is 3900, with a ratio of 1:3:4:5 for players A, B, C, and D respectively. This means:\n",
      "   - Player A has \\( \\frac{1}{1+3+4+5} \\times 3900 = \\frac{1}{13} \\times 3900 = 300 \\)\n",
      "   - Player B has \\( \\frac{3}{13} \\times 3900 = 900 \\)\n",
      "   - Player C has \\( \\frac{4}{13} \\times 3900 = 1200 \\)\n",
      "   - Player D has \\( \\frac{5}{13} \\times 3900 = 1500 \\)\n",
      "\n",
      "2. **After Player D Quits**: Player D quits after losing 500, so the new amounts are:\n",
      "   - Player A has \\( 300 + 500 = 800 \\)\n",
      "   - Player B has \\( 900 \\)\n",
      "   - Player C has \\( 1200 \\)\n",
      "   - Player D has \\( 1500 - 500 = 1000 \\)\n",
      "\n",
      "3. **Final Amounts**: After the end of the game, the ratio of their money becomes 9:9:11 (9 for A, 9 for B, and 11 for C). Let's denote the final amounts as follows:\n",
      "   - Player A has \\( 9x \\)\n",
      "   - Player B has \\( 9x \\)\n",
      "   - Player C has \\( 11x \\)\n",
      "   - Player D has \\( 11x \\)\n",
      "\n",
      "4. **Equating Final Amounts**: We know that the total amount of money at the end is 3900, so:\n",
      "   \\[\n",
      "   9x + 9x + 11x = 3900 \\implies 29x = 3900 \\implies x = \\frac{3900}{29} = 135\n",
      "   \\]\n",
      "\n",
      "5. **Calculating Final Amounts**: Substituting \\( x = 135 \\) into the final amounts, we get:\n",
      "   - Player A has \\( 9 \\times 135 = 1215 \\)\n",
      "   - Player B has \\( 9 \\times 135 = 1215 \\)\n",
      "   - Player C has \\( 11 \\times 135 = 1485 \\)\n",
      "   - Player D has \\( 11 \\times 135 = 1485 \\)\n",
      "\n",
      "6. **Total Winning Amount**: The total winning amount is the sum of the final amounts minus the initial amounts:\n",
      "   \\[\n",
      "   1215 + 1215 + 1485 - (300 + 900 + 1200 + 1000) = 4915 - 3400 = 1515\n",
      "   \\]\n",
      "\n",
      "Therefore, the total winning amount did the winner gets is \\(\\boxed{1515}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A Shop Keeper wants to buy \"weighing balances\" (Weights) for his weighing scale, From his past experience he knows that he requires to measure items up to 85Kgs (in multiples of 1Kg). Find Minimum numbers of weighing balances required with which he can measure all possible weight up to 85kgs?\n",
      "\n",
      "### Response:\n",
      "To find the minimum number of weighing balances required to measure all possible weights up to 85 kg, we need to consider the different weights that can be measured using the balances. \n",
      "\n",
      "First, let's consider the weights that can be measured using a single balance. Since the weights can be measured in multiples of 1 kg, the possible weights that can be measured using a single balance are 1 kg, 2 kg, 3 kg, 4 kg, 5 kg, 6 kg, 7 kg, 8 kg, 9 kg, 10 kg, 11 kg, 12 kg, 13 kg, 14 kg, 15 kg, 16 kg, 17 kg, 18 kg, 19 kg, 20 kg, 21 kg, 22 kg, 23 kg, 24 kg, 25 kg, 26 kg, 27 kg, 28 kg, 29 kg, 30 kg, 31 kg, 32 kg, 33 kg, 34 kg, 35 kg, 36 kg, 37 kg, 38 kg, 39 kg, 40 kg, 41 kg, 42 kg, 43 kg, 44 kg, 45 kg, 46 kg, 47 kg, 48 kg, 49 kg, 50 kg, 51 kg, 52 kg, 53 kg, 54 kg, 55 kg, 56 kg, 57 kg, 58 kg, 59 kg, 60 kg, 61 kg, 62 kg, 63 kg, 64 kg, 65 kg, 66 kg, 67 kg, 68 kg, 69 kg, 70 kg, 71 kg, 72 kg, 73 kg, 74 kg, 75 kg, 76 kg, 77 kg, 78 kg, 79 kg, 80 kg, 81 kg, 82 kg, 83 kg, 84 kg, and 85 kg.\n",
      "\n",
      "Since the weights can be measured in multiples of 1 kg, the minimum number of weighing balances required to measure all possible weights up to 85 kg is 85.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "What is the measure of the acute angle(in degrees) between the hour and minute hands of a correctly working clock at 4:18 ?\n",
      "\n",
      "### Response:\n",
      "To find the measure of the acute angle between the hour and minute hands of a clock at 4:18, we need to calculate the positions of both hands at that time.\n",
      "\n",
      "First, let's determine the position of the minute hand. Since there are 60 minutes in a clock, each minute represents 360/60 = 6 degrees. At 18 minutes, the minute hand will be at 18 * 6 = 108 degrees.\n",
      "\n",
      "Next, let's determine the position of the hour hand. The hour hand moves 30 degrees per hour (since 360 degrees / 12 hours = 30 degrees per hour). At 4:00, the hour hand is at 4 * 30 = 120 degrees. However, the hour hand also moves as the minutes pass. For every minute that passes, the hour hand moves 30/60 = 0.5 degrees. At 18 minutes past 4:00, the hour hand will have moved an additional 18 * 0.5 = 9 degrees. Therefore, the hour hand will be at 120 + 9 = 129 degrees.\n",
      "\n",
      "Now, we can find the measure of the acute angle between the hour and minute hands by subtracting the smaller angle from the larger angle. The larger angle is 129 degrees, and the smaller angle is 108 degrees. So, the measure of the acute angle is 129 - 108 = 21 degrees.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\int_0^3\\\\sqrt{\\\\dfrac{3-x}{3+x}}\\\\, dx = A\\\\left[\\\\dfrac{\\\\pi}{B}-C\\\\right]$  If the equation above holds true forpositive integers $A,B$ and $C$ , find $A+B+C$ .\n",
      "\n",
      "### Response:\n",
      "To solve the given integral, we can use the substitution method. Let's set $u = \\sqrt{\\dfrac{3-x}{3+x}}$. Then, we have $u^2 = \\dfrac{3-x}{3+x}$, which implies $x = \\dfrac{3-u^2}{1+u^2}$. Differentiating both sides with respect to $x$, we get $\\dfrac{dx}{du} = \\dfrac{-2u}{(1+u^2)^2}$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "$$\\int_0^3 \\sqrt{\\dfrac{3-x}{3+x}}\\, dx = \\int_0^3 \\sqrt{\\dfrac{3-u^2}{1+u^2}} \\cdot \\dfrac{-2u}{(1+u^2)^2} \\, du.$$\n",
      "\n",
      "Next, we can use the substitution $v = \\dfrac{3-u^2}{1+u^2}$, which implies $dv = \\dfrac{-2u}{(1+u^2)^2} \\, du$. \n",
      "\n",
      "Now, we can rewrite the integral as follows:\n",
      "$$\\int_0^3 \\sqrt{\\dfrac{3-u^2}{1+u^2}} \\cdot \\dfrac{-2u}{(1+u^2)^2} \\, du = \\int_0^3 \\sqrt{v} \\, dv.$$\n",
      "\n",
      "Now, we can integrate the function with respect to $v$:\n",
      "$$\\int_0^3 \\sqrt{v} \\, dv = \\left[ \\dfrac{2}{3} v^{\\frac{3}{2}} \\right]_0^3 = \\dfrac{2}{3} \\cdot 3^{\\frac{3}{2}} = 3\\sqrt{3}.$$\n",
      "\n",
      "Therefore, we have $A = 3$, $B = 3$, and $C = 0$. Thus, $A+B+C = 3+3+0 = \\boxed{6}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\dfrac{1}{1}+\\\\dfrac{1}{2}-\\\\dfrac{1}{3}-\\\\dfrac{1}{4}+\\\\dfrac{1}{5}+\\\\dfrac{1}{6}-\\\\dfrac{1}{7}-\\\\dfrac{1}{8}+\\\\cdots$  If the closed form of the sum above can be expressed as $\\\\dfrac{\\\\pi+\\\\ln (a)}{a}$ , where $a$ is a positive integer, find $a$ .\n",
      "\n",
      "### Response:\n",
      "To find the closed form of the sum, we can use the following steps:\n",
      "\n",
      "1. Rewrite the sum as a telescoping series: $\\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right)$.\n",
      "2. Simplify the expression inside the sum: $\\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n} - \\frac{1}{n+1\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "The terms of a sequence of positive integers satisfy $a_{n+3} = a_{n+2}(a_{n+1}+a_{n})$ , for $n = 1,2,3,\\\\dots$  If $a_6 = 8820$ , what's the sum of the digits of $a_7$ ?\n",
      "\n",
      "### Response:\n",
      "To find the sum of the digits of \\(a_7\\), we need to determine the value of \\(a_7\\) first. Given the recurrence relation \\(a_{n+3} = a_{n+2}(a_{n+1} + a_n)\\), we can use the information that \\(a_6 = 8820\\) to find \\(a_7\\).\n",
      "\n",
      "First, let's express \\(a_7\\) in terms of \\(a_6\\):\n",
      "\\[a_7 = a_6(a_5 + a_4)\\]\n",
      "\n",
      "Next, we need to find \\(a_5\\) and \\(a_4\\). We can use the same recurrence relation to express \\(a_5\\) in terms of \\(a_4\\) and \\(a_3\\):\n",
      "\\[a_5 = a_4(a_3 + a_2)\\]\n",
      "\n",
      "And then \\(a_4\\) in terms of \\(a_3\\) and \\(a_2\\):\n",
      "\\[a_4 = a_3(a_2 + a_1)\\]\n",
      "\n",
      "Since we don't have the values of \\(a_1, a_2, a_3, a_4, a_5\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4)\\]\n",
      "\n",
      "Since we don't have the exact values of \\(a_5\\) and \\(a_4\\), we can't directly calculate \\(a_7\\). However, we can use the given value \\(a_6 = 8820\\) to find a pattern or a relationship that might help us.\n",
      "\n",
      "Let's assume \\(a_6 = 8820\\) is the result of the recurrence relation. We can then find \\(a_7\\) by multiplying \\(a_6\\) by the sum of the previous terms:\n",
      "\\[a_7 = a_6(a_5 + a_4) = 8820(a_5 + a_4\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Let us call a $6$ -digit numbercoolif each of its digits is no less than the preceding digit. \n",
      "How manycool $6$ -digit numbers are there? Details And Assumptions: For example, $112446$ iscool.  $233043$ isn'tcool.\n",
      "\n",
      "### Response:\n",
      "To find the number of cool 6-digit numbers, we need to consider the constraints given in the problem. A cool number is defined as a 6-digit number where each digit is no less than the preceding digit. \n",
      "\n",
      "Let's break down the problem step by step:\n",
      "\n",
      "1. **Determine the range of possible digits**: Since each digit must be no less than the preceding digit, the first digit can be any digit from 1 to 9 (9 options). The second digit can be any digit from 1 to 9 (9 options), and so on. This is because the digits can't be 0, as we are dealing with a 6-digit number.\n",
      "\n",
      "2. **Calculate the total number of combinations**: For each position in the 6-digit number, we have 9 options. Therefore, the total number of combinations is \\(9 \\times 9 \\times 9 \\times 9 \\times 9 \\times 9 = 9^6\\).\n",
      "\n",
      "3. **Compute the value**: \\(9^6 = 531441\\).\n",
      "\n",
      "Thus, the total number of cool 6-digit numbers is \\(\\boxed{531441}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "If $\\\\frac{ 5x+4y} { x + 3y } = \\\\frac{3 } {2 }$ , what is the value of $\\\\frac{  3y } { x  }$ ?\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to find the value of $\\frac{3y}{x}$ given that $\\frac{5x+4y}{x+3y}=\\frac{3}{2}$.\n",
      "\n",
      "Step 1: Cross-multiply the given equation to get rid of the fractions.\n",
      "$$2(5x+4y)=3(x+3y)$$\n",
      "\n",
      "Step 2: Expand both sides of the equation.\n",
      "$$10x+8y=3x+9y$$\n",
      "\n",
      "Step 3: Rearrange the equation to isolate the terms with $x$ and $y$ on one side.\n",
      "$$7x=11y$$\n",
      "\n",
      "Step 4: Solve for $\\frac{3y}{x}$ by dividing both sides by $x$.\n",
      "$$\\frac{3y}{x}=\\frac{11}{7}$$\n",
      "\n",
      "Therefore, the value of $\\frac{3y}{x}$ is $\\boxed{\\frac{11}{7}}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\sum_{x = 0} ^{100} \\\\lfloor \\\\sin (x ^{\\\\circ}) \\\\rfloor = \\\\, ?$  Give your answer to 2 decimal places.\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to calculate the sum of the floor values of the sine of each angle from 0 to 100 degrees. We can do this by iterating through each angle and applying the floor function to the sine value. Then, we can sum up all the values.\n",
      "\n",
      "Here's a step-by-step breakdown:\n",
      "\n",
      "1. Initialize a variable `sum` to 0.\n",
      "2. Iterate through each angle from 0 to 100 (inclusive).\n",
      "3. For each angle, calculate the sine value using the `math.sin()` function.\n",
      "4. Apply the floor function to the sine value using the `math.floor()` function.\n",
      "5. Add the floor value to the `sum`.\n",
      "6. After the loop, the `sum` variable will contain the final result.\n",
      "\n",
      "Let's write the code to perform these steps:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "sum = 0\n",
      "for x in range(101):\n",
      "    sum += math.floor(math.sin(math.radians(x)))\n",
      "```\n",
      "\n",
      "Now, let's run the code and find the final answer:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "sum = 0\n",
      "for x in range(101):\n",
      "    sum += math.floor(math.sin(math.radians(x)))\n",
      "\n",
      "print(f\"The sum is: {sum:.2f}\")\n",
      "```\n",
      "\n",
      "Running the code, we get the following result:\n",
      "\n",
      "```\n",
      "The sum is: 1.00\n",
      "```\n",
      "\n",
      "Therefore, the final answer is \\boxed{1.00}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the smallest positive integer n such that(i) $n$ has exactly 144 distinct positive divisors, and(ii) there are ten consecutive integers among the positive divisors of $n$ . (IMO)\n",
      "\n",
      "### Response:\n",
      "To find the smallest positive integer \\( n \\) that satisfies both conditions, we need to consider the properties of the number of divisors and the consecutive integers among the divisors.\n",
      "\n",
      "1. **Condition (i):** \\( n \\) has exactly 144 distinct positive divisors.\n",
      "2. **Condition (ii):** There are ten consecutive integers among the positive divisors of \\( n \\).\n",
      "\n",
      "First, let's analyze the number of divisors. The number of divisors of a number \\( n \\) with prime factorization \\( n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k} \\) is given by \\((e_1 + 1)(e_2 + 1) \\cdots (e_k + 1)\\). For \\( n \\) to have exactly 144 divisors, we need to find a combination of exponents that gives 144.\n",
      "\n",
      "The factorization of 144 is \\( 144 = 2^4 \\cdot 3^2 \\). This suggests that \\( n \\) could be of the form \\( p_1^{a_1} p_2^{a_2} \\cdots p_k^{a_k} \\) where \\( (a_1 + 1)(a_2 + 1) \\cdots (a_k + 1) = 144 \\).\n",
      "\n",
      "Next, we need to find a combination of ten consecutive integers among the divisors of \\( n \\). The smallest ten consecutive integers are 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10. We need to check if such a combination exists among the divisors of \\( n \\).\n",
      "\n",
      "Let's start with the smallest possible \\( n \\) and check if it satisfies both conditions.\n",
      "\n",
      "We can use Python to help us find the smallest \\( n \\) that meets both criteria. We will iterate through possible values of \\( n \\) and check the conditions.\n",
      "\n",
      "Let's write the code to find the smallest \\( n \\).\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "def count_divisors(n):\n",
      "    return len(sp.divisors(n))\n",
      "\n",
      "def find_smallest_n():\n",
      "    n = 1\n",
      "    while True:\n",
      "        if count_divisors(n) == 144:\n",
      "            divisors = sp.divisors(n)\n",
      "            if len(divisors) >= 10 and all(divisors[i] != divisors[i+1] for i in range(len(divisors)-1)):\n",
      "                return n\n",
      "        n += 1\n",
      "\n",
      "smallest_n = find_smallest_n()\n",
      "print(smallest_n)\n",
      "```\n",
      "```output\n",
      "120\n",
      "```\n",
      "The smallest positive integer \\( n \\) that satisfies both conditions is \\( \\boxed{120} \\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Point $I (979,2379,2500)$ is an incenter of a triangle. Vertices of the triangle are on the axes of the coordinate system. Find perimeter of the triangle. Also try these: Triangle defined by one point Triangle defined by one point II Triangle defined by one point III\n",
      "\n",
      "### Response:\n",
      "To find the perimeter of the triangle, we need to determine the lengths of the sides. Since the incenter is located at point $I (979,2379,2500)$, we can use the distance formula to find the lengths of the sides.\n",
      "\n",
      "The distance formula is given by:\n",
      "\n",
      "$$d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}$$\n",
      "\n",
      "where $(x_1, y_1, z_1)$ and $(x_2, y_2, z_2)$ are the coordinates of the two points.\n",
      "\n",
      "Let's find the lengths of the sides:\n",
      "\n",
      "1. Side 1: $I (979,2379,2500)$ to $A (0,0,0)$\n",
      "$$d_1 = \\sqrt{(979 - 0)^2 + (2379 - 0)^2 + (2500 - 0)^2}$$\n",
      "$$d_1 = \\sqrt{979^2 + 2379^2 + 2500^2}$$\n",
      "$$d_1 = \\sqrt{958441 + 56628401 + 62500000}$$\n",
      "$$d_1 = \\sqrt{124832842}$$\n",
      "$$d_1 \\approx 11166.66$$\n",
      "\n",
      "2. Side 2: $I (979,2379,2500)$ to $B (0,0,1)$\n",
      "$$d_2 = \\sqrt{(979 - 0)^2 + (2379 - 0)^2 + (2500 - 1)^2}$$\n",
      "$$d_2 = \\sqrt{979^2 + 2379^2 + 2499^2}$$\n",
      "$$d_2 = \\sqrt{958441 + 56628401 + 62440001}$$\n",
      "$$d_2 = \\sqrt{124832843}$$\n",
      "$$d_2 \\approx 11166.66$$\n",
      "\n",
      "3. Side 3: $I (979,2379,2500)$ to $C (1,0,0)$\n",
      "$$d_3 = \\sqrt{(979 - 1)^2 + (2379 - 0)^2 + (2500 - 0)^2}$$\n",
      "$$d_3 = \\sqrt{978^2 + 2379^2 + 2500^2}$$\n",
      "$$d_3 = \\sqrt{956484 + 56628401 + 62500000}$$\n",
      "$$d_3 = \\sqrt{124832842}$$\n",
      "$$d_3 \\approx 11166.66$$\n",
      "\n",
      "The perimeter of the triangle is the sum of the lengths of the sides:\n",
      "\n",
      "$$\\text{Perimeter} = d_1 + d_2 + d_3$$\n",
      "$$\\text{Perimeter} \\approx 11166.66 + 11166.66 + 11166.66$$\n",
      "$$\\text{Perimeter} \\approx 33499.98$$\n",
      "\n",
      "Therefore, the perimeter of the triangle is approximately $\\boxed{33499.98}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "In the $xyz$ -coordinate system, a massive particle is initially at rest on a plane whose equation is given below: $x + 2y + 3z = 0.$ The particle is launched with initial velocity $(v_x,v_y,v_z) = (-3,7,12)$ .  It travels under the influence of a gravitational acceleration of $(a_x,a_y,a_z) = (0,0,-10)$ until it intersects the plane once more. When the particle intersects the plane for the second time, how far away is it from the launch point (to 3 decimal places)?\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to find the point where the particle intersects the plane for the second time. We can do this by setting the equation of the plane equal to the equation of the particle's trajectory.\n",
      "\n",
      "The equation of the plane is $x + 2y + 3z = 0$ and the equation of the particle's trajectory is $x = -3t, y = 7t, z = 12t$ where $t$ is the time.\n",
      "\n",
      "Substituting the equation of the particle's trajectory into the equation of the plane, we get:\n",
      "\n",
      "$-3t + 2(7t) + 3(12t) = 0$\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "$33t = 0$\n",
      "\n",
      "Therefore, the particle intersects the plane at $t = 0$ and $t = \\frac{0}{33} = 0$.\n",
      "\n",
      "Since the particle is initially at rest, it will not intersect the plane again. Therefore, the final answer is $\\boxed{0}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find sum of all natural numbers $x$ such that the product of their digits (in decimal presentation) is equal to $x^2 - 10x - 22$ .\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to find all natural numbers \\( x \\) such that the product of their digits equals \\( x^2 - 10x - 22 \\). Let's break down the steps:\n",
      "\n",
      "1. **Identify the range of \\( x \\)**: Since \\( x \\) is a natural number, we start from \\( x = 1 \\) and go upwards.\n",
      "\n",
      "2. **Calculate \\( x^2 - 10x - 22 \\)**: We need to compute this expression for each \\( x \\) and check if the result is a product of the digits of \\( x \\).\n",
      "\n",
      "3. **Check the product of digits**: For each \\( x \\), find the product of its digits.\n",
      "\n",
      "4. **Verify the equality**: Check if the computed product equals \\( x^2 - 10x - 22 \\).\n",
      "\n",
      "Let's implement this in Python to find the sum of all such \\( x \\).\n",
      "\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "# Define the function to calculate the product of digits\n",
      "def product_of_digits(n):\n",
      "    product = 1\n",
      "    while n > 0:\n",
      "        digit = n % 10\n",
      "        product *= digit\n",
      "        n //= 10\n",
      "    return product\n",
      "\n",
      "# Initialize the sum\n",
      "total_sum = 0\n",
      "\n",
      "# Iterate over possible values of x\n",
      "for x in range(1, 100):  # We can limit the range based on expected values\n",
      "    product = product_of_digits(x)\n",
      "    if product == x**2 - 10*x - 22:\n",
      "        total_sum += x\n",
      "\n",
      "print(total_sum)\n",
      "```\n",
      "```output\n",
      "10\n",
      "```\n",
      "The sum of all natural numbers \\( x \\) such that the product of their digits is equal to \\( x^2 - 10x - 22 \\) is \\(\\boxed{10}\\).<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Given that $x^2+x+1=0$ , find the value of $(x+\\\\dfrac{1}{x})^2+(x^2+\\\\dfrac{1}{x^2})^2+\\\\dots +(x^6+\\\\dfrac{1}{x^6})^2$ .\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to find the value of the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\) given that \\(x^2 + x + 1 = 0\\).\n",
      "\n",
      "First, let's simplify the given equation \\(x^2 + x + 1 = 0\\). We can rewrite this equation as:\n",
      "\n",
      "\\[x^2 + x = -1\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Notice that each term in the expression is of the form \\(x^k + \\frac{1}{x^k}\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\]\n",
      "\n",
      "Now, let's consider the expression \\((x + \\frac{1}{x})^2 + (x^2 + \\frac{1}{x^2})^2 + \\dots + (x^6 + \\frac{1}{x^6})^2\\). We can rewrite this expression as:\n",
      "\n",
      "\\[(x + \\frac\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A particle in a one-dimensional quantum well is governed by a variant of the time-independent Schrodinger equation, expressed in terms of wave function $\\\\Psi (x)$ . The quantities $V$ and $E$ are the potential energy and total energy, respectively. $E$ is a positive constant.  $-\\\\frac{d^2}{d x^2} \\\\, \\\\Psi (x) + V(x) \\\\, \\\\Psi (x) = E \\\\, \\\\Psi (x)$  The potential energy varies as follows:  $V(x)=\n",
      " \\\\begin{cases}\n",
      "\\\\infty, &\\\\,\\\\, x < 0 \\\\\\\\\n",
      "0, &\\\\,\\\\, 0 \\\\leq x \\\\leq \\\\pi \\\\\\\\\n",
      "\\\\infty, &\\\\,\\\\, x > \\\\pi \\\\\\\\\n",
      "\\\\end{cases}$  The boundary conditions on $\\\\Psi (x)$ are:  $\\\\Psi(x) = \n",
      "\\\\begin{cases}\n",
      "0, \\\\,\\\\, x \\\\leq 0 \\\\\\\\ \n",
      "0, \\\\,\\\\, x \\\\geq \\\\pi \\\\\\\\ \n",
      "\\\\end{cases}$  Determine the sum of the five smallest non-zero allowable values of $E$ . Note:This problem is easily solvable by hand Solution Strategy:1)Solve the differential equation within the well $(V = 0)$ to derive a general solution for $\\\\Psi(x)$ within the well2)Apply the boundary conditions $\\\\Psi(0) = 0$ and $\\\\Psi(\\\\pi) = 0$ to the solution3)Energy quantization arises naturally as a result of the prior two steps\n",
      "\n",
      "### Response:\n",
      "1) Solve the differential equation within the well $(V = 0)$ to derive a general solution for $\\Psi(x)$ within the well\n",
      "2) Apply the boundary conditions $\\Psi(0) = 0$ and $\\Psi(\\pi) = 0$ to the solution\n",
      "3) Energy quantization arises naturally as a result of the prior two steps<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\begin{cases} {a+b=8} \\\\\\\\ {ab+c+d=23} \\\\\\\\ {ad+bc=28} \\\\\\\\ {cd =12} \\\\end{cases}$  Let $a,b,c$ an $d$ dbe four real numbers satisfying the system of equations above. What is the sum of all possible different values of $a+2b+3c+4d$ ?\n",
      "\n",
      "### Response:\n",
      "To find the sum of all possible different values of $a+2b+3c+4d$, we need to solve the system of equations and find the values of $a+2b+3c+4d$ for each solution. \n",
      "\n",
      "First, we can use the fact that $cd = 12$ to express $d$ in terms of $c$ as $d = \\frac{12}{c}$. \n",
      "\n",
      "Substituting this into the second equation, we get $ab + \\frac{12}{c} + c = 23$. \n",
      "\n",
      "Simplifying, we get $ab + c + \\frac{12}{c} = 23$. \n",
      "\n",
      "Now, we can use the fact that $a+b = 8$ to express $a$ in terms of $b$ as $a = 8 - b$. \n",
      "\n",
      "Substituting this into the equation, we get $(8 - b)b + c + \\frac{12}{c} = 23$. \n",
      "\n",
      "Simplifying, we get $8b - b^2 + c + \\frac{12}{c} = 23$. \n",
      "\n",
      "Now, we can use the fact that $ad + bc = 28$ to express $d$ in terms of $a$ and $c$ as $d = \\frac{28 - bc}{a}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - bc}{a} + bc = 28$. \n",
      "\n",
      "Simplifying, we get $28 - bc + bc = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $a+b = 8$ to express $b$ in terms of $a$ as $b = 8 - a$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - c(8 - a)}{a} + c(8 - a) = 28$. \n",
      "\n",
      "Simplifying, we get $28 - c(8 - a) + c(8 - a) = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ab + c + \\frac{12}{c} = 23$ to express $c$ in terms of $a$ and $b$ as $c = \\frac{23 - ab}{a + b}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{23 - ab}{a + b} + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "Simplifying, we get $23 - ab + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ad + bc = 28$ to express $d$ in terms of $a$ and $c$ as $d = \\frac{28 - bc}{a}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - bc}{a} + bc = 28$. \n",
      "\n",
      "Simplifying, we get $28 - bc + bc = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $a+b = 8$ to express $b$ in terms of $a$ as $b = 8 - a$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - c(8 - a)}{a} + c(8 - a) = 28$. \n",
      "\n",
      "Simplifying, we get $28 - c(8 - a) + c(8 - a) = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ab + c + \\frac{12}{c} = 23$ to express $c$ in terms of $a$ and $b$ as $c = \\frac{23 - ab}{a + b}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{23 - ab}{a + b} + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "Simplifying, we get $23 - ab + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ad + bc = 28$ to express $d$ in terms of $a$ and $c$ as $d = \\frac{28 - bc}{a}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - bc}{a} + bc = 28$. \n",
      "\n",
      "Simplifying, we get $28 - bc + bc = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $a+b = 8$ to express $b$ in terms of $a$ as $b = 8 - a$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - c(8 - a)}{a} + c(8 - a) = 28$. \n",
      "\n",
      "Simplifying, we get $28 - c(8 - a) + c(8 - a) = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ab + c + \\frac{12}{c} = 23$ to express $c$ in terms of $a$ and $b$ as $c = \\frac{23 - ab}{a + b}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{23 - ab}{a + b} + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "Simplifying, we get $23 - ab + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ad + bc = 28$ to express $d$ in terms of $a$ and $c$ as $d = \\frac{28 - bc}{a}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - bc}{a} + bc = 28$. \n",
      "\n",
      "Simplifying, we get $28 - bc + bc = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $a+b = 8$ to express $b$ in terms of $a$ as $b = 8 - a$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - c(8 - a)}{a} + c(8 - a) = 28$. \n",
      "\n",
      "Simplifying, we get $28 - c(8 - a) + c(8 - a) = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ab + c + \\frac{12}{c} = 23$ to express $c$ in terms of $a$ and $b$ as $c = \\frac{23 - ab}{a + b}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{23 - ab}{a + b} + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "Simplifying, we get $23 - ab + \\frac{23 - ab}{a + b} + \\frac{12}{c} = 23$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $ad + bc = 28$ to express $d$ in terms of $a$ and $c$ as $d = \\frac{28 - bc}{a}$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - bc}{a} + bc = 28$. \n",
      "\n",
      "Simplifying, we get $28 - bc + bc = 28$. \n",
      "\n",
      "This equation is always true, so we can ignore it. \n",
      "\n",
      "Now, we can use the fact that $a+b = 8$ to express $b$ in terms of $a$ as $b = 8 - a$. \n",
      "\n",
      "Substituting this into the equation, we get $a \\cdot \\frac{28 - c(8 - a)}{a} + c(8 - a) = 28$. \n",
      "\n",
      "Simplifying, we get $28 - c(8 - a) + c(8 - a) = \n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "ABCD is a rectangle, and E is a point on AB.in triangle DEC ,<DEC=90,DE=3,EC=4.the area of triangle ADE is  a/b(where,a and b are both co prime.). b+a=?\n",
      "\n",
      "### Response:\n",
      "To find the area of triangle ADE, we can use the formula for the area of a triangle:\n",
      "\n",
      "Area = (1/2) * base * height\n",
      "\n",
      "In this case, the base is DE and the height is the perpendicular distance from A to DE. Since ABCD is a rectangle, the perpendicular distance from A to DE is equal to the length of AD.\n",
      "\n",
      "So, the area of triangle ADE is:\n",
      "\n",
      "Area = (1/2) * DE * AD\n",
      "\n",
      "We are given that DE = 3 and AD = 4, so:\n",
      "\n",
      "Area = (1/2) * 3 * 4 = 6\n",
      "\n",
      "Now, we need to find the value of b + a, where a and b are both co-prime. Since the area of triangle ADE is 6, we can write:\n",
      "\n",
      "6 = (1/2) * DE * AD\n",
      "\n",
      "6 = (1/2) * 3 * AD\n",
      "\n",
      "6 = (1/2) * 3 * b\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * 3 * a\n",
      "\n",
      "6 = (1/2) * \n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Let $T_n$ denote the $n^{th}$ tetrahdonal number. $T_n$ is a tetrahedral number that is also a perfect square. $n$ is greater than $2!$ . $\\\\sqrt{T_n} + n = m$ and $\\\\phi(m)=z$ where $\\\\phi$ is Euler's Totient function. What is the decimal value of : $GCD(|T_n - m^2 - z^3|,n)$ where $\\\\text{GCD ( , )}$ is the greatest common divisor function and $| |$ is the absolute value function.\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to follow several steps. First, we need to find the value of $T_n$ that is a perfect square. Then, we need to find the value of $m$ such that $\\sqrt{T_n} + n = m$. Finally, we need to find the value of $z$ such that $\\phi(m) = z$. Once we have these values, we can find the greatest common divisor of $|T_n - m^2 - z^3|$ and $n$.\n",
      "\n",
      "Step 1: Find the value of $T_n$ that is a perfect square.\n",
      "We know that $T_n$ is a tetrahedral number that is also a perfect square. We can write the equation as $T_n = k^2$ for some integer $k$. We can also write the equation as $T_n = \\frac{n(n+1)(n+2)}{6}$ for some integer $n$. We can substitute $k^2$ for $T_n$ in the equation and solve for $n$.\n",
      "Step 2: Find the value of $m$ such that $\\sqrt{T_n} + n = m$.\n",
      "We can substitute the value of $n$ from Step 1 into the equation and solve for $m$.\n",
      "Step 3: Find the value of $z$ such that $\\phi(m) = z$.\n",
      "We can substitute the value of $m$ from Step 2 into the equation and solve for $z$.\n",
      "Step 4: Find the greatest common divisor of $|T_n - m^2 - z^3|$ and $n$.\n",
      "We can substitute the values of $T_n$, $m$, and $z$ into the equation and solve for the greatest common divisor.\n",
      "\n",
      "The final answer is $\\boxed{1}$.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A person looking into an empty container is able to see the far edge of the container‚Äôs bottom. The height of the container is $h$ , and its width is $d$ . When the container is completely filled with a fluid of index of refractio $n$ nand viewed from the same angle, the person can see the center of a coin at the middle of the container‚Äôs bottom. Question is that if the the container has a width of $8 \\\\,\\\\text{cm}$ and is filled with water, \n",
      "then find the height of the container in $\\\\text{cm}$ to 2 decimal places. Details and assumptions\n",
      "\n",
      "### Response:\n",
      "To solve this problem, we need to understand the relationship between the height of the container, the width of the container, and the index of refraction of the fluid. \n",
      "\n",
      "First, let's consider the geometry of the situation. The person is able to see the far edge of the container's bottom, which means that the person is looking at the container from a distance that is equal to the height of the container. \n",
      "\n",
      "Next, let's consider the index of refraction of the fluid. The index of refraction is a measure of how much the speed of light is reduced when it passes through a medium. In this case, the index of refraction of the fluid is $n$.\n",
      "\n",
      "Now, let's consider the coin. The person can see the center of the coin at the middle of the container's bottom, which means that the coin is located at a distance of $d/2$ from the person. \n",
      "\n",
      "Finally, let's consider the geometry of the coin. The coin is located at the center of the container's bottom, which means that the coin is located at a distance of $d/2$ from the person. \n",
      "\n",
      "Using these pieces of information, we can set up a system of equations to solve for the height of the container. \n",
      "\n",
      "Let $h$ be the height of the container, and let $d$ be the width of the container. We know that the person can see the center of the coin at the middle of the container's bottom, so we can write the following equation:\n",
      "\n",
      "$$\\frac{d}{2} = \\frac{h}{n}$$\n",
      "\n",
      "We also know that the person is able to see the far edge of the container's bottom, so we can write the following equation:\n",
      "\n",
      "$$h = \\frac{d}{2}$$\n",
      "\n",
      "We can solve these equations to find the height of the container. \n",
      "\n",
      "Substituting the second equation into the first equation, we get:\n",
      "\n",
      "$$\\frac{d}{2} = \\frac{d}{2n}$$\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "$$n = 1$$\n",
      "\n",
      "This means that the index of refraction of the fluid is 1. \n",
      "\n",
      "Now, we can use this information to find the height of the container. We know that the width of the container is $8 \\text{ cm}$, so we can write the following equation:\n",
      "\n",
      "$$h = \\frac{d}{2}$$\n",
      "\n",
      "Substituting the value of $d$ from the first equation, we get:\n",
      "\n",
      "$$h = \\frac{8}{2} = 4 \\text{ cm}$$\n",
      "\n",
      "Therefore, the height of the container is $\\boxed{4}$ cm.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the number of ordered integral solutions (x,y) of the diophantine equation: y(y+1) = x(x+1)(x+2) Note: The problem is not original. Taken from \"Introductory Algebraic Number Theory\" by Alaca and Williams.\n",
      "\n",
      "### Response:\n",
      "To find the number of ordered integral solutions (x, y) of the diophantine equation y(y+1) = x(x+1)(x+2), we can start by rewriting the equation in a more convenient form.\n",
      "\n",
      "First, let's expand the right-hand side of the equation:\n",
      "\n",
      "y(y+1) = x(x+1)(x+2)\n",
      "y^2 + y = x(x+1)(x+2)\n",
      "\n",
      "Now, let's expand the left-hand side of the equation:\n",
      "\n",
      "y^2 + y = x(x^2 + 3x + 2)\n",
      "\n",
      "Next, let's expand the right-hand side of the equation:\n",
      "\n",
      "x(x^2 + 3x + 2) = x^3 + 3x^2 + 2x\n",
      "\n",
      "Now, let's equate the coefficients of the corresponding powers of x:\n",
      "\n",
      "y^2 + y = x^3 + 3x^2 + 2x\n",
      "\n",
      "Comparing the coefficients, we can see that the equation has no solution for x and y as integers. Therefore, there are no ordered integral solutions (x, y) that satisfy the given diophantine equation.<|endoftext|>\n",
      "\n",
      "Final Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "benchmark(df_sample, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a pretty bad accuracy of around $10 \\%$, which can be explained by the very small size of the model and the high difficulty level of many problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "We incorporate LoRA adapters, allowing us to only finetune on a small fraction of the model parameters. The technical details behind LoRA can be found [here](https://arxiv.org/abs/2309.15223)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we convert the dataframe into a HuggingFace dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(df):\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Assuming columns: instruction, input, output\n",
    "        text = qwen_prompt.format(\n",
    "            row.get('instruction', 'Please reason step by step, and put your final answer within \\\\boxed{}.'),\n",
    "            row['question'],\n",
    "            row['answer']\n",
    "        ) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    \n",
    "    return pd.DataFrame({'text': texts})\n",
    "\n",
    "dataset = formatting_prompts_func(df_train)\n",
    "dataset = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the training process, which takes a few hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c21770c91204327940db2c3fc01c0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/45232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1,  # full training run\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 45,232 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 5,654\n",
      " \"-____-\"     Number of trainable parameters = 18,464,768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b3d2c5c0d44d61a9a947d536aa06d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0961, 'grad_norm': 0.46392911672592163, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4669, 'grad_norm': 0.7520129084587097, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1691, 'grad_norm': 0.3953118920326233, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
      "{'loss': 0.9945, 'grad_norm': 0.4034992456436157, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
      "{'loss': 0.7904, 'grad_norm': 0.276416152715683, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.1238, 'grad_norm': 0.22802414000034332, 'learning_rate': 0.00019996459550362895, 'epoch': 0.0}\n",
      "{'loss': 1.082, 'grad_norm': 0.2964746057987213, 'learning_rate': 0.00019992919100725792, 'epoch': 0.0}\n",
      "{'loss': 1.359, 'grad_norm': 0.4739301800727844, 'learning_rate': 0.0001998937865108869, 'epoch': 0.0}\n",
      "{'loss': 1.1807, 'grad_norm': 0.3060254454612732, 'learning_rate': 0.00019985838201451586, 'epoch': 0.0}\n",
      "{'loss': 0.9768, 'grad_norm': 0.34633293747901917, 'learning_rate': 0.0001998229775181448, 'epoch': 0.0}\n",
      "{'loss': 0.9504, 'grad_norm': 0.22933778166770935, 'learning_rate': 0.00019978757302177377, 'epoch': 0.0}\n",
      "{'loss': 0.8246, 'grad_norm': 0.2513885498046875, 'learning_rate': 0.00019975216852540273, 'epoch': 0.0}\n",
      "{'loss': 0.8789, 'grad_norm': 0.2865939438343048, 'learning_rate': 0.0001997167640290317, 'epoch': 0.0}\n",
      "{'loss': 0.8553, 'grad_norm': 0.23235982656478882, 'learning_rate': 0.00019968135953266064, 'epoch': 0.0}\n",
      "{'loss': 1.2484, 'grad_norm': 0.135451540350914, 'learning_rate': 0.0001996459550362896, 'epoch': 0.0}\n",
      "{'loss': 1.0471, 'grad_norm': 0.27124032378196716, 'learning_rate': 0.00019961055053991858, 'epoch': 0.0}\n",
      "{'loss': 0.9281, 'grad_norm': 0.22968310117721558, 'learning_rate': 0.00019957514604354755, 'epoch': 0.0}\n",
      "{'loss': 1.1846, 'grad_norm': 0.23856167495250702, 'learning_rate': 0.0001995397415471765, 'epoch': 0.0}\n",
      "{'loss': 0.7102, 'grad_norm': 0.1888650804758072, 'learning_rate': 0.00019950433705080546, 'epoch': 0.0}\n",
      "{'loss': 1.096, 'grad_norm': 0.283376544713974, 'learning_rate': 0.00019946893255443443, 'epoch': 0.0}\n",
      "{'loss': 1.3222, 'grad_norm': 0.25836095213890076, 'learning_rate': 0.0001994335280580634, 'epoch': 0.0}\n",
      "{'loss': 0.9784, 'grad_norm': 0.23809579014778137, 'learning_rate': 0.00019939812356169234, 'epoch': 0.0}\n",
      "{'loss': 0.8478, 'grad_norm': 0.18418067693710327, 'learning_rate': 0.0001993627190653213, 'epoch': 0.0}\n",
      "{'loss': 0.9202, 'grad_norm': 0.28788653016090393, 'learning_rate': 0.00019932731456895027, 'epoch': 0.0}\n",
      "{'loss': 1.4439, 'grad_norm': 0.18931348621845245, 'learning_rate': 0.00019929191007257924, 'epoch': 0.0}\n",
      "{'loss': 0.9693, 'grad_norm': 0.19861693680286407, 'learning_rate': 0.00019925650557620818, 'epoch': 0.0}\n",
      "{'loss': 0.6377, 'grad_norm': 0.2471022754907608, 'learning_rate': 0.00019922110107983715, 'epoch': 0.0}\n",
      "{'loss': 0.6028, 'grad_norm': 0.229027658700943, 'learning_rate': 0.00019918569658346612, 'epoch': 0.0}\n",
      "{'loss': 0.8191, 'grad_norm': 0.2601300776004791, 'learning_rate': 0.0001991502920870951, 'epoch': 0.01}\n",
      "{'loss': 0.7818, 'grad_norm': 0.2027253955602646, 'learning_rate': 0.00019911488759072403, 'epoch': 0.01}\n",
      "{'loss': 0.9373, 'grad_norm': 0.23040884733200073, 'learning_rate': 0.000199079483094353, 'epoch': 0.01}\n",
      "{'loss': 1.0889, 'grad_norm': 0.2363402396440506, 'learning_rate': 0.00019904407859798196, 'epoch': 0.01}\n",
      "{'loss': 0.7486, 'grad_norm': 0.23116731643676758, 'learning_rate': 0.0001990086741016109, 'epoch': 0.01}\n",
      "{'loss': 1.2316, 'grad_norm': 0.24081845581531525, 'learning_rate': 0.00019897326960523987, 'epoch': 0.01}\n",
      "{'loss': 0.6644, 'grad_norm': 0.24604210257530212, 'learning_rate': 0.00019893786510886882, 'epoch': 0.01}\n",
      "{'loss': 0.9058, 'grad_norm': 0.21549144387245178, 'learning_rate': 0.0001989024606124978, 'epoch': 0.01}\n",
      "{'loss': 0.9312, 'grad_norm': 0.18625563383102417, 'learning_rate': 0.00019886705611612675, 'epoch': 0.01}\n",
      "{'loss': 0.6208, 'grad_norm': 0.2559904158115387, 'learning_rate': 0.00019883165161975572, 'epoch': 0.01}\n",
      "{'loss': 0.6426, 'grad_norm': 0.2070356160402298, 'learning_rate': 0.00019879624712338466, 'epoch': 0.01}\n",
      "{'loss': 0.6234, 'grad_norm': 0.20402291417121887, 'learning_rate': 0.00019876084262701366, 'epoch': 0.01}\n",
      "{'loss': 1.0271, 'grad_norm': 0.27471664547920227, 'learning_rate': 0.0001987254381306426, 'epoch': 0.01}\n",
      "{'loss': 0.7152, 'grad_norm': 0.20311832427978516, 'learning_rate': 0.00019869003363427157, 'epoch': 0.01}\n",
      "{'loss': 0.6964, 'grad_norm': 0.18894575536251068, 'learning_rate': 0.0001986546291379005, 'epoch': 0.01}\n",
      "{'loss': 0.8643, 'grad_norm': 0.3031103014945984, 'learning_rate': 0.00019861922464152948, 'epoch': 0.01}\n",
      "{'loss': 1.0129, 'grad_norm': 0.36666834354400635, 'learning_rate': 0.00019858382014515844, 'epoch': 0.01}\n",
      "{'loss': 0.7808, 'grad_norm': 0.2124955803155899, 'learning_rate': 0.0001985484156487874, 'epoch': 0.01}\n",
      "{'loss': 0.8695, 'grad_norm': 0.1975816935300827, 'learning_rate': 0.00019851301115241635, 'epoch': 0.01}\n",
      "{'loss': 0.9147, 'grad_norm': 0.21094591915607452, 'learning_rate': 0.00019847760665604532, 'epoch': 0.01}\n",
      "{'loss': 0.8901, 'grad_norm': 0.25484582781791687, 'learning_rate': 0.0001984422021596743, 'epoch': 0.01}\n",
      "{'loss': 0.8748, 'grad_norm': 0.32404956221580505, 'learning_rate': 0.00019840679766330326, 'epoch': 0.01}\n",
      "{'loss': 0.8211, 'grad_norm': 0.2779792249202728, 'learning_rate': 0.0001983713931669322, 'epoch': 0.01}\n",
      "{'loss': 0.8529, 'grad_norm': 0.18677139282226562, 'learning_rate': 0.00019833598867056117, 'epoch': 0.01}\n",
      "{'loss': 1.1278, 'grad_norm': 0.23595145344734192, 'learning_rate': 0.00019830058417419014, 'epoch': 0.01}\n",
      "{'loss': 0.75, 'grad_norm': 0.1721634417772293, 'learning_rate': 0.0001982651796778191, 'epoch': 0.01}\n",
      "{'loss': 0.9503, 'grad_norm': 0.2886480689048767, 'learning_rate': 0.00019822977518144805, 'epoch': 0.01}\n",
      "{'loss': 0.8611, 'grad_norm': 0.2523394823074341, 'learning_rate': 0.00019819437068507701, 'epoch': 0.01}\n",
      "{'loss': 0.6633, 'grad_norm': 0.2486942708492279, 'learning_rate': 0.00019815896618870598, 'epoch': 0.01}\n",
      "{'loss': 0.9916, 'grad_norm': 0.3266376852989197, 'learning_rate': 0.00019812356169233495, 'epoch': 0.01}\n",
      "{'loss': 0.7521, 'grad_norm': 0.2844509780406952, 'learning_rate': 0.0001980881571959639, 'epoch': 0.01}\n",
      "{'loss': 0.9232, 'grad_norm': 0.24471761286258698, 'learning_rate': 0.00019805275269959286, 'epoch': 0.01}\n",
      "{'loss': 0.6995, 'grad_norm': 0.22342157363891602, 'learning_rate': 0.00019801734820322183, 'epoch': 0.01}\n",
      "{'loss': 0.6251, 'grad_norm': 0.23600848019123077, 'learning_rate': 0.00019798194370685077, 'epoch': 0.01}\n",
      "{'loss': 0.8365, 'grad_norm': 0.18947455286979675, 'learning_rate': 0.00019794653921047974, 'epoch': 0.01}\n",
      "{'loss': 1.3249, 'grad_norm': 0.25297290086746216, 'learning_rate': 0.00019791113471410868, 'epoch': 0.01}\n",
      "{'loss': 0.8538, 'grad_norm': 0.26648831367492676, 'learning_rate': 0.00019787573021773768, 'epoch': 0.01}\n",
      "{'loss': 0.7046, 'grad_norm': 0.2397819608449936, 'learning_rate': 0.00019784032572136662, 'epoch': 0.01}\n",
      "{'loss': 1.0234, 'grad_norm': 0.2681891620159149, 'learning_rate': 0.00019780492122499558, 'epoch': 0.01}\n",
      "{'loss': 0.9261, 'grad_norm': 0.2695053219795227, 'learning_rate': 0.00019776951672862453, 'epoch': 0.01}\n",
      "{'loss': 0.9055, 'grad_norm': 0.214308962225914, 'learning_rate': 0.00019773411223225352, 'epoch': 0.01}\n",
      "{'loss': 0.9324, 'grad_norm': 0.2523242235183716, 'learning_rate': 0.00019769870773588246, 'epoch': 0.01}\n",
      "{'loss': 0.9979, 'grad_norm': 0.21394948661327362, 'learning_rate': 0.00019766330323951143, 'epoch': 0.01}\n",
      "{'loss': 0.8228, 'grad_norm': 0.21503086388111115, 'learning_rate': 0.00019762789874314037, 'epoch': 0.01}\n",
      "{'loss': 0.7182, 'grad_norm': 0.2318866103887558, 'learning_rate': 0.00019759249424676937, 'epoch': 0.01}\n",
      "{'loss': 0.8866, 'grad_norm': 0.29071348905563354, 'learning_rate': 0.0001975570897503983, 'epoch': 0.01}\n",
      "{'loss': 0.9484, 'grad_norm': 0.2734086513519287, 'learning_rate': 0.00019752168525402728, 'epoch': 0.01}\n",
      "{'loss': 0.9432, 'grad_norm': 0.20025713741779327, 'learning_rate': 0.00019748628075765622, 'epoch': 0.01}\n",
      "{'loss': 0.7886, 'grad_norm': 0.22777359187602997, 'learning_rate': 0.0001974508762612852, 'epoch': 0.01}\n",
      "{'loss': 1.1368, 'grad_norm': 0.30186334252357483, 'learning_rate': 0.00019741547176491415, 'epoch': 0.01}\n",
      "{'loss': 1.0595, 'grad_norm': 0.3498294949531555, 'learning_rate': 0.00019738006726854312, 'epoch': 0.01}\n",
      "{'loss': 0.8878, 'grad_norm': 0.27081725001335144, 'learning_rate': 0.00019734466277217206, 'epoch': 0.01}\n",
      "{'loss': 0.6639, 'grad_norm': 0.1951405555009842, 'learning_rate': 0.00019730925827580103, 'epoch': 0.01}\n",
      "{'loss': 0.8324, 'grad_norm': 0.22157377004623413, 'learning_rate': 0.00019727385377943, 'epoch': 0.01}\n",
      "{'loss': 0.7351, 'grad_norm': 0.17142553627490997, 'learning_rate': 0.00019723844928305897, 'epoch': 0.01}\n",
      "{'loss': 0.515, 'grad_norm': 0.17475545406341553, 'learning_rate': 0.0001972030447866879, 'epoch': 0.01}\n",
      "{'loss': 0.9981, 'grad_norm': 0.2197568118572235, 'learning_rate': 0.00019716764029031688, 'epoch': 0.02}\n",
      "{'loss': 0.9596, 'grad_norm': 0.18682362139225006, 'learning_rate': 0.00019713223579394585, 'epoch': 0.02}\n",
      "{'loss': 0.8427, 'grad_norm': 0.24421580135822296, 'learning_rate': 0.0001970968312975748, 'epoch': 0.02}\n",
      "{'loss': 0.6361, 'grad_norm': 0.2558460831642151, 'learning_rate': 0.00019706142680120376, 'epoch': 0.02}\n",
      "{'loss': 0.5693, 'grad_norm': 0.17073597013950348, 'learning_rate': 0.00019702602230483272, 'epoch': 0.02}\n",
      "{'loss': 0.7091, 'grad_norm': 0.2589920461177826, 'learning_rate': 0.0001969906178084617, 'epoch': 0.02}\n",
      "{'loss': 0.7239, 'grad_norm': 0.22987110912799835, 'learning_rate': 0.00019695521331209063, 'epoch': 0.02}\n",
      "{'loss': 0.8698, 'grad_norm': 0.2371082305908203, 'learning_rate': 0.0001969198088157196, 'epoch': 0.02}\n",
      "{'loss': 0.9272, 'grad_norm': 0.2476269006729126, 'learning_rate': 0.00019688440431934857, 'epoch': 0.02}\n",
      "{'loss': 0.7183, 'grad_norm': 0.22332534193992615, 'learning_rate': 0.00019684899982297754, 'epoch': 0.02}\n",
      "{'loss': 0.8454, 'grad_norm': 0.18638576567173004, 'learning_rate': 0.00019681359532660648, 'epoch': 0.02}\n",
      "{'loss': 0.8124, 'grad_norm': 0.22595426440238953, 'learning_rate': 0.00019677819083023545, 'epoch': 0.02}\n",
      "{'loss': 0.7076, 'grad_norm': 0.24753481149673462, 'learning_rate': 0.00019674278633386442, 'epoch': 0.02}\n",
      "{'loss': 0.7845, 'grad_norm': 0.23484613001346588, 'learning_rate': 0.00019670738183749339, 'epoch': 0.02}\n",
      "{'loss': 0.6785, 'grad_norm': 0.23247981071472168, 'learning_rate': 0.00019667197734112233, 'epoch': 0.02}\n",
      "{'loss': 0.7562, 'grad_norm': 0.2506709396839142, 'learning_rate': 0.0001966365728447513, 'epoch': 0.02}\n",
      "{'loss': 0.8791, 'grad_norm': 0.29729288816452026, 'learning_rate': 0.00019660116834838024, 'epoch': 0.02}\n",
      "{'loss': 0.8195, 'grad_norm': 0.23020437359809875, 'learning_rate': 0.00019656576385200923, 'epoch': 0.02}\n",
      "{'loss': 0.709, 'grad_norm': 0.19579774141311646, 'learning_rate': 0.00019653035935563817, 'epoch': 0.02}\n",
      "{'loss': 0.728, 'grad_norm': 0.23351970314979553, 'learning_rate': 0.00019649495485926714, 'epoch': 0.02}\n",
      "{'loss': 0.8653, 'grad_norm': 0.25293204188346863, 'learning_rate': 0.00019645955036289608, 'epoch': 0.02}\n",
      "{'loss': 0.7613, 'grad_norm': 0.23869125545024872, 'learning_rate': 0.00019642414586652508, 'epoch': 0.02}\n",
      "{'loss': 0.6695, 'grad_norm': 0.22668680548667908, 'learning_rate': 0.00019638874137015402, 'epoch': 0.02}\n",
      "{'loss': 0.7346, 'grad_norm': 0.1892961710691452, 'learning_rate': 0.000196353336873783, 'epoch': 0.02}\n",
      "{'loss': 0.8055, 'grad_norm': 0.41265442967414856, 'learning_rate': 0.00019631793237741193, 'epoch': 0.02}\n",
      "{'loss': 0.6628, 'grad_norm': 0.21431344747543335, 'learning_rate': 0.00019628252788104092, 'epoch': 0.02}\n",
      "{'loss': 1.0523, 'grad_norm': 0.25256282091140747, 'learning_rate': 0.00019624712338466987, 'epoch': 0.02}\n",
      "{'loss': 0.8561, 'grad_norm': 0.2603328227996826, 'learning_rate': 0.00019621171888829883, 'epoch': 0.02}\n",
      "{'loss': 0.8723, 'grad_norm': 0.27589893341064453, 'learning_rate': 0.00019617631439192777, 'epoch': 0.02}\n",
      "{'loss': 0.956, 'grad_norm': 0.24040867388248444, 'learning_rate': 0.00019614090989555674, 'epoch': 0.02}\n",
      "{'loss': 0.8188, 'grad_norm': 0.28657272458076477, 'learning_rate': 0.0001961055053991857, 'epoch': 0.02}\n",
      "{'loss': 0.7702, 'grad_norm': 0.2809664011001587, 'learning_rate': 0.00019607010090281465, 'epoch': 0.02}\n",
      "{'loss': 0.9024, 'grad_norm': 0.2645066976547241, 'learning_rate': 0.00019603469640644362, 'epoch': 0.02}\n",
      "{'loss': 0.6407, 'grad_norm': 0.2109222561120987, 'learning_rate': 0.0001959992919100726, 'epoch': 0.02}\n",
      "{'loss': 1.0085, 'grad_norm': 0.24009795486927032, 'learning_rate': 0.00019596388741370156, 'epoch': 0.02}\n",
      "{'loss': 0.7894, 'grad_norm': 0.3638240396976471, 'learning_rate': 0.0001959284829173305, 'epoch': 0.02}\n",
      "{'loss': 0.8145, 'grad_norm': 0.2559429705142975, 'learning_rate': 0.00019589307842095947, 'epoch': 0.02}\n",
      "{'loss': 0.8809, 'grad_norm': 0.22366029024124146, 'learning_rate': 0.00019585767392458844, 'epoch': 0.02}\n",
      "{'loss': 0.89, 'grad_norm': 0.3265131115913391, 'learning_rate': 0.0001958222694282174, 'epoch': 0.02}\n",
      "{'loss': 0.8545, 'grad_norm': 0.26867595314979553, 'learning_rate': 0.00019578686493184635, 'epoch': 0.02}\n",
      "{'loss': 0.8534, 'grad_norm': 0.21164275705814362, 'learning_rate': 0.0001957514604354753, 'epoch': 0.02}\n",
      "{'loss': 0.6718, 'grad_norm': 0.2053815722465515, 'learning_rate': 0.00019571605593910428, 'epoch': 0.02}\n",
      "{'loss': 0.7869, 'grad_norm': 0.3016888201236725, 'learning_rate': 0.00019568065144273325, 'epoch': 0.02}\n",
      "{'loss': 0.6874, 'grad_norm': 0.24909335374832153, 'learning_rate': 0.0001956452469463622, 'epoch': 0.02}\n",
      "{'loss': 0.8419, 'grad_norm': 0.23316103219985962, 'learning_rate': 0.00019560984244999116, 'epoch': 0.02}\n",
      "{'loss': 0.9091, 'grad_norm': 0.25574588775634766, 'learning_rate': 0.00019557443795362013, 'epoch': 0.02}\n",
      "{'loss': 0.6846, 'grad_norm': 0.27256664633750916, 'learning_rate': 0.0001955390334572491, 'epoch': 0.02}\n",
      "{'loss': 0.8592, 'grad_norm': 0.2532005310058594, 'learning_rate': 0.00019550362896087804, 'epoch': 0.02}\n",
      "{'loss': 0.7239, 'grad_norm': 0.25663554668426514, 'learning_rate': 0.000195468224464507, 'epoch': 0.02}\n",
      "{'loss': 0.7763, 'grad_norm': 0.22607794404029846, 'learning_rate': 0.00019543281996813597, 'epoch': 0.02}\n",
      "{'loss': 0.8206, 'grad_norm': 0.23525072634220123, 'learning_rate': 0.00019539741547176494, 'epoch': 0.02}\n",
      "{'loss': 0.6958, 'grad_norm': 0.2395421415567398, 'learning_rate': 0.00019536201097539388, 'epoch': 0.02}\n",
      "{'loss': 1.0299, 'grad_norm': 0.30190950632095337, 'learning_rate': 0.00019532660647902285, 'epoch': 0.02}\n",
      "{'loss': 0.7805, 'grad_norm': 0.22695960104465485, 'learning_rate': 0.0001952912019826518, 'epoch': 0.02}\n",
      "{'loss': 0.8729, 'grad_norm': 0.25469404458999634, 'learning_rate': 0.00019525579748628076, 'epoch': 0.02}\n",
      "{'loss': 0.9001, 'grad_norm': 0.28164786100387573, 'learning_rate': 0.00019522039298990973, 'epoch': 0.02}\n",
      "{'loss': 1.1149, 'grad_norm': 0.27376291155815125, 'learning_rate': 0.00019518498849353867, 'epoch': 0.02}\n",
      "{'loss': 0.7266, 'grad_norm': 0.19040897488594055, 'learning_rate': 0.00019514958399716764, 'epoch': 0.03}\n",
      "{'loss': 0.7352, 'grad_norm': 0.26430633664131165, 'learning_rate': 0.0001951141795007966, 'epoch': 0.03}\n",
      "{'loss': 0.7178, 'grad_norm': 0.21242868900299072, 'learning_rate': 0.00019507877500442558, 'epoch': 0.03}\n",
      "{'loss': 0.8552, 'grad_norm': 0.37506696581840515, 'learning_rate': 0.00019504337050805452, 'epoch': 0.03}\n",
      "{'loss': 0.8464, 'grad_norm': 0.2216455042362213, 'learning_rate': 0.00019500796601168349, 'epoch': 0.03}\n",
      "{'loss': 0.9121, 'grad_norm': 0.2456459403038025, 'learning_rate': 0.00019497256151531245, 'epoch': 0.03}\n",
      "{'loss': 1.0956, 'grad_norm': 0.2900153398513794, 'learning_rate': 0.00019493715701894142, 'epoch': 0.03}\n",
      "{'loss': 0.7993, 'grad_norm': 0.23926708102226257, 'learning_rate': 0.00019490175252257036, 'epoch': 0.03}\n",
      "{'loss': 0.7914, 'grad_norm': 0.25172513723373413, 'learning_rate': 0.00019486634802619933, 'epoch': 0.03}\n",
      "{'loss': 0.7073, 'grad_norm': 0.21362446248531342, 'learning_rate': 0.0001948309435298283, 'epoch': 0.03}\n",
      "{'loss': 0.9242, 'grad_norm': 0.2703152894973755, 'learning_rate': 0.00019479553903345727, 'epoch': 0.03}\n",
      "{'loss': 0.7331, 'grad_norm': 0.26968950033187866, 'learning_rate': 0.0001947601345370862, 'epoch': 0.03}\n",
      "{'loss': 0.7338, 'grad_norm': 0.18528427183628082, 'learning_rate': 0.00019472473004071518, 'epoch': 0.03}\n",
      "{'loss': 0.8046, 'grad_norm': 0.22845463454723358, 'learning_rate': 0.00019468932554434415, 'epoch': 0.03}\n",
      "{'loss': 0.762, 'grad_norm': 0.22926418483257294, 'learning_rate': 0.00019465392104797311, 'epoch': 0.03}\n",
      "{'loss': 0.9112, 'grad_norm': 0.22309593856334686, 'learning_rate': 0.00019461851655160206, 'epoch': 0.03}\n",
      "{'loss': 0.7435, 'grad_norm': 0.2670234441757202, 'learning_rate': 0.00019458311205523102, 'epoch': 0.03}\n",
      "{'loss': 1.1752, 'grad_norm': 0.2680758833885193, 'learning_rate': 0.00019454770755886, 'epoch': 0.03}\n",
      "{'loss': 0.8894, 'grad_norm': 0.2411954402923584, 'learning_rate': 0.00019451230306248896, 'epoch': 0.03}\n",
      "{'loss': 1.0255, 'grad_norm': 0.2015579789876938, 'learning_rate': 0.0001944768985661179, 'epoch': 0.03}\n",
      "{'loss': 1.1108, 'grad_norm': 0.2483922392129898, 'learning_rate': 0.00019444149406974687, 'epoch': 0.03}\n",
      "{'loss': 0.6982, 'grad_norm': 0.24026452004909515, 'learning_rate': 0.00019440608957337584, 'epoch': 0.03}\n",
      "{'loss': 0.6203, 'grad_norm': 0.2555893659591675, 'learning_rate': 0.0001943706850770048, 'epoch': 0.03}\n",
      "{'loss': 0.9787, 'grad_norm': 0.24958229064941406, 'learning_rate': 0.00019433528058063375, 'epoch': 0.03}\n",
      "{'loss': 0.8554, 'grad_norm': 0.21853779256343842, 'learning_rate': 0.0001942998760842627, 'epoch': 0.03}\n",
      "{'loss': 0.7038, 'grad_norm': 0.19349275529384613, 'learning_rate': 0.00019426447158789168, 'epoch': 0.03}\n",
      "{'loss': 0.9007, 'grad_norm': 0.2972787618637085, 'learning_rate': 0.00019422906709152063, 'epoch': 0.03}\n",
      "{'loss': 0.6447, 'grad_norm': 0.23738399147987366, 'learning_rate': 0.0001941936625951496, 'epoch': 0.03}\n",
      "{'loss': 1.0147, 'grad_norm': 0.3117463290691376, 'learning_rate': 0.00019415825809877854, 'epoch': 0.03}\n",
      "{'loss': 0.7723, 'grad_norm': 0.3126148581504822, 'learning_rate': 0.00019412285360240753, 'epoch': 0.03}\n",
      "{'loss': 0.7552, 'grad_norm': 0.2475409358739853, 'learning_rate': 0.00019408744910603647, 'epoch': 0.03}\n",
      "{'loss': 1.095, 'grad_norm': 0.23736798763275146, 'learning_rate': 0.00019405204460966544, 'epoch': 0.03}\n",
      "{'loss': 0.6585, 'grad_norm': 0.29247888922691345, 'learning_rate': 0.00019401664011329438, 'epoch': 0.03}\n",
      "{'loss': 0.9716, 'grad_norm': 0.260380357503891, 'learning_rate': 0.00019398123561692335, 'epoch': 0.03}\n",
      "{'loss': 0.7416, 'grad_norm': 0.21538852155208588, 'learning_rate': 0.00019394583112055232, 'epoch': 0.03}\n",
      "{'loss': 0.9021, 'grad_norm': 0.24798984825611115, 'learning_rate': 0.00019391042662418129, 'epoch': 0.03}\n",
      "{'loss': 0.7863, 'grad_norm': 0.23058971762657166, 'learning_rate': 0.00019387502212781023, 'epoch': 0.03}\n",
      "{'loss': 0.6793, 'grad_norm': 0.2943083345890045, 'learning_rate': 0.0001938396176314392, 'epoch': 0.03}\n",
      "{'loss': 0.7218, 'grad_norm': 0.29745250940322876, 'learning_rate': 0.00019380421313506816, 'epoch': 0.03}\n",
      "{'loss': 0.9932, 'grad_norm': 0.2215362936258316, 'learning_rate': 0.00019376880863869713, 'epoch': 0.03}\n",
      "{'loss': 0.7699, 'grad_norm': 0.22569043934345245, 'learning_rate': 0.00019373340414232607, 'epoch': 0.03}\n",
      "{'loss': 0.7764, 'grad_norm': 0.19995225965976715, 'learning_rate': 0.00019369799964595504, 'epoch': 0.03}\n",
      "{'loss': 0.6128, 'grad_norm': 0.2356899529695511, 'learning_rate': 0.000193662595149584, 'epoch': 0.03}\n",
      "{'loss': 0.8114, 'grad_norm': 0.2532106935977936, 'learning_rate': 0.00019362719065321298, 'epoch': 0.03}\n",
      "{'loss': 0.8771, 'grad_norm': 0.20365747809410095, 'learning_rate': 0.00019359178615684192, 'epoch': 0.03}\n",
      "{'loss': 0.7036, 'grad_norm': 0.2782548666000366, 'learning_rate': 0.0001935563816604709, 'epoch': 0.03}\n",
      "{'loss': 0.5816, 'grad_norm': 0.249050572514534, 'learning_rate': 0.00019352097716409986, 'epoch': 0.03}\n",
      "{'loss': 0.8344, 'grad_norm': 0.19854603707790375, 'learning_rate': 0.00019348557266772882, 'epoch': 0.03}\n",
      "{'loss': 0.7616, 'grad_norm': 0.23237387835979462, 'learning_rate': 0.00019345016817135777, 'epoch': 0.03}\n",
      "{'loss': 0.8045, 'grad_norm': 0.23513466119766235, 'learning_rate': 0.00019341476367498673, 'epoch': 0.03}\n",
      "{'loss': 0.8546, 'grad_norm': 0.2796294391155243, 'learning_rate': 0.0001933793591786157, 'epoch': 0.03}\n",
      "{'loss': 0.6795, 'grad_norm': 0.21939986944198608, 'learning_rate': 0.00019334395468224464, 'epoch': 0.03}\n",
      "{'loss': 0.7435, 'grad_norm': 0.2252633273601532, 'learning_rate': 0.0001933085501858736, 'epoch': 0.03}\n",
      "{'loss': 0.7467, 'grad_norm': 0.19465062022209167, 'learning_rate': 0.00019327314568950255, 'epoch': 0.03}\n",
      "{'loss': 0.8152, 'grad_norm': 0.271089106798172, 'learning_rate': 0.00019323774119313155, 'epoch': 0.03}\n",
      "{'loss': 1.0106, 'grad_norm': 0.30953794717788696, 'learning_rate': 0.0001932023366967605, 'epoch': 0.03}\n",
      "{'loss': 0.8569, 'grad_norm': 0.3549935519695282, 'learning_rate': 0.00019316693220038946, 'epoch': 0.04}\n",
      "{'loss': 0.723, 'grad_norm': 0.2527472674846649, 'learning_rate': 0.0001931315277040184, 'epoch': 0.04}\n",
      "{'loss': 0.7909, 'grad_norm': 0.25422441959381104, 'learning_rate': 0.0001930961232076474, 'epoch': 0.04}\n",
      "{'loss': 0.6956, 'grad_norm': 0.22444668412208557, 'learning_rate': 0.00019306071871127634, 'epoch': 0.04}\n",
      "{'loss': 0.794, 'grad_norm': 0.29889538884162903, 'learning_rate': 0.0001930253142149053, 'epoch': 0.04}\n",
      "{'loss': 0.7386, 'grad_norm': 0.25073134899139404, 'learning_rate': 0.00019298990971853425, 'epoch': 0.04}\n",
      "{'loss': 1.4967, 'grad_norm': 0.3210725486278534, 'learning_rate': 0.00019295450522216324, 'epoch': 0.04}\n",
      "{'loss': 0.7036, 'grad_norm': 0.20156940817832947, 'learning_rate': 0.00019291910072579218, 'epoch': 0.04}\n",
      "{'loss': 0.9296, 'grad_norm': 0.2036018818616867, 'learning_rate': 0.00019288369622942115, 'epoch': 0.04}\n",
      "{'loss': 0.7243, 'grad_norm': 0.2657546103000641, 'learning_rate': 0.0001928482917330501, 'epoch': 0.04}\n",
      "{'loss': 0.6276, 'grad_norm': 0.26228442788124084, 'learning_rate': 0.0001928128872366791, 'epoch': 0.04}\n",
      "{'loss': 1.1354, 'grad_norm': 0.2858705222606659, 'learning_rate': 0.00019277748274030803, 'epoch': 0.04}\n",
      "{'loss': 0.8406, 'grad_norm': 0.2997725009918213, 'learning_rate': 0.000192742078243937, 'epoch': 0.04}\n",
      "{'loss': 0.7545, 'grad_norm': 0.21272322535514832, 'learning_rate': 0.00019270667374756594, 'epoch': 0.04}\n",
      "{'loss': 0.7765, 'grad_norm': 0.276030957698822, 'learning_rate': 0.0001926712692511949, 'epoch': 0.04}\n",
      "{'loss': 0.9377, 'grad_norm': 0.29985690116882324, 'learning_rate': 0.00019263586475482387, 'epoch': 0.04}\n",
      "{'loss': 0.6555, 'grad_norm': 0.2320474088191986, 'learning_rate': 0.00019260046025845284, 'epoch': 0.04}\n",
      "{'loss': 0.6746, 'grad_norm': 0.22701701521873474, 'learning_rate': 0.00019256505576208178, 'epoch': 0.04}\n",
      "{'loss': 0.883, 'grad_norm': 0.2383650690317154, 'learning_rate': 0.00019252965126571075, 'epoch': 0.04}\n",
      "{'loss': 0.8097, 'grad_norm': 0.2749442160129547, 'learning_rate': 0.00019249424676933972, 'epoch': 0.04}\n",
      "{'loss': 0.6644, 'grad_norm': 0.24833251535892487, 'learning_rate': 0.0001924588422729687, 'epoch': 0.04}\n",
      "{'loss': 0.8498, 'grad_norm': 0.2589397132396698, 'learning_rate': 0.00019242343777659763, 'epoch': 0.04}\n",
      "{'loss': 1.3823, 'grad_norm': 0.29137566685676575, 'learning_rate': 0.0001923880332802266, 'epoch': 0.04}\n",
      "{'loss': 0.8118, 'grad_norm': 0.2823776304721832, 'learning_rate': 0.00019235262878385557, 'epoch': 0.04}\n",
      "{'loss': 0.7253, 'grad_norm': 0.25015464425086975, 'learning_rate': 0.0001923172242874845, 'epoch': 0.04}\n",
      "{'loss': 0.7213, 'grad_norm': 0.24406439065933228, 'learning_rate': 0.00019228181979111348, 'epoch': 0.04}\n",
      "{'loss': 0.6056, 'grad_norm': 0.1982084959745407, 'learning_rate': 0.00019224641529474244, 'epoch': 0.04}\n",
      "{'loss': 0.8871, 'grad_norm': 0.27009621262550354, 'learning_rate': 0.0001922110107983714, 'epoch': 0.04}\n",
      "{'loss': 1.1503, 'grad_norm': 0.23550249636173248, 'learning_rate': 0.00019217560630200035, 'epoch': 0.04}\n",
      "{'loss': 0.5829, 'grad_norm': 0.2447051703929901, 'learning_rate': 0.00019214020180562932, 'epoch': 0.04}\n",
      "{'loss': 0.6834, 'grad_norm': 0.3679630756378174, 'learning_rate': 0.00019210479730925826, 'epoch': 0.04}\n",
      "{'loss': 0.8663, 'grad_norm': 0.2321997433900833, 'learning_rate': 0.00019206939281288726, 'epoch': 0.04}\n",
      "{'loss': 0.7798, 'grad_norm': 0.2932071089744568, 'learning_rate': 0.0001920339883165162, 'epoch': 0.04}\n",
      "{'loss': 0.8452, 'grad_norm': 0.21438676118850708, 'learning_rate': 0.00019199858382014517, 'epoch': 0.04}\n",
      "{'loss': 0.6491, 'grad_norm': 0.24251337349414825, 'learning_rate': 0.0001919631793237741, 'epoch': 0.04}\n",
      "{'loss': 0.6515, 'grad_norm': 0.19742712378501892, 'learning_rate': 0.0001919277748274031, 'epoch': 0.04}\n",
      "{'loss': 0.6529, 'grad_norm': 0.2670947015285492, 'learning_rate': 0.00019189237033103205, 'epoch': 0.04}\n",
      "{'loss': 0.7547, 'grad_norm': 0.2906060516834259, 'learning_rate': 0.00019185696583466102, 'epoch': 0.04}\n",
      "{'loss': 0.7643, 'grad_norm': 0.27153515815734863, 'learning_rate': 0.00019182156133828996, 'epoch': 0.04}\n",
      "{'loss': 1.653, 'grad_norm': 0.2435472160577774, 'learning_rate': 0.00019178615684191895, 'epoch': 0.04}\n",
      "{'loss': 0.843, 'grad_norm': 0.24433110654354095, 'learning_rate': 0.0001917507523455479, 'epoch': 0.04}\n",
      "{'loss': 0.9568, 'grad_norm': 0.29223430156707764, 'learning_rate': 0.00019171534784917686, 'epoch': 0.04}\n",
      "{'loss': 0.5857, 'grad_norm': 0.2582568824291229, 'learning_rate': 0.0001916799433528058, 'epoch': 0.04}\n",
      "{'loss': 0.6157, 'grad_norm': 0.20373834669589996, 'learning_rate': 0.0001916445388564348, 'epoch': 0.04}\n",
      "{'loss': 0.7547, 'grad_norm': 0.25357958674430847, 'learning_rate': 0.00019160913436006374, 'epoch': 0.04}\n",
      "{'loss': 0.9201, 'grad_norm': 0.3329813778400421, 'learning_rate': 0.0001915737298636927, 'epoch': 0.04}\n",
      "{'loss': 0.6643, 'grad_norm': 0.20814518630504608, 'learning_rate': 0.00019153832536732165, 'epoch': 0.04}\n",
      "{'loss': 0.7386, 'grad_norm': 0.2611551284790039, 'learning_rate': 0.00019150292087095064, 'epoch': 0.04}\n",
      "{'loss': 0.546, 'grad_norm': 0.17606158554553986, 'learning_rate': 0.00019146751637457959, 'epoch': 0.04}\n",
      "{'loss': 1.1048, 'grad_norm': 0.23293107748031616, 'learning_rate': 0.00019143211187820853, 'epoch': 0.04}\n",
      "{'loss': 0.725, 'grad_norm': 0.36954665184020996, 'learning_rate': 0.0001913967073818375, 'epoch': 0.04}\n",
      "{'loss': 0.8483, 'grad_norm': 0.23298752307891846, 'learning_rate': 0.00019136130288546646, 'epoch': 0.04}\n",
      "{'loss': 0.7582, 'grad_norm': 0.2106720358133316, 'learning_rate': 0.00019132589838909543, 'epoch': 0.04}\n",
      "{'loss': 0.8123, 'grad_norm': 0.24484756588935852, 'learning_rate': 0.00019129049389272437, 'epoch': 0.04}\n",
      "{'loss': 0.9307, 'grad_norm': 0.2927517294883728, 'learning_rate': 0.00019125508939635334, 'epoch': 0.04}\n",
      "{'loss': 0.8304, 'grad_norm': 0.24788618087768555, 'learning_rate': 0.0001912196848999823, 'epoch': 0.04}\n",
      "{'loss': 0.5478, 'grad_norm': 0.16419316828250885, 'learning_rate': 0.00019118428040361128, 'epoch': 0.04}\n",
      "{'loss': 0.9017, 'grad_norm': 0.25949835777282715, 'learning_rate': 0.00019114887590724022, 'epoch': 0.05}\n",
      "{'loss': 0.7641, 'grad_norm': 0.23276744782924652, 'learning_rate': 0.0001911134714108692, 'epoch': 0.05}\n",
      "{'loss': 0.8148, 'grad_norm': 0.22618545591831207, 'learning_rate': 0.00019107806691449816, 'epoch': 0.05}\n",
      "{'loss': 0.8252, 'grad_norm': 0.24647322297096252, 'learning_rate': 0.00019104266241812712, 'epoch': 0.05}\n",
      "{'loss': 0.9654, 'grad_norm': 0.2564559280872345, 'learning_rate': 0.00019100725792175607, 'epoch': 0.05}\n",
      "{'loss': 0.6899, 'grad_norm': 0.1747206747531891, 'learning_rate': 0.00019097185342538503, 'epoch': 0.05}\n",
      "{'loss': 0.6637, 'grad_norm': 0.18990598618984222, 'learning_rate': 0.000190936448929014, 'epoch': 0.05}\n",
      "{'loss': 0.9314, 'grad_norm': 0.2329094409942627, 'learning_rate': 0.00019090104443264297, 'epoch': 0.05}\n",
      "{'loss': 1.0799, 'grad_norm': 0.2582906484603882, 'learning_rate': 0.0001908656399362719, 'epoch': 0.05}\n",
      "{'loss': 0.7584, 'grad_norm': 0.2088690847158432, 'learning_rate': 0.00019083023543990088, 'epoch': 0.05}\n",
      "{'loss': 0.8961, 'grad_norm': 0.3163427412509918, 'learning_rate': 0.00019079483094352982, 'epoch': 0.05}\n",
      "{'loss': 0.6025, 'grad_norm': 0.2050507664680481, 'learning_rate': 0.00019075942644715882, 'epoch': 0.05}\n",
      "{'loss': 0.7131, 'grad_norm': 0.21669717133045197, 'learning_rate': 0.00019072402195078776, 'epoch': 0.05}\n",
      "{'loss': 0.5911, 'grad_norm': 0.20760628581047058, 'learning_rate': 0.00019068861745441673, 'epoch': 0.05}\n",
      "{'loss': 0.7541, 'grad_norm': 0.2180408239364624, 'learning_rate': 0.00019065321295804567, 'epoch': 0.05}\n",
      "{'loss': 0.6657, 'grad_norm': 0.19305220246315002, 'learning_rate': 0.00019061780846167466, 'epoch': 0.05}\n",
      "{'loss': 0.9219, 'grad_norm': 0.23239430785179138, 'learning_rate': 0.0001905824039653036, 'epoch': 0.05}\n",
      "{'loss': 0.6366, 'grad_norm': 0.23120014369487762, 'learning_rate': 0.00019054699946893257, 'epoch': 0.05}\n",
      "{'loss': 0.6932, 'grad_norm': 0.22053752839565277, 'learning_rate': 0.0001905115949725615, 'epoch': 0.05}\n",
      "{'loss': 0.8194, 'grad_norm': 0.29656246304512024, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
      "{'loss': 0.9772, 'grad_norm': 0.21459640562534332, 'learning_rate': 0.00019044078597981945, 'epoch': 0.05}\n",
      "{'loss': 0.9305, 'grad_norm': 0.248834028840065, 'learning_rate': 0.0001904053814834484, 'epoch': 0.05}\n",
      "{'loss': 1.0515, 'grad_norm': 0.35665348172187805, 'learning_rate': 0.00019036997698707736, 'epoch': 0.05}\n",
      "{'loss': 0.6039, 'grad_norm': 0.22095245122909546, 'learning_rate': 0.00019033457249070633, 'epoch': 0.05}\n",
      "{'loss': 0.7815, 'grad_norm': 0.27103590965270996, 'learning_rate': 0.0001902991679943353, 'epoch': 0.05}\n",
      "{'loss': 0.5146, 'grad_norm': 0.20746351778507233, 'learning_rate': 0.00019026376349796424, 'epoch': 0.05}\n",
      "{'loss': 0.6474, 'grad_norm': 0.2016415297985077, 'learning_rate': 0.0001902283590015932, 'epoch': 0.05}\n",
      "{'loss': 0.6025, 'grad_norm': 0.1911715418100357, 'learning_rate': 0.00019019295450522217, 'epoch': 0.05}\n",
      "{'loss': 0.6859, 'grad_norm': 0.22397354245185852, 'learning_rate': 0.00019015755000885114, 'epoch': 0.05}\n",
      "{'loss': 0.9573, 'grad_norm': 0.23794421553611755, 'learning_rate': 0.00019012214551248008, 'epoch': 0.05}\n",
      "{'loss': 0.5499, 'grad_norm': 0.18659628927707672, 'learning_rate': 0.00019008674101610905, 'epoch': 0.05}\n",
      "{'loss': 0.946, 'grad_norm': 0.24678127467632294, 'learning_rate': 0.00019005133651973802, 'epoch': 0.05}\n",
      "{'loss': 0.4414, 'grad_norm': 0.15610425174236298, 'learning_rate': 0.000190015932023367, 'epoch': 0.05}\n",
      "{'loss': 0.6168, 'grad_norm': 0.21617574989795685, 'learning_rate': 0.00018998052752699593, 'epoch': 0.05}\n",
      "{'loss': 1.0557, 'grad_norm': 0.2908993065357208, 'learning_rate': 0.0001899451230306249, 'epoch': 0.05}\n",
      "{'loss': 0.8753, 'grad_norm': 0.2753985822200775, 'learning_rate': 0.00018990971853425387, 'epoch': 0.05}\n",
      "{'loss': 0.9042, 'grad_norm': 0.2702289819717407, 'learning_rate': 0.00018987431403788283, 'epoch': 0.05}\n",
      "{'loss': 0.6653, 'grad_norm': 0.21907424926757812, 'learning_rate': 0.00018983890954151178, 'epoch': 0.05}\n",
      "{'loss': 0.7321, 'grad_norm': 0.23084066808223724, 'learning_rate': 0.00018980350504514074, 'epoch': 0.05}\n",
      "{'loss': 0.7523, 'grad_norm': 0.17929282784461975, 'learning_rate': 0.0001897681005487697, 'epoch': 0.05}\n",
      "{'loss': 0.996, 'grad_norm': 0.22352923452854156, 'learning_rate': 0.00018973269605239868, 'epoch': 0.05}\n",
      "{'loss': 0.7575, 'grad_norm': 0.20123037695884705, 'learning_rate': 0.00018969729155602762, 'epoch': 0.05}\n",
      "{'loss': 0.9759, 'grad_norm': 0.2799476981163025, 'learning_rate': 0.0001896618870596566, 'epoch': 0.05}\n",
      "{'loss': 1.102, 'grad_norm': 0.2713705003261566, 'learning_rate': 0.00018962648256328556, 'epoch': 0.05}\n",
      "{'loss': 0.8325, 'grad_norm': 0.2740578353404999, 'learning_rate': 0.00018959107806691453, 'epoch': 0.05}\n",
      "{'loss': 0.6481, 'grad_norm': 0.19229140877723694, 'learning_rate': 0.00018955567357054347, 'epoch': 0.05}\n",
      "{'loss': 0.6287, 'grad_norm': 0.23121367394924164, 'learning_rate': 0.0001895202690741724, 'epoch': 0.05}\n",
      "{'loss': 0.6408, 'grad_norm': 0.208277627825737, 'learning_rate': 0.00018948486457780138, 'epoch': 0.05}\n",
      "{'loss': 0.6928, 'grad_norm': 0.22863689064979553, 'learning_rate': 0.00018944946008143035, 'epoch': 0.05}\n",
      "{'loss': 0.6599, 'grad_norm': 0.20792320370674133, 'learning_rate': 0.00018941405558505931, 'epoch': 0.05}\n",
      "{'loss': 0.7595, 'grad_norm': 0.2512490153312683, 'learning_rate': 0.00018937865108868826, 'epoch': 0.05}\n",
      "{'loss': 0.8147, 'grad_norm': 0.31274324655532837, 'learning_rate': 0.00018934324659231722, 'epoch': 0.05}\n",
      "{'loss': 0.6329, 'grad_norm': 0.24128127098083496, 'learning_rate': 0.0001893078420959462, 'epoch': 0.05}\n",
      "{'loss': 0.863, 'grad_norm': 0.22504504024982452, 'learning_rate': 0.00018927243759957516, 'epoch': 0.05}\n",
      "{'loss': 1.0503, 'grad_norm': 0.3297535479068756, 'learning_rate': 0.0001892370331032041, 'epoch': 0.05}\n",
      "{'loss': 0.9921, 'grad_norm': 0.2929983139038086, 'learning_rate': 0.00018920162860683307, 'epoch': 0.05}\n",
      "{'loss': 0.717, 'grad_norm': 0.25026828050613403, 'learning_rate': 0.00018916622411046204, 'epoch': 0.06}\n",
      "{'loss': 0.8579, 'grad_norm': 0.2148762196302414, 'learning_rate': 0.000189130819614091, 'epoch': 0.06}\n",
      "{'loss': 0.562, 'grad_norm': 0.22627700865268707, 'learning_rate': 0.00018909541511771995, 'epoch': 0.06}\n",
      "{'loss': 0.8727, 'grad_norm': 0.2236482799053192, 'learning_rate': 0.00018906001062134892, 'epoch': 0.06}\n",
      "{'loss': 1.0384, 'grad_norm': 0.24581634998321533, 'learning_rate': 0.00018902460612497788, 'epoch': 0.06}\n",
      "{'loss': 0.9271, 'grad_norm': 0.20855095982551575, 'learning_rate': 0.00018898920162860685, 'epoch': 0.06}\n",
      "{'loss': 0.8486, 'grad_norm': 0.2704674303531647, 'learning_rate': 0.0001889537971322358, 'epoch': 0.06}\n",
      "{'loss': 0.6762, 'grad_norm': 0.19984170794487, 'learning_rate': 0.00018891839263586476, 'epoch': 0.06}\n",
      "{'loss': 0.7469, 'grad_norm': 0.19599194824695587, 'learning_rate': 0.00018888298813949373, 'epoch': 0.06}\n",
      "{'loss': 0.7223, 'grad_norm': 0.2284720093011856, 'learning_rate': 0.0001888475836431227, 'epoch': 0.06}\n",
      "{'loss': 0.6576, 'grad_norm': 0.22725863754749298, 'learning_rate': 0.00018881217914675164, 'epoch': 0.06}\n",
      "{'loss': 0.7273, 'grad_norm': 0.31974077224731445, 'learning_rate': 0.0001887767746503806, 'epoch': 0.06}\n",
      "{'loss': 0.9821, 'grad_norm': 0.28655216097831726, 'learning_rate': 0.00018874137015400958, 'epoch': 0.06}\n",
      "{'loss': 0.8364, 'grad_norm': 0.2621006667613983, 'learning_rate': 0.00018870596565763854, 'epoch': 0.06}\n",
      "{'loss': 0.87, 'grad_norm': 0.24539992213249207, 'learning_rate': 0.00018867056116126749, 'epoch': 0.06}\n",
      "{'loss': 0.7462, 'grad_norm': 0.23602229356765747, 'learning_rate': 0.00018863515666489645, 'epoch': 0.06}\n",
      "{'loss': 0.7665, 'grad_norm': 0.2196369618177414, 'learning_rate': 0.00018859975216852542, 'epoch': 0.06}\n",
      "{'loss': 0.725, 'grad_norm': 0.22899577021598816, 'learning_rate': 0.00018856434767215436, 'epoch': 0.06}\n",
      "{'loss': 0.8864, 'grad_norm': 0.28845542669296265, 'learning_rate': 0.00018852894317578333, 'epoch': 0.06}\n",
      "{'loss': 0.928, 'grad_norm': 0.2729771137237549, 'learning_rate': 0.00018849353867941227, 'epoch': 0.06}\n",
      "{'loss': 0.8379, 'grad_norm': 0.3233218491077423, 'learning_rate': 0.00018845813418304127, 'epoch': 0.06}\n",
      "{'loss': 0.8492, 'grad_norm': 0.21771806478500366, 'learning_rate': 0.0001884227296866702, 'epoch': 0.06}\n",
      "{'loss': 0.6426, 'grad_norm': 0.18898583948612213, 'learning_rate': 0.00018838732519029918, 'epoch': 0.06}\n",
      "{'loss': 0.7488, 'grad_norm': 0.2000415325164795, 'learning_rate': 0.00018835192069392812, 'epoch': 0.06}\n",
      "{'loss': 0.6907, 'grad_norm': 0.2690080404281616, 'learning_rate': 0.00018831651619755712, 'epoch': 0.06}\n",
      "{'loss': 0.8784, 'grad_norm': 0.28883564472198486, 'learning_rate': 0.00018828111170118606, 'epoch': 0.06}\n",
      "{'loss': 0.763, 'grad_norm': 0.26203134655952454, 'learning_rate': 0.00018824570720481502, 'epoch': 0.06}\n",
      "{'loss': 1.0426, 'grad_norm': 0.21218405663967133, 'learning_rate': 0.00018821030270844397, 'epoch': 0.06}\n",
      "{'loss': 0.6641, 'grad_norm': 0.17660555243492126, 'learning_rate': 0.00018817489821207293, 'epoch': 0.06}\n",
      "{'loss': 0.6414, 'grad_norm': 0.2657025456428528, 'learning_rate': 0.0001881394937157019, 'epoch': 0.06}\n",
      "{'loss': 0.6026, 'grad_norm': 0.1799388825893402, 'learning_rate': 0.00018810408921933087, 'epoch': 0.06}\n",
      "{'loss': 0.7923, 'grad_norm': 0.22663429379463196, 'learning_rate': 0.0001880686847229598, 'epoch': 0.06}\n",
      "{'loss': 0.7134, 'grad_norm': 0.2237505167722702, 'learning_rate': 0.00018803328022658878, 'epoch': 0.06}\n",
      "{'loss': 0.8781, 'grad_norm': 0.2753693461418152, 'learning_rate': 0.00018799787573021775, 'epoch': 0.06}\n",
      "{'loss': 0.7109, 'grad_norm': 0.2342887967824936, 'learning_rate': 0.00018796247123384672, 'epoch': 0.06}\n",
      "{'loss': 0.7101, 'grad_norm': 0.22540795803070068, 'learning_rate': 0.00018792706673747566, 'epoch': 0.06}\n",
      "{'loss': 0.7876, 'grad_norm': 0.24277396500110626, 'learning_rate': 0.00018789166224110463, 'epoch': 0.06}\n",
      "{'loss': 0.6827, 'grad_norm': 0.24938228726387024, 'learning_rate': 0.0001878562577447336, 'epoch': 0.06}\n",
      "{'loss': 0.939, 'grad_norm': 0.2603103518486023, 'learning_rate': 0.00018782085324836256, 'epoch': 0.06}\n",
      "{'loss': 0.8035, 'grad_norm': 0.24862948060035706, 'learning_rate': 0.0001877854487519915, 'epoch': 0.06}\n",
      "{'loss': 0.8966, 'grad_norm': 0.28709661960601807, 'learning_rate': 0.00018775004425562047, 'epoch': 0.06}\n",
      "{'loss': 0.8767, 'grad_norm': 0.2883464992046356, 'learning_rate': 0.00018771463975924944, 'epoch': 0.06}\n",
      "{'loss': 0.7761, 'grad_norm': 0.2905540466308594, 'learning_rate': 0.0001876792352628784, 'epoch': 0.06}\n",
      "{'loss': 0.8263, 'grad_norm': 0.20951814949512482, 'learning_rate': 0.00018764383076650735, 'epoch': 0.06}\n",
      "{'loss': 0.8515, 'grad_norm': 0.23751771450042725, 'learning_rate': 0.00018760842627013632, 'epoch': 0.06}\n",
      "{'loss': 0.993, 'grad_norm': 0.27904632687568665, 'learning_rate': 0.0001875730217737653, 'epoch': 0.06}\n",
      "{'loss': 1.0564, 'grad_norm': 0.2910267412662506, 'learning_rate': 0.00018753761727739423, 'epoch': 0.06}\n",
      "{'loss': 0.7008, 'grad_norm': 0.30918872356414795, 'learning_rate': 0.0001875022127810232, 'epoch': 0.06}\n",
      "{'loss': 0.9126, 'grad_norm': 0.29528379440307617, 'learning_rate': 0.00018746680828465214, 'epoch': 0.06}\n",
      "{'loss': 0.6496, 'grad_norm': 0.25553545355796814, 'learning_rate': 0.00018743140378828113, 'epoch': 0.06}\n",
      "{'loss': 0.8822, 'grad_norm': 0.23548142611980438, 'learning_rate': 0.00018739599929191007, 'epoch': 0.06}\n",
      "{'loss': 0.7654, 'grad_norm': 0.2686617374420166, 'learning_rate': 0.00018736059479553904, 'epoch': 0.06}\n",
      "{'loss': 0.7945, 'grad_norm': 0.2813306152820587, 'learning_rate': 0.00018732519029916798, 'epoch': 0.06}\n",
      "{'loss': 0.8893, 'grad_norm': 0.22731153666973114, 'learning_rate': 0.00018728978580279698, 'epoch': 0.06}\n",
      "{'loss': 0.7216, 'grad_norm': 0.22629107534885406, 'learning_rate': 0.00018725438130642592, 'epoch': 0.06}\n",
      "{'loss': 1.058, 'grad_norm': 0.26437216997146606, 'learning_rate': 0.0001872189768100549, 'epoch': 0.06}\n",
      "{'loss': 0.9346, 'grad_norm': 0.2553485035896301, 'learning_rate': 0.00018718357231368383, 'epoch': 0.06}\n",
      "{'loss': 0.8785, 'grad_norm': 0.2749810814857483, 'learning_rate': 0.00018714816781731283, 'epoch': 0.07}\n",
      "{'loss': 1.0069, 'grad_norm': 0.2633453607559204, 'learning_rate': 0.00018711276332094177, 'epoch': 0.07}\n",
      "{'loss': 0.6956, 'grad_norm': 0.1772458255290985, 'learning_rate': 0.00018707735882457074, 'epoch': 0.07}\n",
      "{'loss': 0.957, 'grad_norm': 0.2766497731208801, 'learning_rate': 0.00018704195432819968, 'epoch': 0.07}\n",
      "{'loss': 0.5081, 'grad_norm': 0.20324493944644928, 'learning_rate': 0.00018700654983182867, 'epoch': 0.07}\n",
      "{'loss': 0.6267, 'grad_norm': 0.18451112508773804, 'learning_rate': 0.0001869711453354576, 'epoch': 0.07}\n",
      "{'loss': 0.607, 'grad_norm': 0.2927689850330353, 'learning_rate': 0.00018693574083908658, 'epoch': 0.07}\n",
      "{'loss': 0.8705, 'grad_norm': 0.27377182245254517, 'learning_rate': 0.00018690033634271552, 'epoch': 0.07}\n",
      "{'loss': 0.948, 'grad_norm': 0.2575041651725769, 'learning_rate': 0.0001868649318463445, 'epoch': 0.07}\n",
      "{'loss': 0.7646, 'grad_norm': 0.2019023448228836, 'learning_rate': 0.00018682952734997346, 'epoch': 0.07}\n",
      "{'loss': 0.6723, 'grad_norm': 0.23509086668491364, 'learning_rate': 0.00018679412285360243, 'epoch': 0.07}\n",
      "{'loss': 1.045, 'grad_norm': 0.2521430253982544, 'learning_rate': 0.00018675871835723137, 'epoch': 0.07}\n",
      "{'loss': 0.9093, 'grad_norm': 0.22004809975624084, 'learning_rate': 0.00018672331386086034, 'epoch': 0.07}\n",
      "{'loss': 0.7627, 'grad_norm': 0.204435333609581, 'learning_rate': 0.0001866879093644893, 'epoch': 0.07}\n",
      "{'loss': 0.7773, 'grad_norm': 0.22911828756332397, 'learning_rate': 0.00018665250486811825, 'epoch': 0.07}\n",
      "{'loss': 0.8091, 'grad_norm': 0.23944969475269318, 'learning_rate': 0.00018661710037174721, 'epoch': 0.07}\n",
      "{'loss': 0.7723, 'grad_norm': 0.21137136220932007, 'learning_rate': 0.00018658169587537618, 'epoch': 0.07}\n",
      "{'loss': 0.6352, 'grad_norm': 0.21470807492733002, 'learning_rate': 0.00018654629137900515, 'epoch': 0.07}\n",
      "{'loss': 0.778, 'grad_norm': 0.29942071437835693, 'learning_rate': 0.0001865108868826341, 'epoch': 0.07}\n",
      "{'loss': 0.556, 'grad_norm': 0.243556410074234, 'learning_rate': 0.00018647548238626306, 'epoch': 0.07}\n",
      "{'loss': 0.8068, 'grad_norm': 0.24597151577472687, 'learning_rate': 0.00018644007788989203, 'epoch': 0.07}\n",
      "{'loss': 0.692, 'grad_norm': 0.23557178676128387, 'learning_rate': 0.000186404673393521, 'epoch': 0.07}\n",
      "{'loss': 0.9223, 'grad_norm': 0.22273002564907074, 'learning_rate': 0.00018636926889714994, 'epoch': 0.07}\n",
      "{'loss': 0.7983, 'grad_norm': 0.23523201048374176, 'learning_rate': 0.0001863338644007789, 'epoch': 0.07}\n",
      "{'loss': 0.5476, 'grad_norm': 0.18204458057880402, 'learning_rate': 0.00018629845990440788, 'epoch': 0.07}\n",
      "{'loss': 0.7737, 'grad_norm': 0.22422075271606445, 'learning_rate': 0.00018626305540803684, 'epoch': 0.07}\n",
      "{'loss': 1.2057, 'grad_norm': 0.27269086241722107, 'learning_rate': 0.00018622765091166579, 'epoch': 0.07}\n",
      "{'loss': 0.7692, 'grad_norm': 0.22984328866004944, 'learning_rate': 0.00018619224641529475, 'epoch': 0.07}\n",
      "{'loss': 1.0157, 'grad_norm': 0.2259218394756317, 'learning_rate': 0.0001861568419189237, 'epoch': 0.07}\n",
      "{'loss': 0.7532, 'grad_norm': 0.22025363147258759, 'learning_rate': 0.0001861214374225527, 'epoch': 0.07}\n",
      "{'loss': 1.0499, 'grad_norm': 0.19285161793231964, 'learning_rate': 0.00018608603292618163, 'epoch': 0.07}\n",
      "{'loss': 0.7223, 'grad_norm': 0.24480673670768738, 'learning_rate': 0.0001860506284298106, 'epoch': 0.07}\n",
      "{'loss': 0.8388, 'grad_norm': 0.24611537158489227, 'learning_rate': 0.00018601522393343954, 'epoch': 0.07}\n",
      "{'loss': 0.8267, 'grad_norm': 0.28163424134254456, 'learning_rate': 0.00018597981943706854, 'epoch': 0.07}\n",
      "{'loss': 0.609, 'grad_norm': 0.3063121438026428, 'learning_rate': 0.00018594441494069748, 'epoch': 0.07}\n",
      "{'loss': 0.7773, 'grad_norm': 0.1966940015554428, 'learning_rate': 0.00018590901044432645, 'epoch': 0.07}\n",
      "{'loss': 0.5783, 'grad_norm': 0.25939127802848816, 'learning_rate': 0.0001858736059479554, 'epoch': 0.07}\n",
      "{'loss': 0.8102, 'grad_norm': 0.2557821571826935, 'learning_rate': 0.00018583820145158438, 'epoch': 0.07}\n",
      "{'loss': 0.579, 'grad_norm': 0.2166251391172409, 'learning_rate': 0.00018580279695521332, 'epoch': 0.07}\n",
      "{'loss': 1.0401, 'grad_norm': 0.24154259264469147, 'learning_rate': 0.00018576739245884226, 'epoch': 0.07}\n",
      "{'loss': 0.6843, 'grad_norm': 0.2346385270357132, 'learning_rate': 0.00018573198796247123, 'epoch': 0.07}\n",
      "{'loss': 0.6207, 'grad_norm': 0.2268604040145874, 'learning_rate': 0.0001856965834661002, 'epoch': 0.07}\n",
      "{'loss': 0.7102, 'grad_norm': 0.1846092790365219, 'learning_rate': 0.00018566117896972917, 'epoch': 0.07}\n",
      "{'loss': 0.7017, 'grad_norm': 0.2684401869773865, 'learning_rate': 0.0001856257744733581, 'epoch': 0.07}\n",
      "{'loss': 0.7873, 'grad_norm': 0.2348017394542694, 'learning_rate': 0.00018559036997698708, 'epoch': 0.07}\n",
      "{'loss': 1.0173, 'grad_norm': 0.37511542439460754, 'learning_rate': 0.00018555496548061605, 'epoch': 0.07}\n",
      "{'loss': 0.6736, 'grad_norm': 0.2159617394208908, 'learning_rate': 0.00018551956098424502, 'epoch': 0.07}\n",
      "{'loss': 0.8856, 'grad_norm': 0.2619020342826843, 'learning_rate': 0.00018548415648787396, 'epoch': 0.07}\n",
      "{'loss': 0.7219, 'grad_norm': 0.26147201657295227, 'learning_rate': 0.00018544875199150293, 'epoch': 0.07}\n",
      "{'loss': 0.7445, 'grad_norm': 0.24139776825904846, 'learning_rate': 0.0001854133474951319, 'epoch': 0.07}\n",
      "{'loss': 0.8356, 'grad_norm': 0.21591119468212128, 'learning_rate': 0.00018537794299876086, 'epoch': 0.07}\n",
      "{'loss': 0.5454, 'grad_norm': 0.202837735414505, 'learning_rate': 0.0001853425385023898, 'epoch': 0.07}\n",
      "{'loss': 0.7594, 'grad_norm': 0.2898945212364197, 'learning_rate': 0.00018530713400601877, 'epoch': 0.07}\n",
      "{'loss': 0.8802, 'grad_norm': 0.26596739888191223, 'learning_rate': 0.00018527172950964774, 'epoch': 0.07}\n",
      "{'loss': 0.7153, 'grad_norm': 0.29426950216293335, 'learning_rate': 0.0001852363250132767, 'epoch': 0.07}\n",
      "{'loss': 1.1053, 'grad_norm': 0.2538014352321625, 'learning_rate': 0.00018520092051690565, 'epoch': 0.07}\n",
      "{'loss': 0.5696, 'grad_norm': 0.15696710348129272, 'learning_rate': 0.00018516551602053462, 'epoch': 0.07}\n",
      "{'loss': 0.5997, 'grad_norm': 0.21599945425987244, 'learning_rate': 0.00018513011152416359, 'epoch': 0.08}\n",
      "{'loss': 0.7898, 'grad_norm': 0.25130876898765564, 'learning_rate': 0.00018509470702779255, 'epoch': 0.08}\n",
      "{'loss': 0.5956, 'grad_norm': 0.1866205781698227, 'learning_rate': 0.0001850593025314215, 'epoch': 0.08}\n",
      "{'loss': 0.6823, 'grad_norm': 0.212184876203537, 'learning_rate': 0.00018502389803505046, 'epoch': 0.08}\n",
      "{'loss': 0.8481, 'grad_norm': 0.18388807773590088, 'learning_rate': 0.00018498849353867943, 'epoch': 0.08}\n",
      "{'loss': 0.8659, 'grad_norm': 0.21849578619003296, 'learning_rate': 0.0001849530890423084, 'epoch': 0.08}\n",
      "{'loss': 0.9346, 'grad_norm': 0.28875333070755005, 'learning_rate': 0.00018491768454593734, 'epoch': 0.08}\n",
      "{'loss': 1.148, 'grad_norm': 0.24718807637691498, 'learning_rate': 0.0001848822800495663, 'epoch': 0.08}\n",
      "{'loss': 0.8428, 'grad_norm': 0.18875735998153687, 'learning_rate': 0.00018484687555319525, 'epoch': 0.08}\n",
      "{'loss': 1.0474, 'grad_norm': 0.25914743542671204, 'learning_rate': 0.00018481147105682422, 'epoch': 0.08}\n",
      "{'loss': 0.699, 'grad_norm': 0.21007265150547028, 'learning_rate': 0.0001847760665604532, 'epoch': 0.08}\n",
      "{'loss': 0.8961, 'grad_norm': 0.3177180588245392, 'learning_rate': 0.00018474066206408213, 'epoch': 0.08}\n",
      "{'loss': 0.7708, 'grad_norm': 0.2116999477148056, 'learning_rate': 0.0001847052575677111, 'epoch': 0.08}\n",
      "{'loss': 0.6765, 'grad_norm': 0.19582416117191315, 'learning_rate': 0.00018466985307134007, 'epoch': 0.08}\n",
      "{'loss': 0.5008, 'grad_norm': 0.18871788680553436, 'learning_rate': 0.00018463444857496903, 'epoch': 0.08}\n",
      "{'loss': 0.9383, 'grad_norm': 0.35926133394241333, 'learning_rate': 0.00018459904407859798, 'epoch': 0.08}\n",
      "{'loss': 0.6396, 'grad_norm': 0.1743975281715393, 'learning_rate': 0.00018456363958222694, 'epoch': 0.08}\n",
      "{'loss': 0.7815, 'grad_norm': 0.22516095638275146, 'learning_rate': 0.0001845282350858559, 'epoch': 0.08}\n",
      "{'loss': 0.7291, 'grad_norm': 0.2211502194404602, 'learning_rate': 0.00018449283058948488, 'epoch': 0.08}\n",
      "{'loss': 0.5784, 'grad_norm': 0.22184590995311737, 'learning_rate': 0.00018445742609311382, 'epoch': 0.08}\n",
      "{'loss': 0.5437, 'grad_norm': 0.17305390536785126, 'learning_rate': 0.0001844220215967428, 'epoch': 0.08}\n",
      "{'loss': 0.7094, 'grad_norm': 0.21761785447597504, 'learning_rate': 0.00018438661710037176, 'epoch': 0.08}\n",
      "{'loss': 0.5901, 'grad_norm': 0.29623615741729736, 'learning_rate': 0.00018435121260400073, 'epoch': 0.08}\n",
      "{'loss': 0.7315, 'grad_norm': 0.21809101104736328, 'learning_rate': 0.00018431580810762967, 'epoch': 0.08}\n",
      "{'loss': 0.6772, 'grad_norm': 0.19755133986473083, 'learning_rate': 0.00018428040361125864, 'epoch': 0.08}\n",
      "{'loss': 0.8298, 'grad_norm': 0.23604722321033478, 'learning_rate': 0.0001842449991148876, 'epoch': 0.08}\n",
      "{'loss': 0.7087, 'grad_norm': 0.2112450897693634, 'learning_rate': 0.00018420959461851657, 'epoch': 0.08}\n",
      "{'loss': 0.6234, 'grad_norm': 0.23498527705669403, 'learning_rate': 0.00018417419012214551, 'epoch': 0.08}\n",
      "{'loss': 0.6037, 'grad_norm': 0.1946784406900406, 'learning_rate': 0.00018413878562577448, 'epoch': 0.08}\n",
      "{'loss': 0.7521, 'grad_norm': 0.21921704709529877, 'learning_rate': 0.00018410338112940345, 'epoch': 0.08}\n",
      "{'loss': 0.9512, 'grad_norm': 0.2754797041416168, 'learning_rate': 0.00018406797663303242, 'epoch': 0.08}\n",
      "{'loss': 0.7286, 'grad_norm': 0.266990065574646, 'learning_rate': 0.00018403257213666136, 'epoch': 0.08}\n",
      "{'loss': 0.7782, 'grad_norm': 0.2134501039981842, 'learning_rate': 0.00018399716764029033, 'epoch': 0.08}\n",
      "{'loss': 0.8362, 'grad_norm': 0.2906323969364166, 'learning_rate': 0.0001839617631439193, 'epoch': 0.08}\n",
      "{'loss': 0.5824, 'grad_norm': 0.21172450482845306, 'learning_rate': 0.00018392635864754827, 'epoch': 0.08}\n",
      "{'loss': 0.659, 'grad_norm': 0.17971938848495483, 'learning_rate': 0.0001838909541511772, 'epoch': 0.08}\n",
      "{'loss': 0.66, 'grad_norm': 0.17553354799747467, 'learning_rate': 0.00018385554965480615, 'epoch': 0.08}\n",
      "{'loss': 0.8073, 'grad_norm': 0.211532860994339, 'learning_rate': 0.00018382014515843514, 'epoch': 0.08}\n",
      "{'loss': 0.6782, 'grad_norm': 0.2726423442363739, 'learning_rate': 0.00018378474066206408, 'epoch': 0.08}\n",
      "{'loss': 0.9526, 'grad_norm': 0.2784196436405182, 'learning_rate': 0.00018374933616569305, 'epoch': 0.08}\n",
      "{'loss': 0.7725, 'grad_norm': 0.23764318227767944, 'learning_rate': 0.000183713931669322, 'epoch': 0.08}\n",
      "{'loss': 0.8971, 'grad_norm': 0.2905215322971344, 'learning_rate': 0.000183678527172951, 'epoch': 0.08}\n",
      "{'loss': 1.0563, 'grad_norm': 0.22385546565055847, 'learning_rate': 0.00018364312267657993, 'epoch': 0.08}\n",
      "{'loss': 0.8202, 'grad_norm': 0.25485602021217346, 'learning_rate': 0.0001836077181802089, 'epoch': 0.08}\n",
      "{'loss': 0.5604, 'grad_norm': 0.21522200107574463, 'learning_rate': 0.00018357231368383784, 'epoch': 0.08}\n",
      "{'loss': 1.3032, 'grad_norm': 0.26494330167770386, 'learning_rate': 0.0001835369091874668, 'epoch': 0.08}\n",
      "{'loss': 0.5951, 'grad_norm': 0.1762133538722992, 'learning_rate': 0.00018350150469109578, 'epoch': 0.08}\n",
      "{'loss': 0.7546, 'grad_norm': 0.21957020461559296, 'learning_rate': 0.00018346610019472474, 'epoch': 0.08}\n",
      "{'loss': 0.6261, 'grad_norm': 0.2196192890405655, 'learning_rate': 0.00018343069569835369, 'epoch': 0.08}\n",
      "{'loss': 0.675, 'grad_norm': 0.2411092072725296, 'learning_rate': 0.00018339529120198265, 'epoch': 0.08}\n",
      "{'loss': 0.607, 'grad_norm': 0.1551031470298767, 'learning_rate': 0.00018335988670561162, 'epoch': 0.08}\n",
      "{'loss': 0.7429, 'grad_norm': 0.1779288351535797, 'learning_rate': 0.0001833244822092406, 'epoch': 0.08}\n",
      "{'loss': 0.6214, 'grad_norm': 0.23715980350971222, 'learning_rate': 0.00018328907771286953, 'epoch': 0.08}\n",
      "{'loss': 0.5116, 'grad_norm': 0.20470421016216278, 'learning_rate': 0.0001832536732164985, 'epoch': 0.08}\n",
      "{'loss': 1.0214, 'grad_norm': 0.2626008689403534, 'learning_rate': 0.00018321826872012747, 'epoch': 0.08}\n",
      "{'loss': 0.5699, 'grad_norm': 0.1701887995004654, 'learning_rate': 0.00018318286422375644, 'epoch': 0.08}\n",
      "{'loss': 0.6999, 'grad_norm': 0.1864832043647766, 'learning_rate': 0.00018314745972738538, 'epoch': 0.09}\n",
      "{'loss': 0.8949, 'grad_norm': 0.21944086253643036, 'learning_rate': 0.00018311205523101435, 'epoch': 0.09}\n",
      "{'loss': 0.9502, 'grad_norm': 0.27386733889579773, 'learning_rate': 0.00018307665073464331, 'epoch': 0.09}\n",
      "{'loss': 0.8524, 'grad_norm': 0.22015924751758575, 'learning_rate': 0.00018304124623827228, 'epoch': 0.09}\n",
      "{'loss': 0.4839, 'grad_norm': 0.15232311189174652, 'learning_rate': 0.00018300584174190122, 'epoch': 0.09}\n",
      "{'loss': 0.8366, 'grad_norm': 0.22231416404247284, 'learning_rate': 0.0001829704372455302, 'epoch': 0.09}\n",
      "{'loss': 0.8767, 'grad_norm': 0.23438933491706848, 'learning_rate': 0.00018293503274915916, 'epoch': 0.09}\n",
      "{'loss': 0.9199, 'grad_norm': 0.23730875551700592, 'learning_rate': 0.0001828996282527881, 'epoch': 0.09}\n",
      "{'loss': 0.6816, 'grad_norm': 0.2385353296995163, 'learning_rate': 0.00018286422375641707, 'epoch': 0.09}\n",
      "{'loss': 0.734, 'grad_norm': 0.19711120426654816, 'learning_rate': 0.000182828819260046, 'epoch': 0.09}\n",
      "{'loss': 0.6865, 'grad_norm': 0.15737615525722504, 'learning_rate': 0.000182793414763675, 'epoch': 0.09}\n",
      "{'loss': 0.9036, 'grad_norm': 0.23066674172878265, 'learning_rate': 0.00018275801026730395, 'epoch': 0.09}\n",
      "{'loss': 0.7275, 'grad_norm': 0.2020532488822937, 'learning_rate': 0.00018272260577093292, 'epoch': 0.09}\n",
      "{'loss': 0.6763, 'grad_norm': 0.19175218045711517, 'learning_rate': 0.00018268720127456186, 'epoch': 0.09}\n",
      "{'loss': 0.6682, 'grad_norm': 0.23898915946483612, 'learning_rate': 0.00018265179677819085, 'epoch': 0.09}\n",
      "{'loss': 0.6562, 'grad_norm': 0.21191856265068054, 'learning_rate': 0.0001826163922818198, 'epoch': 0.09}\n",
      "{'loss': 0.8681, 'grad_norm': 0.21694563329219818, 'learning_rate': 0.00018258098778544876, 'epoch': 0.09}\n",
      "{'loss': 0.6347, 'grad_norm': 0.20394261181354523, 'learning_rate': 0.0001825455832890777, 'epoch': 0.09}\n",
      "{'loss': 0.7289, 'grad_norm': 0.288836270570755, 'learning_rate': 0.0001825101787927067, 'epoch': 0.09}\n",
      "{'loss': 0.7261, 'grad_norm': 0.17962823808193207, 'learning_rate': 0.00018247477429633564, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76edddd0ed40>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: eb45ce79-02fc-4d3c-ba75-91c21dbb500e)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8176, 'grad_norm': 0.2005707174539566, 'learning_rate': 0.0001824393697999646, 'epoch': 0.09}\n",
      "{'loss': 0.7567, 'grad_norm': 0.2651562988758087, 'learning_rate': 0.00018240396530359355, 'epoch': 0.09}\n",
      "{'loss': 0.8587, 'grad_norm': 0.193593367934227, 'learning_rate': 0.00018236856080722255, 'epoch': 0.09}\n",
      "{'loss': 0.8512, 'grad_norm': 0.2551272213459015, 'learning_rate': 0.0001823331563108515, 'epoch': 0.09}\n",
      "{'loss': 1.2736, 'grad_norm': 0.22416122257709503, 'learning_rate': 0.00018229775181448046, 'epoch': 0.09}\n",
      "{'loss': 1.0359, 'grad_norm': 0.2738470137119293, 'learning_rate': 0.0001822623473181094, 'epoch': 0.09}\n",
      "{'loss': 0.6352, 'grad_norm': 0.24015548825263977, 'learning_rate': 0.00018222694282173836, 'epoch': 0.09}\n",
      "{'loss': 0.7356, 'grad_norm': 0.22841660678386688, 'learning_rate': 0.00018219153832536733, 'epoch': 0.09}\n",
      "{'loss': 0.7827, 'grad_norm': 0.25998130440711975, 'learning_rate': 0.0001821561338289963, 'epoch': 0.09}\n",
      "{'loss': 0.7521, 'grad_norm': 0.27344679832458496, 'learning_rate': 0.00018212072933262524, 'epoch': 0.09}\n",
      "{'loss': 1.009, 'grad_norm': 0.2511638402938843, 'learning_rate': 0.0001820853248362542, 'epoch': 0.09}\n",
      "{'loss': 0.6054, 'grad_norm': 0.18199080228805542, 'learning_rate': 0.00018204992033988318, 'epoch': 0.09}\n",
      "{'loss': 0.7079, 'grad_norm': 0.2276214212179184, 'learning_rate': 0.00018201451584351215, 'epoch': 0.09}\n",
      "{'loss': 0.7929, 'grad_norm': 0.2074022740125656, 'learning_rate': 0.0001819791113471411, 'epoch': 0.09}\n",
      "{'loss': 0.8022, 'grad_norm': 0.22013460099697113, 'learning_rate': 0.00018194370685077006, 'epoch': 0.09}\n",
      "{'loss': 0.734, 'grad_norm': 0.18522928655147552, 'learning_rate': 0.00018190830235439903, 'epoch': 0.09}\n",
      "{'loss': 0.8754, 'grad_norm': 0.18310987949371338, 'learning_rate': 0.00018187289785802797, 'epoch': 0.09}\n",
      "{'loss': 0.629, 'grad_norm': 0.20454023778438568, 'learning_rate': 0.00018183749336165694, 'epoch': 0.09}\n",
      "{'loss': 0.7367, 'grad_norm': 0.22334261238574982, 'learning_rate': 0.0001818020888652859, 'epoch': 0.09}\n",
      "{'loss': 0.9466, 'grad_norm': 0.24626338481903076, 'learning_rate': 0.00018176668436891487, 'epoch': 0.09}\n",
      "{'loss': 0.9358, 'grad_norm': 0.2675406038761139, 'learning_rate': 0.0001817312798725438, 'epoch': 0.09}\n",
      "{'loss': 0.7763, 'grad_norm': 0.25203099846839905, 'learning_rate': 0.00018169587537617278, 'epoch': 0.09}\n",
      "{'loss': 0.8929, 'grad_norm': 0.23244591057300568, 'learning_rate': 0.00018166047087980175, 'epoch': 0.09}\n",
      "{'loss': 0.8227, 'grad_norm': 0.22430740296840668, 'learning_rate': 0.00018162506638343072, 'epoch': 0.09}\n",
      "{'loss': 0.7379, 'grad_norm': 0.2404753416776657, 'learning_rate': 0.00018158966188705966, 'epoch': 0.09}\n",
      "{'loss': 0.6709, 'grad_norm': 0.22822459042072296, 'learning_rate': 0.00018155425739068863, 'epoch': 0.09}\n",
      "{'loss': 0.7862, 'grad_norm': 0.2759609520435333, 'learning_rate': 0.00018151885289431757, 'epoch': 0.09}\n",
      "{'loss': 0.7663, 'grad_norm': 0.2741493284702301, 'learning_rate': 0.00018148344839794656, 'epoch': 0.09}\n",
      "{'loss': 0.8076, 'grad_norm': 0.24615953862667084, 'learning_rate': 0.0001814480439015755, 'epoch': 0.09}\n",
      "{'loss': 0.7201, 'grad_norm': 0.2032606303691864, 'learning_rate': 0.00018141263940520447, 'epoch': 0.09}\n",
      "{'loss': 0.7468, 'grad_norm': 0.23733067512512207, 'learning_rate': 0.00018137723490883341, 'epoch': 0.09}\n",
      "{'loss': 0.5401, 'grad_norm': 0.15895718336105347, 'learning_rate': 0.0001813418304124624, 'epoch': 0.09}\n",
      "{'loss': 0.7655, 'grad_norm': 0.204707533121109, 'learning_rate': 0.00018130642591609135, 'epoch': 0.09}\n",
      "{'loss': 0.9828, 'grad_norm': 0.2865615487098694, 'learning_rate': 0.00018127102141972032, 'epoch': 0.09}\n",
      "{'loss': 0.6622, 'grad_norm': 0.20291121304035187, 'learning_rate': 0.00018123561692334926, 'epoch': 0.09}\n",
      "{'loss': 0.7942, 'grad_norm': 0.2020646631717682, 'learning_rate': 0.00018120021242697826, 'epoch': 0.09}\n",
      "{'loss': 0.564, 'grad_norm': 0.17725934088230133, 'learning_rate': 0.0001811648079306072, 'epoch': 0.09}\n",
      "{'loss': 0.6206, 'grad_norm': 0.19756999611854553, 'learning_rate': 0.00018112940343423617, 'epoch': 0.1}\n",
      "{'loss': 0.6572, 'grad_norm': 0.21708829700946808, 'learning_rate': 0.0001810939989378651, 'epoch': 0.1}\n",
      "{'loss': 0.6168, 'grad_norm': 0.17716974020004272, 'learning_rate': 0.0001810585944414941, 'epoch': 0.1}\n",
      "{'loss': 0.6196, 'grad_norm': 0.2612893283367157, 'learning_rate': 0.00018102318994512304, 'epoch': 0.1}\n",
      "{'loss': 1.0431, 'grad_norm': 0.210316464304924, 'learning_rate': 0.00018098778544875198, 'epoch': 0.1}\n",
      "{'loss': 0.6651, 'grad_norm': 0.26222047209739685, 'learning_rate': 0.00018095238095238095, 'epoch': 0.1}\n",
      "{'loss': 0.8156, 'grad_norm': 0.25095245242118835, 'learning_rate': 0.00018091697645600992, 'epoch': 0.1}\n",
      "{'loss': 0.6581, 'grad_norm': 0.21669945120811462, 'learning_rate': 0.0001808815719596389, 'epoch': 0.1}\n",
      "{'loss': 0.6457, 'grad_norm': 0.27503499388694763, 'learning_rate': 0.00018084616746326783, 'epoch': 0.1}\n",
      "{'loss': 0.8277, 'grad_norm': 0.3058030605316162, 'learning_rate': 0.0001808107629668968, 'epoch': 0.1}\n",
      "{'loss': 1.1708, 'grad_norm': 0.2305867224931717, 'learning_rate': 0.00018077535847052577, 'epoch': 0.1}\n",
      "{'loss': 0.7224, 'grad_norm': 0.19557391107082367, 'learning_rate': 0.00018073995397415474, 'epoch': 0.1}\n",
      "{'loss': 1.068, 'grad_norm': 0.26772186160087585, 'learning_rate': 0.00018070454947778368, 'epoch': 0.1}\n",
      "{'loss': 1.0327, 'grad_norm': 0.1970367282629013, 'learning_rate': 0.00018066914498141265, 'epoch': 0.1}\n",
      "{'loss': 0.7888, 'grad_norm': 0.24791450798511505, 'learning_rate': 0.00018063374048504161, 'epoch': 0.1}\n",
      "{'loss': 0.7853, 'grad_norm': 0.22297494113445282, 'learning_rate': 0.00018059833598867058, 'epoch': 0.1}\n",
      "{'loss': 1.0484, 'grad_norm': 0.24971474707126617, 'learning_rate': 0.00018056293149229952, 'epoch': 0.1}\n",
      "{'loss': 1.0065, 'grad_norm': 0.3511302173137665, 'learning_rate': 0.0001805275269959285, 'epoch': 0.1}\n",
      "{'loss': 0.8026, 'grad_norm': 0.18965911865234375, 'learning_rate': 0.00018049212249955746, 'epoch': 0.1}\n",
      "{'loss': 0.5415, 'grad_norm': 0.2138763815164566, 'learning_rate': 0.00018045671800318643, 'epoch': 0.1}\n",
      "{'loss': 0.7971, 'grad_norm': 0.23248809576034546, 'learning_rate': 0.00018042131350681537, 'epoch': 0.1}\n",
      "{'loss': 0.6629, 'grad_norm': 0.19145789742469788, 'learning_rate': 0.00018038590901044434, 'epoch': 0.1}\n",
      "{'loss': 1.463, 'grad_norm': 0.26762378215789795, 'learning_rate': 0.0001803505045140733, 'epoch': 0.1}\n",
      "{'loss': 0.6886, 'grad_norm': 0.27778205275535583, 'learning_rate': 0.00018031510001770227, 'epoch': 0.1}\n",
      "{'loss': 0.6726, 'grad_norm': 0.2091716229915619, 'learning_rate': 0.00018027969552133122, 'epoch': 0.1}\n",
      "{'loss': 0.5953, 'grad_norm': 0.19795548915863037, 'learning_rate': 0.00018024429102496018, 'epoch': 0.1}\n",
      "{'loss': 0.8592, 'grad_norm': 0.2177082598209381, 'learning_rate': 0.00018020888652858913, 'epoch': 0.1}\n",
      "{'loss': 0.8297, 'grad_norm': 0.27130934596061707, 'learning_rate': 0.00018017348203221812, 'epoch': 0.1}\n",
      "{'loss': 0.666, 'grad_norm': 0.24121692776679993, 'learning_rate': 0.00018013807753584706, 'epoch': 0.1}\n",
      "{'loss': 0.7622, 'grad_norm': 0.2043384164571762, 'learning_rate': 0.00018010267303947603, 'epoch': 0.1}\n",
      "{'loss': 0.5481, 'grad_norm': 0.2613885700702667, 'learning_rate': 0.00018006726854310497, 'epoch': 0.1}\n",
      "{'loss': 0.7517, 'grad_norm': 0.17579717934131622, 'learning_rate': 0.00018003186404673394, 'epoch': 0.1}\n",
      "{'loss': 0.7719, 'grad_norm': 0.22034159302711487, 'learning_rate': 0.0001799964595503629, 'epoch': 0.1}\n",
      "{'loss': 0.7815, 'grad_norm': 0.22484539449214935, 'learning_rate': 0.00017996105505399185, 'epoch': 0.1}\n",
      "{'loss': 0.7229, 'grad_norm': 0.24182361364364624, 'learning_rate': 0.00017992565055762082, 'epoch': 0.1}\n",
      "{'loss': 0.9582, 'grad_norm': 0.19952866435050964, 'learning_rate': 0.00017989024606124979, 'epoch': 0.1}\n",
      "{'loss': 0.8739, 'grad_norm': 0.2660241723060608, 'learning_rate': 0.00017985484156487875, 'epoch': 0.1}\n",
      "{'loss': 1.0041, 'grad_norm': 0.22983144223690033, 'learning_rate': 0.0001798194370685077, 'epoch': 0.1}\n",
      "{'loss': 0.6931, 'grad_norm': 0.20739369094371796, 'learning_rate': 0.00017978403257213666, 'epoch': 0.1}\n",
      "{'loss': 0.7318, 'grad_norm': 0.20263509452342987, 'learning_rate': 0.00017974862807576563, 'epoch': 0.1}\n",
      "{'loss': 0.8777, 'grad_norm': 0.2120191901922226, 'learning_rate': 0.0001797132235793946, 'epoch': 0.1}\n",
      "{'loss': 0.6725, 'grad_norm': 0.1834409087896347, 'learning_rate': 0.00017967781908302354, 'epoch': 0.1}\n",
      "{'loss': 0.7016, 'grad_norm': 0.24115470051765442, 'learning_rate': 0.0001796424145866525, 'epoch': 0.1}\n",
      "{'loss': 0.4486, 'grad_norm': 0.17989929020404816, 'learning_rate': 0.00017960701009028148, 'epoch': 0.1}\n",
      "{'loss': 0.6728, 'grad_norm': 0.21533477306365967, 'learning_rate': 0.00017957160559391045, 'epoch': 0.1}\n",
      "{'loss': 0.7218, 'grad_norm': 0.256012499332428, 'learning_rate': 0.0001795362010975394, 'epoch': 0.1}\n",
      "{'loss': 0.7367, 'grad_norm': 0.2648676931858063, 'learning_rate': 0.00017950079660116836, 'epoch': 0.1}\n",
      "{'loss': 0.9404, 'grad_norm': 0.21717624366283417, 'learning_rate': 0.00017946539210479732, 'epoch': 0.1}\n",
      "{'loss': 0.6356, 'grad_norm': 0.22937855124473572, 'learning_rate': 0.0001794299876084263, 'epoch': 0.1}\n",
      "{'loss': 0.6851, 'grad_norm': 0.23424100875854492, 'learning_rate': 0.00017939458311205523, 'epoch': 0.1}\n",
      "{'loss': 0.6244, 'grad_norm': 0.20047643780708313, 'learning_rate': 0.0001793591786156842, 'epoch': 0.1}\n",
      "{'loss': 0.8922, 'grad_norm': 0.21128840744495392, 'learning_rate': 0.00017932377411931317, 'epoch': 0.1}\n",
      "{'loss': 0.6236, 'grad_norm': 0.19878548383712769, 'learning_rate': 0.00017928836962294214, 'epoch': 0.1}\n",
      "{'loss': 0.9216, 'grad_norm': 0.22742420434951782, 'learning_rate': 0.00017925296512657108, 'epoch': 0.1}\n",
      "{'loss': 0.5623, 'grad_norm': 0.25798699259757996, 'learning_rate': 0.00017921756063020005, 'epoch': 0.1}\n",
      "{'loss': 0.8836, 'grad_norm': 0.20905901491641998, 'learning_rate': 0.00017918215613382902, 'epoch': 0.1}\n",
      "{'loss': 0.5634, 'grad_norm': 0.2273516207933426, 'learning_rate': 0.00017914675163745796, 'epoch': 0.11}\n",
      "{'loss': 1.0877, 'grad_norm': 0.26177743077278137, 'learning_rate': 0.00017911134714108693, 'epoch': 0.11}\n",
      "{'loss': 0.5456, 'grad_norm': 0.1769556850194931, 'learning_rate': 0.00017907594264471587, 'epoch': 0.11}\n",
      "{'loss': 0.5668, 'grad_norm': 0.19081610441207886, 'learning_rate': 0.00017904053814834486, 'epoch': 0.11}\n",
      "{'loss': 0.6007, 'grad_norm': 0.22503314912319183, 'learning_rate': 0.0001790051336519738, 'epoch': 0.11}\n",
      "{'loss': 0.5509, 'grad_norm': 0.2318468987941742, 'learning_rate': 0.00017896972915560277, 'epoch': 0.11}\n",
      "{'loss': 0.7022, 'grad_norm': 0.16869069635868073, 'learning_rate': 0.00017893432465923171, 'epoch': 0.11}\n",
      "{'loss': 0.6152, 'grad_norm': 0.1898360550403595, 'learning_rate': 0.00017889892016286068, 'epoch': 0.11}\n",
      "{'loss': 0.7911, 'grad_norm': 0.2672859728336334, 'learning_rate': 0.00017886351566648965, 'epoch': 0.11}\n",
      "{'loss': 0.6269, 'grad_norm': 0.20901159942150116, 'learning_rate': 0.00017882811117011862, 'epoch': 0.11}\n",
      "{'loss': 0.7708, 'grad_norm': 0.2006014883518219, 'learning_rate': 0.00017879270667374756, 'epoch': 0.11}\n",
      "{'loss': 0.5019, 'grad_norm': 0.21744060516357422, 'learning_rate': 0.00017875730217737653, 'epoch': 0.11}\n",
      "{'loss': 1.1238, 'grad_norm': 0.267981618642807, 'learning_rate': 0.0001787218976810055, 'epoch': 0.11}\n",
      "{'loss': 0.5654, 'grad_norm': 0.21633967757225037, 'learning_rate': 0.00017868649318463446, 'epoch': 0.11}\n",
      "{'loss': 0.867, 'grad_norm': 0.2663680613040924, 'learning_rate': 0.0001786510886882634, 'epoch': 0.11}\n",
      "{'loss': 0.7139, 'grad_norm': 0.2365158051252365, 'learning_rate': 0.00017861568419189237, 'epoch': 0.11}\n",
      "{'loss': 0.6794, 'grad_norm': 0.21710574626922607, 'learning_rate': 0.00017858027969552134, 'epoch': 0.11}\n",
      "{'loss': 0.6406, 'grad_norm': 0.19852764904499054, 'learning_rate': 0.0001785448751991503, 'epoch': 0.11}\n",
      "{'loss': 0.794, 'grad_norm': 0.3467292785644531, 'learning_rate': 0.00017850947070277925, 'epoch': 0.11}\n",
      "{'loss': 0.531, 'grad_norm': 0.19452887773513794, 'learning_rate': 0.00017847406620640822, 'epoch': 0.11}\n",
      "{'loss': 0.8032, 'grad_norm': 0.22897659242153168, 'learning_rate': 0.0001784386617100372, 'epoch': 0.11}\n",
      "{'loss': 0.6737, 'grad_norm': 0.1777261346578598, 'learning_rate': 0.00017840325721366616, 'epoch': 0.11}\n",
      "{'loss': 0.8052, 'grad_norm': 0.27494123578071594, 'learning_rate': 0.0001783678527172951, 'epoch': 0.11}\n",
      "{'loss': 1.002, 'grad_norm': 0.291083425283432, 'learning_rate': 0.00017833244822092407, 'epoch': 0.11}\n",
      "{'loss': 0.8287, 'grad_norm': 0.20539218187332153, 'learning_rate': 0.00017829704372455304, 'epoch': 0.11}\n",
      "{'loss': 0.6057, 'grad_norm': 0.19673195481300354, 'learning_rate': 0.000178261639228182, 'epoch': 0.11}\n",
      "{'loss': 0.7187, 'grad_norm': 0.2412085384130478, 'learning_rate': 0.00017822623473181094, 'epoch': 0.11}\n",
      "{'loss': 0.5958, 'grad_norm': 0.2336392104625702, 'learning_rate': 0.00017819083023543989, 'epoch': 0.11}\n",
      "{'loss': 1.0358, 'grad_norm': 0.30894017219543457, 'learning_rate': 0.00017815542573906888, 'epoch': 0.11}\n",
      "{'loss': 0.7343, 'grad_norm': 0.2503460645675659, 'learning_rate': 0.00017812002124269782, 'epoch': 0.11}\n",
      "{'loss': 0.7689, 'grad_norm': 0.24323312938213348, 'learning_rate': 0.0001780846167463268, 'epoch': 0.11}\n",
      "{'loss': 0.8771, 'grad_norm': 0.27116096019744873, 'learning_rate': 0.00017804921224995573, 'epoch': 0.11}\n",
      "{'loss': 0.6613, 'grad_norm': 0.18180447816848755, 'learning_rate': 0.00017801380775358473, 'epoch': 0.11}\n",
      "{'loss': 0.7501, 'grad_norm': 0.22846931219100952, 'learning_rate': 0.00017797840325721367, 'epoch': 0.11}\n",
      "{'loss': 0.7468, 'grad_norm': 0.2975316643714905, 'learning_rate': 0.00017794299876084264, 'epoch': 0.11}\n",
      "{'loss': 0.6943, 'grad_norm': 0.21785224974155426, 'learning_rate': 0.00017790759426447158, 'epoch': 0.11}\n",
      "{'loss': 1.0185, 'grad_norm': 0.2161627560853958, 'learning_rate': 0.00017787218976810057, 'epoch': 0.11}\n",
      "{'loss': 0.574, 'grad_norm': 0.20522712171077728, 'learning_rate': 0.00017783678527172951, 'epoch': 0.11}\n",
      "{'loss': 0.6293, 'grad_norm': 0.24729010462760925, 'learning_rate': 0.00017780138077535848, 'epoch': 0.11}\n",
      "{'loss': 0.7585, 'grad_norm': 0.22207756340503693, 'learning_rate': 0.00017776597627898742, 'epoch': 0.11}\n",
      "{'loss': 0.8578, 'grad_norm': 0.2523398995399475, 'learning_rate': 0.00017773057178261642, 'epoch': 0.11}\n",
      "{'loss': 0.68, 'grad_norm': 0.22067266702651978, 'learning_rate': 0.00017769516728624536, 'epoch': 0.11}\n",
      "{'loss': 0.9467, 'grad_norm': 0.25812065601348877, 'learning_rate': 0.00017765976278987433, 'epoch': 0.11}\n",
      "{'loss': 0.8565, 'grad_norm': 0.24859490990638733, 'learning_rate': 0.00017762435829350327, 'epoch': 0.11}\n",
      "{'loss': 0.605, 'grad_norm': 0.1734771728515625, 'learning_rate': 0.00017758895379713224, 'epoch': 0.11}\n",
      "{'loss': 0.531, 'grad_norm': 0.27067074179649353, 'learning_rate': 0.0001775535493007612, 'epoch': 0.11}\n",
      "{'loss': 0.7189, 'grad_norm': 0.2281029224395752, 'learning_rate': 0.00017751814480439018, 'epoch': 0.11}\n",
      "{'loss': 0.7671, 'grad_norm': 0.23259271681308746, 'learning_rate': 0.00017748274030801912, 'epoch': 0.11}\n",
      "{'loss': 1.1338, 'grad_norm': 0.2961011826992035, 'learning_rate': 0.00017744733581164808, 'epoch': 0.11}\n",
      "{'loss': 0.9328, 'grad_norm': 0.2758317291736603, 'learning_rate': 0.00017741193131527705, 'epoch': 0.11}\n",
      "{'loss': 0.7804, 'grad_norm': 0.28405237197875977, 'learning_rate': 0.00017737652681890602, 'epoch': 0.11}\n",
      "{'loss': 0.7152, 'grad_norm': 0.2709481418132782, 'learning_rate': 0.00017734112232253496, 'epoch': 0.11}\n",
      "{'loss': 0.8439, 'grad_norm': 0.2687464952468872, 'learning_rate': 0.00017730571782616393, 'epoch': 0.11}\n",
      "{'loss': 0.9498, 'grad_norm': 0.24184422194957733, 'learning_rate': 0.0001772703133297929, 'epoch': 0.11}\n",
      "{'loss': 0.7199, 'grad_norm': 0.2796688973903656, 'learning_rate': 0.00017723490883342184, 'epoch': 0.11}\n",
      "{'loss': 0.8912, 'grad_norm': 0.2194376289844513, 'learning_rate': 0.0001771995043370508, 'epoch': 0.11}\n",
      "{'loss': 0.8379, 'grad_norm': 0.2008875608444214, 'learning_rate': 0.00017716409984067978, 'epoch': 0.11}\n",
      "{'loss': 0.82, 'grad_norm': 0.22723178565502167, 'learning_rate': 0.00017712869534430875, 'epoch': 0.12}\n",
      "{'loss': 0.7946, 'grad_norm': 0.24295173585414886, 'learning_rate': 0.0001770932908479377, 'epoch': 0.12}\n",
      "{'loss': 0.6377, 'grad_norm': 0.18903668224811554, 'learning_rate': 0.00017705788635156666, 'epoch': 0.12}\n",
      "{'loss': 0.6863, 'grad_norm': 0.18089339137077332, 'learning_rate': 0.0001770224818551956, 'epoch': 0.12}\n",
      "{'loss': 0.6041, 'grad_norm': 0.18794628977775574, 'learning_rate': 0.0001769870773588246, 'epoch': 0.12}\n",
      "{'loss': 0.895, 'grad_norm': 0.22938993573188782, 'learning_rate': 0.00017695167286245353, 'epoch': 0.12}\n",
      "{'loss': 0.6361, 'grad_norm': 0.24597163498401642, 'learning_rate': 0.0001769162683660825, 'epoch': 0.12}\n",
      "{'loss': 0.9324, 'grad_norm': 0.2801332175731659, 'learning_rate': 0.00017688086386971144, 'epoch': 0.12}\n",
      "{'loss': 0.7051, 'grad_norm': 0.28986459970474243, 'learning_rate': 0.00017684545937334044, 'epoch': 0.12}\n",
      "{'loss': 0.9548, 'grad_norm': 0.2535170018672943, 'learning_rate': 0.00017681005487696938, 'epoch': 0.12}\n",
      "{'loss': 0.7219, 'grad_norm': 0.27127915620803833, 'learning_rate': 0.00017677465038059835, 'epoch': 0.12}\n",
      "{'loss': 0.9143, 'grad_norm': 0.24846045672893524, 'learning_rate': 0.0001767392458842273, 'epoch': 0.12}\n",
      "{'loss': 0.6545, 'grad_norm': 0.22191664576530457, 'learning_rate': 0.00017670384138785628, 'epoch': 0.12}\n",
      "{'loss': 0.7775, 'grad_norm': 0.22152861952781677, 'learning_rate': 0.00017666843689148523, 'epoch': 0.12}\n",
      "{'loss': 0.5053, 'grad_norm': 0.18925391137599945, 'learning_rate': 0.0001766330323951142, 'epoch': 0.12}\n",
      "{'loss': 0.7076, 'grad_norm': 0.2640804648399353, 'learning_rate': 0.00017659762789874313, 'epoch': 0.12}\n",
      "{'loss': 0.9422, 'grad_norm': 0.3053146302700043, 'learning_rate': 0.00017656222340237213, 'epoch': 0.12}\n",
      "{'loss': 0.7706, 'grad_norm': 0.22063902020454407, 'learning_rate': 0.00017652681890600107, 'epoch': 0.12}\n",
      "{'loss': 0.7107, 'grad_norm': 0.21540740132331848, 'learning_rate': 0.00017649141440963004, 'epoch': 0.12}\n",
      "{'loss': 1.0243, 'grad_norm': 0.2885130047798157, 'learning_rate': 0.00017645600991325898, 'epoch': 0.12}\n",
      "{'loss': 0.8898, 'grad_norm': 0.2882598638534546, 'learning_rate': 0.00017642060541688798, 'epoch': 0.12}\n",
      "{'loss': 0.6082, 'grad_norm': 0.21509002149105072, 'learning_rate': 0.00017638520092051692, 'epoch': 0.12}\n",
      "{'loss': 0.7497, 'grad_norm': 0.21291159093379974, 'learning_rate': 0.00017634979642414589, 'epoch': 0.12}\n",
      "{'loss': 0.8827, 'grad_norm': 0.25356119871139526, 'learning_rate': 0.00017631439192777483, 'epoch': 0.12}\n",
      "{'loss': 0.8765, 'grad_norm': 0.2484690248966217, 'learning_rate': 0.0001762789874314038, 'epoch': 0.12}\n",
      "{'loss': 0.636, 'grad_norm': 0.24003556370735168, 'learning_rate': 0.00017624358293503276, 'epoch': 0.12}\n",
      "{'loss': 0.6595, 'grad_norm': 0.2787053883075714, 'learning_rate': 0.0001762081784386617, 'epoch': 0.12}\n",
      "{'loss': 0.8202, 'grad_norm': 0.252584308385849, 'learning_rate': 0.00017617277394229067, 'epoch': 0.12}\n",
      "{'loss': 0.9427, 'grad_norm': 0.22464220225811005, 'learning_rate': 0.00017613736944591964, 'epoch': 0.12}\n",
      "{'loss': 0.8656, 'grad_norm': 0.24145029485225677, 'learning_rate': 0.0001761019649495486, 'epoch': 0.12}\n",
      "{'loss': 1.0233, 'grad_norm': 0.27233847975730896, 'learning_rate': 0.00017606656045317755, 'epoch': 0.12}\n",
      "{'loss': 0.8505, 'grad_norm': 0.22310739755630493, 'learning_rate': 0.00017603115595680652, 'epoch': 0.12}\n",
      "{'loss': 0.6369, 'grad_norm': 0.2096373289823532, 'learning_rate': 0.0001759957514604355, 'epoch': 0.12}\n",
      "{'loss': 0.7525, 'grad_norm': 0.17741842567920685, 'learning_rate': 0.00017596034696406446, 'epoch': 0.12}\n",
      "{'loss': 1.0428, 'grad_norm': 0.17313876748085022, 'learning_rate': 0.0001759249424676934, 'epoch': 0.12}\n",
      "{'loss': 0.8228, 'grad_norm': 0.21636958420276642, 'learning_rate': 0.00017588953797132237, 'epoch': 0.12}\n",
      "{'loss': 0.7382, 'grad_norm': 0.3058307468891144, 'learning_rate': 0.00017585413347495133, 'epoch': 0.12}\n",
      "{'loss': 0.7117, 'grad_norm': 0.2178758680820465, 'learning_rate': 0.0001758187289785803, 'epoch': 0.12}\n",
      "{'loss': 0.5951, 'grad_norm': 0.2009379118680954, 'learning_rate': 0.00017578332448220924, 'epoch': 0.12}\n",
      "{'loss': 0.7805, 'grad_norm': 0.23804430663585663, 'learning_rate': 0.0001757479199858382, 'epoch': 0.12}\n",
      "{'loss': 0.9197, 'grad_norm': 0.2166328728199005, 'learning_rate': 0.00017571251548946715, 'epoch': 0.12}\n",
      "{'loss': 1.243, 'grad_norm': 0.2823125123977661, 'learning_rate': 0.00017567711099309615, 'epoch': 0.12}\n",
      "{'loss': 0.7121, 'grad_norm': 0.1951080709695816, 'learning_rate': 0.0001756417064967251, 'epoch': 0.12}\n",
      "{'loss': 0.5382, 'grad_norm': 0.16354933381080627, 'learning_rate': 0.00017560630200035406, 'epoch': 0.12}\n",
      "{'loss': 0.7871, 'grad_norm': 0.3144620954990387, 'learning_rate': 0.000175570897503983, 'epoch': 0.12}\n",
      "{'loss': 0.8418, 'grad_norm': 0.16712960600852966, 'learning_rate': 0.000175535493007612, 'epoch': 0.12}\n",
      "{'loss': 0.8373, 'grad_norm': 0.22924531996250153, 'learning_rate': 0.00017550008851124094, 'epoch': 0.12}\n",
      "{'loss': 0.5182, 'grad_norm': 0.32888737320899963, 'learning_rate': 0.0001754646840148699, 'epoch': 0.12}\n",
      "{'loss': 0.9741, 'grad_norm': 0.2983091473579407, 'learning_rate': 0.00017542927951849885, 'epoch': 0.12}\n",
      "{'loss': 0.632, 'grad_norm': 0.234945148229599, 'learning_rate': 0.00017539387502212784, 'epoch': 0.12}\n",
      "{'loss': 0.5772, 'grad_norm': 0.2016560584306717, 'learning_rate': 0.00017535847052575678, 'epoch': 0.12}\n",
      "{'loss': 0.5592, 'grad_norm': 0.20195648074150085, 'learning_rate': 0.00017532306602938572, 'epoch': 0.12}\n",
      "{'loss': 0.7575, 'grad_norm': 0.2657720148563385, 'learning_rate': 0.0001752876615330147, 'epoch': 0.12}\n",
      "{'loss': 0.6018, 'grad_norm': 0.24242781102657318, 'learning_rate': 0.00017525225703664366, 'epoch': 0.12}\n",
      "{'loss': 0.721, 'grad_norm': 0.22963108122348785, 'learning_rate': 0.00017521685254027263, 'epoch': 0.12}\n",
      "{'loss': 0.6462, 'grad_norm': 0.2828397750854492, 'learning_rate': 0.00017518144804390157, 'epoch': 0.12}\n",
      "{'loss': 0.9988, 'grad_norm': 0.2534734606742859, 'learning_rate': 0.00017514604354753054, 'epoch': 0.13}\n",
      "{'loss': 0.8774, 'grad_norm': 0.2042267918586731, 'learning_rate': 0.0001751106390511595, 'epoch': 0.13}\n",
      "{'loss': 0.643, 'grad_norm': 0.18442578613758087, 'learning_rate': 0.00017507523455478847, 'epoch': 0.13}\n",
      "{'loss': 0.7807, 'grad_norm': 0.2111368328332901, 'learning_rate': 0.00017503983005841742, 'epoch': 0.13}\n",
      "{'loss': 0.9032, 'grad_norm': 0.20825019478797913, 'learning_rate': 0.00017500442556204638, 'epoch': 0.13}\n",
      "{'loss': 0.4989, 'grad_norm': 0.1910320669412613, 'learning_rate': 0.00017496902106567535, 'epoch': 0.13}\n",
      "{'loss': 0.8747, 'grad_norm': 0.25790542364120483, 'learning_rate': 0.00017493361656930432, 'epoch': 0.13}\n",
      "{'loss': 0.6979, 'grad_norm': 0.244378924369812, 'learning_rate': 0.00017489821207293326, 'epoch': 0.13}\n",
      "{'loss': 0.7619, 'grad_norm': 0.2540455162525177, 'learning_rate': 0.00017486280757656223, 'epoch': 0.13}\n",
      "{'loss': 0.7222, 'grad_norm': 0.2635929584503174, 'learning_rate': 0.0001748274030801912, 'epoch': 0.13}\n",
      "{'loss': 0.7503, 'grad_norm': 0.301248162984848, 'learning_rate': 0.00017479199858382017, 'epoch': 0.13}\n",
      "{'loss': 0.7049, 'grad_norm': 0.24205270409584045, 'learning_rate': 0.0001747565940874491, 'epoch': 0.13}\n",
      "{'loss': 0.5366, 'grad_norm': 0.1664794534444809, 'learning_rate': 0.00017472118959107808, 'epoch': 0.13}\n",
      "{'loss': 0.5756, 'grad_norm': 0.23888742923736572, 'learning_rate': 0.00017468578509470704, 'epoch': 0.13}\n",
      "{'loss': 0.8387, 'grad_norm': 0.26094311475753784, 'learning_rate': 0.000174650380598336, 'epoch': 0.13}\n",
      "{'loss': 0.8714, 'grad_norm': 0.26922228932380676, 'learning_rate': 0.00017461497610196495, 'epoch': 0.13}\n",
      "{'loss': 0.7562, 'grad_norm': 0.20791777968406677, 'learning_rate': 0.00017457957160559392, 'epoch': 0.13}\n",
      "{'loss': 0.8778, 'grad_norm': 0.18764756619930267, 'learning_rate': 0.0001745441671092229, 'epoch': 0.13}\n",
      "{'loss': 0.6228, 'grad_norm': 0.22951093316078186, 'learning_rate': 0.00017450876261285186, 'epoch': 0.13}\n",
      "{'loss': 0.7623, 'grad_norm': 0.23748651146888733, 'learning_rate': 0.0001744733581164808, 'epoch': 0.13}\n",
      "{'loss': 0.7432, 'grad_norm': 0.28930801153182983, 'learning_rate': 0.00017443795362010977, 'epoch': 0.13}\n",
      "{'loss': 0.5998, 'grad_norm': 0.22434493899345398, 'learning_rate': 0.0001744025491237387, 'epoch': 0.13}\n",
      "{'loss': 0.7959, 'grad_norm': 0.1945776790380478, 'learning_rate': 0.00017436714462736768, 'epoch': 0.13}\n",
      "{'loss': 0.7123, 'grad_norm': 0.2174208164215088, 'learning_rate': 0.00017433174013099665, 'epoch': 0.13}\n",
      "{'loss': 0.6119, 'grad_norm': 0.17093639075756073, 'learning_rate': 0.0001742963356346256, 'epoch': 0.13}\n",
      "{'loss': 0.9509, 'grad_norm': 0.18358159065246582, 'learning_rate': 0.00017426093113825456, 'epoch': 0.13}\n",
      "{'loss': 0.7726, 'grad_norm': 0.24041931331157684, 'learning_rate': 0.00017422552664188352, 'epoch': 0.13}\n",
      "{'loss': 0.9686, 'grad_norm': 0.29866093397140503, 'learning_rate': 0.0001741901221455125, 'epoch': 0.13}\n",
      "{'loss': 1.098, 'grad_norm': 0.24005919694900513, 'learning_rate': 0.00017415471764914143, 'epoch': 0.13}\n",
      "{'loss': 0.7476, 'grad_norm': 0.2205277532339096, 'learning_rate': 0.0001741193131527704, 'epoch': 0.13}\n",
      "{'loss': 1.1304, 'grad_norm': 0.33766990900039673, 'learning_rate': 0.00017408390865639937, 'epoch': 0.13}\n",
      "{'loss': 0.7525, 'grad_norm': 0.20539270341396332, 'learning_rate': 0.00017404850416002834, 'epoch': 0.13}\n",
      "{'loss': 0.5355, 'grad_norm': 0.21266093850135803, 'learning_rate': 0.00017401309966365728, 'epoch': 0.13}\n",
      "{'loss': 0.6562, 'grad_norm': 0.18448951840400696, 'learning_rate': 0.00017397769516728625, 'epoch': 0.13}\n",
      "{'loss': 1.0217, 'grad_norm': 0.26391178369522095, 'learning_rate': 0.00017394229067091522, 'epoch': 0.13}\n",
      "{'loss': 0.9036, 'grad_norm': 0.26216912269592285, 'learning_rate': 0.00017390688617454418, 'epoch': 0.13}\n",
      "{'loss': 0.8564, 'grad_norm': 0.18201668560504913, 'learning_rate': 0.00017387148167817313, 'epoch': 0.13}\n",
      "{'loss': 0.8092, 'grad_norm': 0.22853347659111023, 'learning_rate': 0.0001738360771818021, 'epoch': 0.13}\n",
      "{'loss': 0.7676, 'grad_norm': 0.24601224064826965, 'learning_rate': 0.00017380067268543106, 'epoch': 0.13}\n",
      "{'loss': 0.581, 'grad_norm': 0.18769489228725433, 'learning_rate': 0.00017376526818906003, 'epoch': 0.13}\n",
      "{'loss': 0.619, 'grad_norm': 0.24623453617095947, 'learning_rate': 0.00017372986369268897, 'epoch': 0.13}\n",
      "{'loss': 0.5674, 'grad_norm': 0.26662278175354004, 'learning_rate': 0.00017369445919631794, 'epoch': 0.13}\n",
      "{'loss': 0.852, 'grad_norm': 0.2802114486694336, 'learning_rate': 0.0001736590546999469, 'epoch': 0.13}\n",
      "{'loss': 0.7977, 'grad_norm': 0.2498350441455841, 'learning_rate': 0.00017362365020357588, 'epoch': 0.13}\n",
      "{'loss': 0.7587, 'grad_norm': 0.17930160462856293, 'learning_rate': 0.00017358824570720482, 'epoch': 0.13}\n",
      "{'loss': 0.7417, 'grad_norm': 0.20998476445674896, 'learning_rate': 0.0001735528412108338, 'epoch': 0.13}\n",
      "{'loss': 0.6791, 'grad_norm': 0.22117368876934052, 'learning_rate': 0.00017351743671446276, 'epoch': 0.13}\n",
      "{'loss': 0.6183, 'grad_norm': 0.1807505339384079, 'learning_rate': 0.00017348203221809172, 'epoch': 0.13}\n",
      "{'loss': 1.0331, 'grad_norm': 0.2816881537437439, 'learning_rate': 0.00017344662772172066, 'epoch': 0.13}\n",
      "{'loss': 1.1247, 'grad_norm': 0.21873250603675842, 'learning_rate': 0.0001734112232253496, 'epoch': 0.13}\n",
      "{'loss': 0.6083, 'grad_norm': 0.17333769798278809, 'learning_rate': 0.0001733758187289786, 'epoch': 0.13}\n",
      "{'loss': 0.6784, 'grad_norm': 0.2521618902683258, 'learning_rate': 0.00017334041423260754, 'epoch': 0.13}\n",
      "{'loss': 0.6127, 'grad_norm': 0.18063220381736755, 'learning_rate': 0.0001733050097362365, 'epoch': 0.13}\n",
      "{'loss': 0.691, 'grad_norm': 0.2472054660320282, 'learning_rate': 0.00017326960523986545, 'epoch': 0.13}\n",
      "{'loss': 0.66, 'grad_norm': 0.18751323223114014, 'learning_rate': 0.00017323420074349445, 'epoch': 0.13}\n",
      "{'loss': 0.6356, 'grad_norm': 0.20931467413902283, 'learning_rate': 0.0001731987962471234, 'epoch': 0.13}\n",
      "{'loss': 0.5854, 'grad_norm': 0.21337704360485077, 'learning_rate': 0.00017316339175075236, 'epoch': 0.13}\n",
      "{'loss': 0.5729, 'grad_norm': 0.1991846114397049, 'learning_rate': 0.0001731279872543813, 'epoch': 0.14}\n",
      "{'loss': 0.8897, 'grad_norm': 0.23467417061328888, 'learning_rate': 0.00017309258275801027, 'epoch': 0.14}\n",
      "{'loss': 0.8577, 'grad_norm': 0.28185030817985535, 'learning_rate': 0.00017305717826163923, 'epoch': 0.14}\n",
      "{'loss': 0.698, 'grad_norm': 0.3000168800354004, 'learning_rate': 0.0001730217737652682, 'epoch': 0.14}\n",
      "{'loss': 0.7078, 'grad_norm': 0.20003503561019897, 'learning_rate': 0.00017298636926889714, 'epoch': 0.14}\n",
      "{'loss': 0.9152, 'grad_norm': 0.24019303917884827, 'learning_rate': 0.0001729509647725261, 'epoch': 0.14}\n",
      "{'loss': 0.6053, 'grad_norm': 0.21719428896903992, 'learning_rate': 0.00017291556027615508, 'epoch': 0.14}\n",
      "{'loss': 0.8163, 'grad_norm': 0.1810539811849594, 'learning_rate': 0.00017288015577978405, 'epoch': 0.14}\n",
      "{'loss': 0.9785, 'grad_norm': 0.1958015263080597, 'learning_rate': 0.000172844751283413, 'epoch': 0.14}\n",
      "{'loss': 0.6772, 'grad_norm': 0.24743197858333588, 'learning_rate': 0.00017280934678704196, 'epoch': 0.14}\n",
      "{'loss': 0.7581, 'grad_norm': 0.2767001688480377, 'learning_rate': 0.00017277394229067093, 'epoch': 0.14}\n",
      "{'loss': 0.7475, 'grad_norm': 0.2666517496109009, 'learning_rate': 0.0001727385377942999, 'epoch': 0.14}\n",
      "{'loss': 0.7348, 'grad_norm': 0.2340007871389389, 'learning_rate': 0.00017270313329792884, 'epoch': 0.14}\n",
      "{'loss': 0.8119, 'grad_norm': 0.2345450520515442, 'learning_rate': 0.0001726677288015578, 'epoch': 0.14}\n",
      "{'loss': 0.9754, 'grad_norm': 0.28068095445632935, 'learning_rate': 0.00017263232430518677, 'epoch': 0.14}\n",
      "{'loss': 0.9082, 'grad_norm': 0.24065321683883667, 'learning_rate': 0.00017259691980881574, 'epoch': 0.14}\n",
      "{'loss': 0.8655, 'grad_norm': 0.21397057175636292, 'learning_rate': 0.00017256151531244468, 'epoch': 0.14}\n",
      "{'loss': 0.7363, 'grad_norm': 0.2671877145767212, 'learning_rate': 0.00017252611081607365, 'epoch': 0.14}\n",
      "{'loss': 0.7707, 'grad_norm': 0.22482815384864807, 'learning_rate': 0.00017249070631970262, 'epoch': 0.14}\n",
      "{'loss': 0.708, 'grad_norm': 0.19999036192893982, 'learning_rate': 0.00017245530182333156, 'epoch': 0.14}\n",
      "{'loss': 0.9224, 'grad_norm': 0.23732012510299683, 'learning_rate': 0.00017241989732696053, 'epoch': 0.14}\n",
      "{'loss': 0.5676, 'grad_norm': 0.23926228284835815, 'learning_rate': 0.00017238449283058947, 'epoch': 0.14}\n",
      "{'loss': 0.8452, 'grad_norm': 0.2575230598449707, 'learning_rate': 0.00017234908833421847, 'epoch': 0.14}\n",
      "{'loss': 0.7186, 'grad_norm': 0.22902028262615204, 'learning_rate': 0.0001723136838378474, 'epoch': 0.14}\n",
      "{'loss': 0.727, 'grad_norm': 0.2584119439125061, 'learning_rate': 0.00017227827934147638, 'epoch': 0.14}\n",
      "{'loss': 0.5342, 'grad_norm': 0.20882096886634827, 'learning_rate': 0.00017224287484510532, 'epoch': 0.14}\n",
      "{'loss': 1.229, 'grad_norm': 0.22621583938598633, 'learning_rate': 0.0001722074703487343, 'epoch': 0.14}\n",
      "{'loss': 0.8583, 'grad_norm': 0.2554478347301483, 'learning_rate': 0.00017217206585236325, 'epoch': 0.14}\n",
      "{'loss': 0.7444, 'grad_norm': 0.24152451753616333, 'learning_rate': 0.00017213666135599222, 'epoch': 0.14}\n",
      "{'loss': 0.8683, 'grad_norm': 0.25787773728370667, 'learning_rate': 0.00017210125685962116, 'epoch': 0.14}\n",
      "{'loss': 0.9834, 'grad_norm': 0.26194891333580017, 'learning_rate': 0.00017206585236325016, 'epoch': 0.14}\n",
      "{'loss': 0.7385, 'grad_norm': 0.2622663080692291, 'learning_rate': 0.0001720304478668791, 'epoch': 0.14}\n",
      "{'loss': 0.7844, 'grad_norm': 0.22995729744434357, 'learning_rate': 0.00017199504337050807, 'epoch': 0.14}\n",
      "{'loss': 0.6621, 'grad_norm': 0.2299666851758957, 'learning_rate': 0.000171959638874137, 'epoch': 0.14}\n",
      "{'loss': 0.9355, 'grad_norm': 0.2318325787782669, 'learning_rate': 0.000171924234377766, 'epoch': 0.14}\n",
      "{'loss': 0.9258, 'grad_norm': 0.23809599876403809, 'learning_rate': 0.00017188882988139495, 'epoch': 0.14}\n",
      "{'loss': 0.6542, 'grad_norm': 0.2681502103805542, 'learning_rate': 0.00017185342538502391, 'epoch': 0.14}\n",
      "{'loss': 0.8643, 'grad_norm': 0.21467812359333038, 'learning_rate': 0.00017181802088865285, 'epoch': 0.14}\n",
      "{'loss': 0.7243, 'grad_norm': 0.25506502389907837, 'learning_rate': 0.00017178261639228182, 'epoch': 0.14}\n",
      "{'loss': 0.5298, 'grad_norm': 0.1693207323551178, 'learning_rate': 0.0001717472118959108, 'epoch': 0.14}\n",
      "{'loss': 0.8023, 'grad_norm': 0.27744260430336, 'learning_rate': 0.00017171180739953976, 'epoch': 0.14}\n",
      "{'loss': 0.5798, 'grad_norm': 0.21055835485458374, 'learning_rate': 0.0001716764029031687, 'epoch': 0.14}\n",
      "{'loss': 0.9181, 'grad_norm': 0.27150261402130127, 'learning_rate': 0.00017164099840679767, 'epoch': 0.14}\n",
      "{'loss': 0.6493, 'grad_norm': 0.2587144672870636, 'learning_rate': 0.00017160559391042664, 'epoch': 0.14}\n",
      "{'loss': 0.7034, 'grad_norm': 0.2628047466278076, 'learning_rate': 0.0001715701894140556, 'epoch': 0.14}\n",
      "{'loss': 1.4271, 'grad_norm': 0.3028928339481354, 'learning_rate': 0.00017153478491768455, 'epoch': 0.14}\n",
      "{'loss': 0.7062, 'grad_norm': 0.20200732350349426, 'learning_rate': 0.00017149938042131352, 'epoch': 0.14}\n",
      "{'loss': 0.9384, 'grad_norm': 0.30352890491485596, 'learning_rate': 0.00017146397592494248, 'epoch': 0.14}\n",
      "{'loss': 0.7428, 'grad_norm': 0.28970271348953247, 'learning_rate': 0.00017142857142857143, 'epoch': 0.14}\n",
      "{'loss': 0.8943, 'grad_norm': 0.2746383249759674, 'learning_rate': 0.0001713931669322004, 'epoch': 0.14}\n",
      "{'loss': 0.7578, 'grad_norm': 0.24007363617420197, 'learning_rate': 0.00017135776243582936, 'epoch': 0.14}\n",
      "{'loss': 0.571, 'grad_norm': 0.20994235575199127, 'learning_rate': 0.00017132235793945833, 'epoch': 0.14}\n",
      "{'loss': 0.526, 'grad_norm': 0.16842526197433472, 'learning_rate': 0.00017128695344308727, 'epoch': 0.14}\n",
      "{'loss': 0.7439, 'grad_norm': 0.2925006151199341, 'learning_rate': 0.00017125154894671624, 'epoch': 0.14}\n",
      "{'loss': 0.7955, 'grad_norm': 0.321994423866272, 'learning_rate': 0.0001712161444503452, 'epoch': 0.14}\n",
      "{'loss': 0.66, 'grad_norm': 0.21743956208229065, 'learning_rate': 0.00017118073995397418, 'epoch': 0.14}\n",
      "{'loss': 0.7632, 'grad_norm': 0.2891748249530792, 'learning_rate': 0.00017114533545760312, 'epoch': 0.15}\n",
      "{'loss': 0.6865, 'grad_norm': 0.15435953438282013, 'learning_rate': 0.00017110993096123209, 'epoch': 0.15}\n",
      "{'loss': 0.7622, 'grad_norm': 0.2727760076522827, 'learning_rate': 0.00017107452646486103, 'epoch': 0.15}\n",
      "{'loss': 0.6848, 'grad_norm': 0.20737412571907043, 'learning_rate': 0.00017103912196849002, 'epoch': 0.15}\n",
      "{'loss': 0.8774, 'grad_norm': 0.3298850357532501, 'learning_rate': 0.00017100371747211896, 'epoch': 0.15}\n",
      "{'loss': 0.8746, 'grad_norm': 0.22198963165283203, 'learning_rate': 0.00017096831297574793, 'epoch': 0.15}\n",
      "{'loss': 0.5082, 'grad_norm': 0.15834438800811768, 'learning_rate': 0.00017093290847937687, 'epoch': 0.15}\n",
      "{'loss': 0.817, 'grad_norm': 0.19577598571777344, 'learning_rate': 0.00017089750398300587, 'epoch': 0.15}\n",
      "{'loss': 0.8232, 'grad_norm': 0.22189149260520935, 'learning_rate': 0.0001708620994866348, 'epoch': 0.15}\n",
      "{'loss': 1.1381, 'grad_norm': 0.2341461032629013, 'learning_rate': 0.00017082669499026378, 'epoch': 0.15}\n",
      "{'loss': 0.7639, 'grad_norm': 0.2511502504348755, 'learning_rate': 0.00017079129049389272, 'epoch': 0.15}\n",
      "{'loss': 0.6292, 'grad_norm': 0.2039566934108734, 'learning_rate': 0.00017075588599752171, 'epoch': 0.15}\n",
      "{'loss': 0.8965, 'grad_norm': 0.2910492420196533, 'learning_rate': 0.00017072048150115066, 'epoch': 0.15}\n",
      "{'loss': 0.9089, 'grad_norm': 0.24502064287662506, 'learning_rate': 0.00017068507700477962, 'epoch': 0.15}\n",
      "{'loss': 0.5697, 'grad_norm': 0.22271233797073364, 'learning_rate': 0.00017064967250840857, 'epoch': 0.15}\n",
      "{'loss': 0.6376, 'grad_norm': 0.24885541200637817, 'learning_rate': 0.00017061426801203753, 'epoch': 0.15}\n",
      "{'loss': 0.9676, 'grad_norm': 0.28715476393699646, 'learning_rate': 0.0001705788635156665, 'epoch': 0.15}\n",
      "{'loss': 0.5841, 'grad_norm': 0.18857301771640778, 'learning_rate': 0.00017054345901929544, 'epoch': 0.15}\n",
      "{'loss': 0.6153, 'grad_norm': 0.22582851350307465, 'learning_rate': 0.0001705080545229244, 'epoch': 0.15}\n",
      "{'loss': 0.7352, 'grad_norm': 0.21035714447498322, 'learning_rate': 0.00017047265002655338, 'epoch': 0.15}\n",
      "{'loss': 0.5347, 'grad_norm': 0.2048071026802063, 'learning_rate': 0.00017043724553018235, 'epoch': 0.15}\n",
      "{'loss': 0.4872, 'grad_norm': 0.16464264690876007, 'learning_rate': 0.0001704018410338113, 'epoch': 0.15}\n",
      "{'loss': 0.7981, 'grad_norm': 0.23505666851997375, 'learning_rate': 0.00017036643653744026, 'epoch': 0.15}\n",
      "{'loss': 0.6428, 'grad_norm': 0.29691842198371887, 'learning_rate': 0.00017033103204106923, 'epoch': 0.15}\n",
      "{'loss': 1.3939, 'grad_norm': 0.2841445207595825, 'learning_rate': 0.0001702956275446982, 'epoch': 0.15}\n",
      "{'loss': 0.8025, 'grad_norm': 0.18450674414634705, 'learning_rate': 0.00017026022304832714, 'epoch': 0.15}\n",
      "{'loss': 1.0555, 'grad_norm': 0.32672810554504395, 'learning_rate': 0.0001702248185519561, 'epoch': 0.15}\n",
      "{'loss': 0.8418, 'grad_norm': 0.2970661520957947, 'learning_rate': 0.00017018941405558507, 'epoch': 0.15}\n",
      "{'loss': 0.9133, 'grad_norm': 0.24704168736934662, 'learning_rate': 0.00017015400955921404, 'epoch': 0.15}\n",
      "{'loss': 0.6569, 'grad_norm': 0.2275906652212143, 'learning_rate': 0.00017011860506284298, 'epoch': 0.15}\n",
      "{'loss': 0.8854, 'grad_norm': 0.24525995552539825, 'learning_rate': 0.00017008320056647195, 'epoch': 0.15}\n",
      "{'loss': 0.9114, 'grad_norm': 0.2197452187538147, 'learning_rate': 0.00017004779607010092, 'epoch': 0.15}\n",
      "{'loss': 0.6419, 'grad_norm': 0.2606414556503296, 'learning_rate': 0.0001700123915737299, 'epoch': 0.15}\n",
      "{'loss': 0.7871, 'grad_norm': 0.20269332826137543, 'learning_rate': 0.00016997698707735883, 'epoch': 0.15}\n",
      "{'loss': 0.9328, 'grad_norm': 0.25876384973526, 'learning_rate': 0.0001699415825809878, 'epoch': 0.15}\n",
      "{'loss': 0.836, 'grad_norm': 0.28410330414772034, 'learning_rate': 0.00016990617808461676, 'epoch': 0.15}\n",
      "{'loss': 0.7327, 'grad_norm': 0.24109748005867004, 'learning_rate': 0.00016987077358824573, 'epoch': 0.15}\n",
      "{'loss': 0.7649, 'grad_norm': 0.25866472721099854, 'learning_rate': 0.00016983536909187467, 'epoch': 0.15}\n",
      "{'loss': 0.7495, 'grad_norm': 0.2573387920856476, 'learning_rate': 0.00016979996459550364, 'epoch': 0.15}\n",
      "{'loss': 0.8651, 'grad_norm': 0.20969827473163605, 'learning_rate': 0.00016976456009913258, 'epoch': 0.15}\n",
      "{'loss': 0.6203, 'grad_norm': 0.24225495755672455, 'learning_rate': 0.00016972915560276158, 'epoch': 0.15}\n",
      "{'loss': 0.7799, 'grad_norm': 0.2464064657688141, 'learning_rate': 0.00016969375110639052, 'epoch': 0.15}\n",
      "{'loss': 0.921, 'grad_norm': 0.22888317704200745, 'learning_rate': 0.00016965834661001946, 'epoch': 0.15}\n",
      "{'loss': 0.9077, 'grad_norm': 0.2457389384508133, 'learning_rate': 0.00016962294211364843, 'epoch': 0.15}\n",
      "{'loss': 0.6953, 'grad_norm': 0.22849558293819427, 'learning_rate': 0.0001695875376172774, 'epoch': 0.15}\n",
      "{'loss': 0.6512, 'grad_norm': 0.22424377501010895, 'learning_rate': 0.00016955213312090637, 'epoch': 0.15}\n",
      "{'loss': 0.5091, 'grad_norm': 0.19257579743862152, 'learning_rate': 0.0001695167286245353, 'epoch': 0.15}\n",
      "{'loss': 0.7364, 'grad_norm': 0.18417192995548248, 'learning_rate': 0.00016948132412816428, 'epoch': 0.15}\n",
      "{'loss': 0.9625, 'grad_norm': 0.26651889085769653, 'learning_rate': 0.00016944591963179324, 'epoch': 0.15}\n",
      "{'loss': 0.6004, 'grad_norm': 0.246760755777359, 'learning_rate': 0.0001694105151354222, 'epoch': 0.15}\n",
      "{'loss': 0.6158, 'grad_norm': 0.18574140965938568, 'learning_rate': 0.00016937511063905115, 'epoch': 0.15}\n",
      "{'loss': 0.7899, 'grad_norm': 0.20720404386520386, 'learning_rate': 0.00016933970614268012, 'epoch': 0.15}\n",
      "{'loss': 0.7305, 'grad_norm': 0.24853235483169556, 'learning_rate': 0.0001693043016463091, 'epoch': 0.15}\n",
      "{'loss': 0.6973, 'grad_norm': 0.20367969572544098, 'learning_rate': 0.00016926889714993806, 'epoch': 0.15}\n",
      "{'loss': 0.722, 'grad_norm': 0.18409672379493713, 'learning_rate': 0.000169233492653567, 'epoch': 0.15}\n",
      "{'loss': 1.0081, 'grad_norm': 0.21386724710464478, 'learning_rate': 0.00016919808815719597, 'epoch': 0.15}\n",
      "{'loss': 0.9, 'grad_norm': 0.24717676639556885, 'learning_rate': 0.00016916268366082494, 'epoch': 0.15}\n",
      "{'loss': 0.8892, 'grad_norm': 0.2673003077507019, 'learning_rate': 0.0001691272791644539, 'epoch': 0.16}\n",
      "{'loss': 0.8697, 'grad_norm': 0.2809651792049408, 'learning_rate': 0.00016909187466808285, 'epoch': 0.16}\n",
      "{'loss': 0.8791, 'grad_norm': 0.22070080041885376, 'learning_rate': 0.00016905647017171181, 'epoch': 0.16}\n",
      "{'loss': 0.704, 'grad_norm': 0.21225334703922272, 'learning_rate': 0.00016902106567534078, 'epoch': 0.16}\n",
      "{'loss': 0.7347, 'grad_norm': 0.21578563749790192, 'learning_rate': 0.00016898566117896975, 'epoch': 0.16}\n",
      "{'loss': 0.656, 'grad_norm': 0.21220532059669495, 'learning_rate': 0.0001689502566825987, 'epoch': 0.16}\n",
      "{'loss': 0.7803, 'grad_norm': 0.2756214737892151, 'learning_rate': 0.00016891485218622766, 'epoch': 0.16}\n",
      "{'loss': 0.5915, 'grad_norm': 0.2304113507270813, 'learning_rate': 0.00016887944768985663, 'epoch': 0.16}\n",
      "{'loss': 0.5231, 'grad_norm': 0.26523151993751526, 'learning_rate': 0.0001688440431934856, 'epoch': 0.16}\n",
      "{'loss': 0.6956, 'grad_norm': 0.1930885761976242, 'learning_rate': 0.00016880863869711454, 'epoch': 0.16}\n",
      "{'loss': 0.6099, 'grad_norm': 0.1968931257724762, 'learning_rate': 0.0001687732342007435, 'epoch': 0.16}\n",
      "{'loss': 0.8635, 'grad_norm': 0.25102347135543823, 'learning_rate': 0.00016873782970437248, 'epoch': 0.16}\n",
      "{'loss': 0.739, 'grad_norm': 0.2623029351234436, 'learning_rate': 0.00016870242520800142, 'epoch': 0.16}\n",
      "{'loss': 0.8003, 'grad_norm': 0.2260851263999939, 'learning_rate': 0.00016866702071163038, 'epoch': 0.16}\n",
      "{'loss': 0.8395, 'grad_norm': 0.25908342003822327, 'learning_rate': 0.00016863161621525933, 'epoch': 0.16}\n",
      "{'loss': 0.5823, 'grad_norm': 0.1632009744644165, 'learning_rate': 0.00016859621171888832, 'epoch': 0.16}\n",
      "{'loss': 0.6513, 'grad_norm': 0.2725539207458496, 'learning_rate': 0.00016856080722251726, 'epoch': 0.16}\n",
      "{'loss': 1.1766, 'grad_norm': 0.24561423063278198, 'learning_rate': 0.00016852540272614623, 'epoch': 0.16}\n",
      "{'loss': 0.5624, 'grad_norm': 0.19954058527946472, 'learning_rate': 0.00016848999822977517, 'epoch': 0.16}\n",
      "{'loss': 0.7457, 'grad_norm': 0.262992262840271, 'learning_rate': 0.00016845459373340414, 'epoch': 0.16}\n",
      "{'loss': 0.7405, 'grad_norm': 0.19078075885772705, 'learning_rate': 0.0001684191892370331, 'epoch': 0.16}\n",
      "{'loss': 1.0361, 'grad_norm': 0.2213447093963623, 'learning_rate': 0.00016838378474066208, 'epoch': 0.16}\n",
      "{'loss': 0.7699, 'grad_norm': 0.2402651607990265, 'learning_rate': 0.00016834838024429102, 'epoch': 0.16}\n",
      "{'loss': 0.7273, 'grad_norm': 0.2498483508825302, 'learning_rate': 0.00016831297574792, 'epoch': 0.16}\n",
      "{'loss': 0.7943, 'grad_norm': 0.27989932894706726, 'learning_rate': 0.00016827757125154895, 'epoch': 0.16}\n",
      "{'loss': 0.7735, 'grad_norm': 0.35931745171546936, 'learning_rate': 0.00016824216675517792, 'epoch': 0.16}\n",
      "{'loss': 0.6731, 'grad_norm': 0.20497897267341614, 'learning_rate': 0.00016820676225880686, 'epoch': 0.16}\n",
      "{'loss': 0.5381, 'grad_norm': 0.21547025442123413, 'learning_rate': 0.00016817135776243583, 'epoch': 0.16}\n",
      "{'loss': 0.8071, 'grad_norm': 0.22410733997821808, 'learning_rate': 0.0001681359532660648, 'epoch': 0.16}\n",
      "{'loss': 0.6196, 'grad_norm': 0.20039847493171692, 'learning_rate': 0.00016810054876969377, 'epoch': 0.16}\n",
      "{'loss': 0.8253, 'grad_norm': 0.20851962268352509, 'learning_rate': 0.0001680651442733227, 'epoch': 0.16}\n",
      "{'loss': 0.6257, 'grad_norm': 0.21247945725917816, 'learning_rate': 0.00016802973977695168, 'epoch': 0.16}\n",
      "{'loss': 0.8081, 'grad_norm': 0.2817380428314209, 'learning_rate': 0.00016799433528058065, 'epoch': 0.16}\n",
      "{'loss': 0.9024, 'grad_norm': 0.2465694397687912, 'learning_rate': 0.00016795893078420962, 'epoch': 0.16}\n",
      "{'loss': 0.6695, 'grad_norm': 0.23084130883216858, 'learning_rate': 0.00016792352628783856, 'epoch': 0.16}\n",
      "{'loss': 0.7578, 'grad_norm': 0.2230021357536316, 'learning_rate': 0.00016788812179146753, 'epoch': 0.16}\n",
      "{'loss': 0.5771, 'grad_norm': 0.24467773735523224, 'learning_rate': 0.0001678527172950965, 'epoch': 0.16}\n",
      "{'loss': 0.724, 'grad_norm': 0.17580845952033997, 'learning_rate': 0.00016781731279872546, 'epoch': 0.16}\n",
      "{'loss': 0.6392, 'grad_norm': 0.26386624574661255, 'learning_rate': 0.0001677819083023544, 'epoch': 0.16}\n",
      "{'loss': 0.6477, 'grad_norm': 0.1958332359790802, 'learning_rate': 0.00016774650380598334, 'epoch': 0.16}\n",
      "{'loss': 0.8146, 'grad_norm': 0.2396976351737976, 'learning_rate': 0.00016771109930961234, 'epoch': 0.16}\n",
      "{'loss': 0.6373, 'grad_norm': 0.24342864751815796, 'learning_rate': 0.00016767569481324128, 'epoch': 0.16}\n",
      "{'loss': 0.887, 'grad_norm': 0.2242630571126938, 'learning_rate': 0.00016764029031687025, 'epoch': 0.16}\n",
      "{'loss': 0.8473, 'grad_norm': 0.2909117043018341, 'learning_rate': 0.0001676048858204992, 'epoch': 0.16}\n",
      "{'loss': 0.8135, 'grad_norm': 0.26031169295310974, 'learning_rate': 0.00016756948132412819, 'epoch': 0.16}\n",
      "{'loss': 0.5924, 'grad_norm': 0.18359719216823578, 'learning_rate': 0.00016753407682775713, 'epoch': 0.16}\n",
      "{'loss': 0.7823, 'grad_norm': 0.21505941450595856, 'learning_rate': 0.0001674986723313861, 'epoch': 0.16}\n",
      "{'loss': 0.6106, 'grad_norm': 0.19117511808872223, 'learning_rate': 0.00016746326783501504, 'epoch': 0.16}\n",
      "{'loss': 0.715, 'grad_norm': 0.21006959676742554, 'learning_rate': 0.00016742786333864403, 'epoch': 0.16}\n",
      "{'loss': 1.0527, 'grad_norm': 0.2091868817806244, 'learning_rate': 0.00016739245884227297, 'epoch': 0.16}\n",
      "{'loss': 0.7091, 'grad_norm': 0.20705462992191315, 'learning_rate': 0.00016735705434590194, 'epoch': 0.16}\n",
      "{'loss': 0.7177, 'grad_norm': 0.21392111480236053, 'learning_rate': 0.00016732164984953088, 'epoch': 0.16}\n",
      "{'loss': 0.907, 'grad_norm': 0.1719861477613449, 'learning_rate': 0.00016728624535315988, 'epoch': 0.16}\n",
      "{'loss': 1.1573, 'grad_norm': 0.2734195291996002, 'learning_rate': 0.00016725084085678882, 'epoch': 0.16}\n",
      "{'loss': 0.6902, 'grad_norm': 0.20700660347938538, 'learning_rate': 0.0001672154363604178, 'epoch': 0.16}\n",
      "{'loss': 0.7032, 'grad_norm': 0.19634920358657837, 'learning_rate': 0.00016718003186404673, 'epoch': 0.16}\n",
      "{'loss': 0.539, 'grad_norm': 0.1949102133512497, 'learning_rate': 0.0001671446273676757, 'epoch': 0.17}\n",
      "{'loss': 0.8071, 'grad_norm': 0.21712392568588257, 'learning_rate': 0.00016710922287130467, 'epoch': 0.17}\n",
      "{'loss': 0.7075, 'grad_norm': 0.26103028655052185, 'learning_rate': 0.00016707381837493363, 'epoch': 0.17}\n",
      "{'loss': 0.7524, 'grad_norm': 0.20291867852210999, 'learning_rate': 0.00016703841387856258, 'epoch': 0.17}\n",
      "{'loss': 0.992, 'grad_norm': 0.26113906502723694, 'learning_rate': 0.00016700300938219154, 'epoch': 0.17}\n",
      "{'loss': 0.5766, 'grad_norm': 0.23358291387557983, 'learning_rate': 0.0001669676048858205, 'epoch': 0.17}\n",
      "{'loss': 0.9577, 'grad_norm': 0.2811603546142578, 'learning_rate': 0.00016693220038944948, 'epoch': 0.17}\n",
      "{'loss': 0.9349, 'grad_norm': 0.2631794512271881, 'learning_rate': 0.00016689679589307842, 'epoch': 0.17}\n",
      "{'loss': 0.6804, 'grad_norm': 0.19934722781181335, 'learning_rate': 0.0001668613913967074, 'epoch': 0.17}\n",
      "{'loss': 0.8943, 'grad_norm': 0.27732107043266296, 'learning_rate': 0.00016682598690033636, 'epoch': 0.17}\n",
      "{'loss': 0.6444, 'grad_norm': 0.25125235319137573, 'learning_rate': 0.0001667905824039653, 'epoch': 0.17}\n",
      "{'loss': 0.8801, 'grad_norm': 0.17620989680290222, 'learning_rate': 0.00016675517790759427, 'epoch': 0.17}\n",
      "{'loss': 0.7065, 'grad_norm': 0.2205227166414261, 'learning_rate': 0.00016671977341122324, 'epoch': 0.17}\n",
      "{'loss': 0.8296, 'grad_norm': 0.22918997704982758, 'learning_rate': 0.0001666843689148522, 'epoch': 0.17}\n",
      "{'loss': 0.9697, 'grad_norm': 0.31017133593559265, 'learning_rate': 0.00016664896441848115, 'epoch': 0.17}\n",
      "{'loss': 0.832, 'grad_norm': 0.24231868982315063, 'learning_rate': 0.00016661355992211011, 'epoch': 0.17}\n",
      "{'loss': 0.8262, 'grad_norm': 0.24482429027557373, 'learning_rate': 0.00016657815542573908, 'epoch': 0.17}\n",
      "{'loss': 0.7036, 'grad_norm': 0.28126299381256104, 'learning_rate': 0.00016654275092936805, 'epoch': 0.17}\n",
      "{'loss': 0.8546, 'grad_norm': 0.19189676642417908, 'learning_rate': 0.000166507346432997, 'epoch': 0.17}\n",
      "{'loss': 0.6883, 'grad_norm': 0.2661490738391876, 'learning_rate': 0.00016647194193662596, 'epoch': 0.17}\n",
      "{'loss': 0.5371, 'grad_norm': 0.18039749562740326, 'learning_rate': 0.0001664365374402549, 'epoch': 0.17}\n",
      "{'loss': 0.7401, 'grad_norm': 0.1952734887599945, 'learning_rate': 0.0001664011329438839, 'epoch': 0.17}\n",
      "{'loss': 0.9393, 'grad_norm': 0.2414054274559021, 'learning_rate': 0.00016636572844751284, 'epoch': 0.17}\n",
      "{'loss': 0.7377, 'grad_norm': 0.22819696366786957, 'learning_rate': 0.0001663303239511418, 'epoch': 0.17}\n",
      "{'loss': 0.9699, 'grad_norm': 0.27987298369407654, 'learning_rate': 0.00016629491945477075, 'epoch': 0.17}\n",
      "{'loss': 0.7037, 'grad_norm': 0.19414332509040833, 'learning_rate': 0.00016625951495839974, 'epoch': 0.17}\n",
      "{'loss': 0.7628, 'grad_norm': 0.19165262579917908, 'learning_rate': 0.00016622411046202868, 'epoch': 0.17}\n",
      "{'loss': 0.6219, 'grad_norm': 0.1997675597667694, 'learning_rate': 0.00016618870596565765, 'epoch': 0.17}\n",
      "{'loss': 0.7147, 'grad_norm': 0.2010972797870636, 'learning_rate': 0.0001661533014692866, 'epoch': 0.17}\n",
      "{'loss': 0.82, 'grad_norm': 0.23891542851924896, 'learning_rate': 0.0001661178969729156, 'epoch': 0.17}\n",
      "{'loss': 0.7738, 'grad_norm': 0.23231974244117737, 'learning_rate': 0.00016608249247654453, 'epoch': 0.17}\n",
      "{'loss': 0.9037, 'grad_norm': 0.20380935072898865, 'learning_rate': 0.0001660470879801735, 'epoch': 0.17}\n",
      "{'loss': 0.7216, 'grad_norm': 0.28146618604660034, 'learning_rate': 0.00016601168348380244, 'epoch': 0.17}\n",
      "{'loss': 1.0773, 'grad_norm': 0.23277214169502258, 'learning_rate': 0.00016597627898743143, 'epoch': 0.17}\n",
      "{'loss': 0.8925, 'grad_norm': 0.23746603727340698, 'learning_rate': 0.00016594087449106038, 'epoch': 0.17}\n",
      "{'loss': 0.6447, 'grad_norm': 0.21317972242832184, 'learning_rate': 0.00016590546999468934, 'epoch': 0.17}\n",
      "{'loss': 0.6695, 'grad_norm': 0.2785779535770416, 'learning_rate': 0.00016587006549831829, 'epoch': 0.17}\n",
      "{'loss': 0.7415, 'grad_norm': 0.19881150126457214, 'learning_rate': 0.00016583466100194725, 'epoch': 0.17}\n",
      "{'loss': 0.8709, 'grad_norm': 0.23666054010391235, 'learning_rate': 0.00016579925650557622, 'epoch': 0.17}\n",
      "{'loss': 0.5419, 'grad_norm': 0.15071481466293335, 'learning_rate': 0.00016576385200920516, 'epoch': 0.17}\n",
      "{'loss': 0.6695, 'grad_norm': 0.20995573699474335, 'learning_rate': 0.00016572844751283413, 'epoch': 0.17}\n",
      "{'loss': 0.8849, 'grad_norm': 0.1923331767320633, 'learning_rate': 0.0001656930430164631, 'epoch': 0.17}\n",
      "{'loss': 0.7436, 'grad_norm': 0.214082732796669, 'learning_rate': 0.00016565763852009207, 'epoch': 0.17}\n",
      "{'loss': 0.5815, 'grad_norm': 0.16731798648834229, 'learning_rate': 0.000165622234023721, 'epoch': 0.17}\n",
      "{'loss': 0.889, 'grad_norm': 0.22798274457454681, 'learning_rate': 0.00016558682952734998, 'epoch': 0.17}\n",
      "{'loss': 0.9582, 'grad_norm': 0.2764710485935211, 'learning_rate': 0.00016555142503097895, 'epoch': 0.17}\n",
      "{'loss': 0.6704, 'grad_norm': 0.2522183656692505, 'learning_rate': 0.00016551602053460791, 'epoch': 0.17}\n",
      "{'loss': 0.6388, 'grad_norm': 0.1979680210351944, 'learning_rate': 0.00016548061603823686, 'epoch': 0.17}\n",
      "{'loss': 0.9617, 'grad_norm': 0.27014991641044617, 'learning_rate': 0.00016544521154186582, 'epoch': 0.17}\n",
      "{'loss': 0.6893, 'grad_norm': 0.2067023068666458, 'learning_rate': 0.0001654098070454948, 'epoch': 0.17}\n",
      "{'loss': 0.6162, 'grad_norm': 0.20229117572307587, 'learning_rate': 0.00016537440254912376, 'epoch': 0.17}\n",
      "{'loss': 0.7707, 'grad_norm': 0.25449079275131226, 'learning_rate': 0.0001653389980527527, 'epoch': 0.17}\n",
      "{'loss': 0.7754, 'grad_norm': 0.21420985460281372, 'learning_rate': 0.00016530359355638167, 'epoch': 0.17}\n",
      "{'loss': 0.6685, 'grad_norm': 0.2103530317544937, 'learning_rate': 0.00016526818906001064, 'epoch': 0.17}\n",
      "{'loss': 0.6424, 'grad_norm': 0.2471807599067688, 'learning_rate': 0.0001652327845636396, 'epoch': 0.17}\n",
      "{'loss': 1.0036, 'grad_norm': 0.2150648683309555, 'learning_rate': 0.00016519738006726855, 'epoch': 0.17}\n",
      "{'loss': 0.7558, 'grad_norm': 0.263912558555603, 'learning_rate': 0.00016516197557089752, 'epoch': 0.17}\n",
      "{'loss': 0.6265, 'grad_norm': 0.22851026058197021, 'learning_rate': 0.00016512657107452646, 'epoch': 0.18}\n",
      "{'loss': 0.6192, 'grad_norm': 0.22801390290260315, 'learning_rate': 0.00016509116657815545, 'epoch': 0.18}\n",
      "{'loss': 0.7843, 'grad_norm': 0.19109220802783966, 'learning_rate': 0.0001650557620817844, 'epoch': 0.18}\n",
      "{'loss': 0.7691, 'grad_norm': 0.21780693531036377, 'learning_rate': 0.00016502035758541336, 'epoch': 0.18}\n",
      "{'loss': 0.7111, 'grad_norm': 0.27014851570129395, 'learning_rate': 0.0001649849530890423, 'epoch': 0.18}\n",
      "{'loss': 0.7019, 'grad_norm': 0.24508582055568695, 'learning_rate': 0.0001649495485926713, 'epoch': 0.18}\n",
      "{'loss': 0.6692, 'grad_norm': 0.25512683391571045, 'learning_rate': 0.00016491414409630024, 'epoch': 0.18}\n",
      "{'loss': 0.5508, 'grad_norm': 0.2574497163295746, 'learning_rate': 0.00016487873959992918, 'epoch': 0.18}\n",
      "{'loss': 0.6564, 'grad_norm': 0.23079554736614227, 'learning_rate': 0.00016484333510355815, 'epoch': 0.18}\n",
      "{'loss': 1.1183, 'grad_norm': 0.24656282365322113, 'learning_rate': 0.00016480793060718712, 'epoch': 0.18}\n",
      "{'loss': 0.678, 'grad_norm': 0.29171469807624817, 'learning_rate': 0.0001647725261108161, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee37636770>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 1390bc46-ec5a-4dd6-80c5-a9ebe22293b1)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6655, 'grad_norm': 0.24759244918823242, 'learning_rate': 0.00016473712161444503, 'epoch': 0.18}\n",
      "{'loss': 0.6099, 'grad_norm': 0.25295934081077576, 'learning_rate': 0.000164701717118074, 'epoch': 0.18}\n",
      "{'loss': 0.6678, 'grad_norm': 0.2240295708179474, 'learning_rate': 0.00016466631262170296, 'epoch': 0.18}\n",
      "{'loss': 0.8088, 'grad_norm': 0.2650209665298462, 'learning_rate': 0.00016463090812533193, 'epoch': 0.18}\n",
      "{'loss': 1.1236, 'grad_norm': 0.2574915587902069, 'learning_rate': 0.00016459550362896087, 'epoch': 0.18}\n",
      "{'loss': 0.9638, 'grad_norm': 0.22810406982898712, 'learning_rate': 0.00016456009913258984, 'epoch': 0.18}\n",
      "{'loss': 0.7118, 'grad_norm': 0.2429683655500412, 'learning_rate': 0.0001645246946362188, 'epoch': 0.18}\n",
      "{'loss': 0.7952, 'grad_norm': 0.18560190498828888, 'learning_rate': 0.00016448929013984778, 'epoch': 0.18}\n",
      "{'loss': 0.8675, 'grad_norm': 0.23652635514736176, 'learning_rate': 0.00016445388564347672, 'epoch': 0.18}\n",
      "{'loss': 0.9203, 'grad_norm': 0.24769042432308197, 'learning_rate': 0.0001644184811471057, 'epoch': 0.18}\n",
      "{'loss': 0.6278, 'grad_norm': 0.25244903564453125, 'learning_rate': 0.00016438307665073466, 'epoch': 0.18}\n",
      "{'loss': 0.5394, 'grad_norm': 0.22726021707057953, 'learning_rate': 0.00016434767215436363, 'epoch': 0.18}\n",
      "{'loss': 0.6977, 'grad_norm': 0.22093966603279114, 'learning_rate': 0.00016431226765799257, 'epoch': 0.18}\n",
      "{'loss': 0.8267, 'grad_norm': 0.23402518033981323, 'learning_rate': 0.00016427686316162153, 'epoch': 0.18}\n",
      "{'loss': 0.8342, 'grad_norm': 0.19988873600959778, 'learning_rate': 0.0001642414586652505, 'epoch': 0.18}\n",
      "{'loss': 0.6354, 'grad_norm': 0.1926012933254242, 'learning_rate': 0.00016420605416887947, 'epoch': 0.18}\n",
      "{'loss': 0.6566, 'grad_norm': 0.20321884751319885, 'learning_rate': 0.0001641706496725084, 'epoch': 0.18}\n",
      "{'loss': 0.5392, 'grad_norm': 0.1703670173883438, 'learning_rate': 0.00016413524517613738, 'epoch': 0.18}\n",
      "{'loss': 0.9347, 'grad_norm': 0.24570263922214508, 'learning_rate': 0.00016409984067976635, 'epoch': 0.18}\n",
      "{'loss': 0.9869, 'grad_norm': 0.22265249490737915, 'learning_rate': 0.00016406443618339532, 'epoch': 0.18}\n",
      "{'loss': 0.5069, 'grad_norm': 0.1748729646205902, 'learning_rate': 0.00016402903168702426, 'epoch': 0.18}\n",
      "{'loss': 0.6607, 'grad_norm': 0.17622283101081848, 'learning_rate': 0.00016399362719065323, 'epoch': 0.18}\n",
      "{'loss': 0.6121, 'grad_norm': 0.2005806863307953, 'learning_rate': 0.0001639582226942822, 'epoch': 0.18}\n",
      "{'loss': 0.9304, 'grad_norm': 0.2473623901605606, 'learning_rate': 0.00016392281819791114, 'epoch': 0.18}\n",
      "{'loss': 0.6627, 'grad_norm': 0.193589985370636, 'learning_rate': 0.0001638874137015401, 'epoch': 0.18}\n",
      "{'loss': 0.8483, 'grad_norm': 0.21595355868339539, 'learning_rate': 0.00016385200920516905, 'epoch': 0.18}\n",
      "{'loss': 0.7398, 'grad_norm': 0.22053396701812744, 'learning_rate': 0.00016381660470879801, 'epoch': 0.18}\n",
      "{'loss': 0.6412, 'grad_norm': 0.29672735929489136, 'learning_rate': 0.00016378120021242698, 'epoch': 0.18}\n",
      "{'loss': 1.0165, 'grad_norm': 0.2328302264213562, 'learning_rate': 0.00016374579571605595, 'epoch': 0.18}\n",
      "{'loss': 0.6558, 'grad_norm': 0.2380823940038681, 'learning_rate': 0.0001637103912196849, 'epoch': 0.18}\n",
      "{'loss': 0.9397, 'grad_norm': 0.28980377316474915, 'learning_rate': 0.00016367498672331386, 'epoch': 0.18}\n",
      "{'loss': 0.7264, 'grad_norm': 0.19448867440223694, 'learning_rate': 0.00016363958222694283, 'epoch': 0.18}\n",
      "{'loss': 0.6084, 'grad_norm': 0.25499483942985535, 'learning_rate': 0.0001636041777305718, 'epoch': 0.18}\n",
      "{'loss': 0.8008, 'grad_norm': 0.2872440814971924, 'learning_rate': 0.00016356877323420074, 'epoch': 0.18}\n",
      "{'loss': 0.3773, 'grad_norm': 0.18248403072357178, 'learning_rate': 0.0001635333687378297, 'epoch': 0.18}\n",
      "{'loss': 0.7317, 'grad_norm': 0.20620691776275635, 'learning_rate': 0.00016349796424145868, 'epoch': 0.18}\n",
      "{'loss': 0.8216, 'grad_norm': 0.2602476477622986, 'learning_rate': 0.00016346255974508764, 'epoch': 0.18}\n",
      "{'loss': 0.8109, 'grad_norm': 0.24270334839820862, 'learning_rate': 0.00016342715524871658, 'epoch': 0.18}\n",
      "{'loss': 0.495, 'grad_norm': 0.20637297630310059, 'learning_rate': 0.00016339175075234555, 'epoch': 0.18}\n",
      "{'loss': 0.6512, 'grad_norm': 0.19989781081676483, 'learning_rate': 0.00016335634625597452, 'epoch': 0.18}\n",
      "{'loss': 0.5631, 'grad_norm': 0.2176423966884613, 'learning_rate': 0.0001633209417596035, 'epoch': 0.18}\n",
      "{'loss': 0.9554, 'grad_norm': 0.2955259382724762, 'learning_rate': 0.00016328553726323243, 'epoch': 0.18}\n",
      "{'loss': 0.5003, 'grad_norm': 0.2131069004535675, 'learning_rate': 0.0001632501327668614, 'epoch': 0.18}\n",
      "{'loss': 0.9713, 'grad_norm': 0.27759432792663574, 'learning_rate': 0.00016321472827049037, 'epoch': 0.18}\n",
      "{'loss': 0.8229, 'grad_norm': 0.1990564912557602, 'learning_rate': 0.00016317932377411934, 'epoch': 0.18}\n",
      "{'loss': 0.8956, 'grad_norm': 0.22616469860076904, 'learning_rate': 0.00016314391927774828, 'epoch': 0.19}\n",
      "{'loss': 0.8474, 'grad_norm': 0.24911659955978394, 'learning_rate': 0.00016310851478137725, 'epoch': 0.19}\n",
      "{'loss': 0.8275, 'grad_norm': 0.19480811059474945, 'learning_rate': 0.00016307311028500621, 'epoch': 0.19}\n",
      "{'loss': 0.5677, 'grad_norm': 0.1732354313135147, 'learning_rate': 0.00016303770578863518, 'epoch': 0.19}\n",
      "{'loss': 0.6703, 'grad_norm': 0.21425890922546387, 'learning_rate': 0.00016300230129226412, 'epoch': 0.19}\n",
      "{'loss': 0.5831, 'grad_norm': 0.27613410353660583, 'learning_rate': 0.00016296689679589306, 'epoch': 0.19}\n",
      "{'loss': 0.8582, 'grad_norm': 0.24451932311058044, 'learning_rate': 0.00016293149229952206, 'epoch': 0.19}\n",
      "{'loss': 0.7348, 'grad_norm': 0.20907030999660492, 'learning_rate': 0.000162896087803151, 'epoch': 0.19}\n",
      "{'loss': 0.7561, 'grad_norm': 0.20346416532993317, 'learning_rate': 0.00016286068330677997, 'epoch': 0.19}\n",
      "{'loss': 0.8456, 'grad_norm': 0.2524235248565674, 'learning_rate': 0.0001628252788104089, 'epoch': 0.19}\n",
      "{'loss': 1.0446, 'grad_norm': 0.265596866607666, 'learning_rate': 0.0001627898743140379, 'epoch': 0.19}\n",
      "{'loss': 0.7513, 'grad_norm': 0.19444780051708221, 'learning_rate': 0.00016275446981766685, 'epoch': 0.19}\n",
      "{'loss': 0.8252, 'grad_norm': 0.2344866693019867, 'learning_rate': 0.00016271906532129582, 'epoch': 0.19}\n",
      "{'loss': 0.6837, 'grad_norm': 0.27724117040634155, 'learning_rate': 0.00016268366082492476, 'epoch': 0.19}\n",
      "{'loss': 0.691, 'grad_norm': 0.2794668674468994, 'learning_rate': 0.00016264825632855375, 'epoch': 0.19}\n",
      "{'loss': 0.7425, 'grad_norm': 0.18416471779346466, 'learning_rate': 0.0001626128518321827, 'epoch': 0.19}\n",
      "{'loss': 0.7068, 'grad_norm': 0.2101747989654541, 'learning_rate': 0.00016257744733581166, 'epoch': 0.19}\n",
      "{'loss': 0.6646, 'grad_norm': 0.21633969247341156, 'learning_rate': 0.0001625420428394406, 'epoch': 0.19}\n",
      "{'loss': 0.6307, 'grad_norm': 0.2087099254131317, 'learning_rate': 0.00016250663834306957, 'epoch': 0.19}\n",
      "{'loss': 0.7268, 'grad_norm': 0.2056889832019806, 'learning_rate': 0.00016247123384669854, 'epoch': 0.19}\n",
      "{'loss': 0.5945, 'grad_norm': 0.21068799495697021, 'learning_rate': 0.0001624358293503275, 'epoch': 0.19}\n",
      "{'loss': 1.2194, 'grad_norm': 0.2957381010055542, 'learning_rate': 0.00016240042485395645, 'epoch': 0.19}\n",
      "{'loss': 0.8344, 'grad_norm': 0.20503272116184235, 'learning_rate': 0.00016236502035758542, 'epoch': 0.19}\n",
      "{'loss': 1.0474, 'grad_norm': 0.2804170548915863, 'learning_rate': 0.00016232961586121439, 'epoch': 0.19}\n",
      "{'loss': 0.7966, 'grad_norm': 0.3092297613620758, 'learning_rate': 0.00016229421136484335, 'epoch': 0.19}\n",
      "{'loss': 0.7009, 'grad_norm': 0.3175075650215149, 'learning_rate': 0.0001622588068684723, 'epoch': 0.19}\n",
      "{'loss': 0.5212, 'grad_norm': 0.18881447613239288, 'learning_rate': 0.00016222340237210126, 'epoch': 0.19}\n",
      "{'loss': 0.7893, 'grad_norm': 0.2266702651977539, 'learning_rate': 0.00016218799787573023, 'epoch': 0.19}\n",
      "{'loss': 1.1227, 'grad_norm': 0.29509732127189636, 'learning_rate': 0.0001621525933793592, 'epoch': 0.19}\n",
      "{'loss': 0.8417, 'grad_norm': 0.2186478078365326, 'learning_rate': 0.00016211718888298814, 'epoch': 0.19}\n",
      "{'loss': 0.6772, 'grad_norm': 0.2232738733291626, 'learning_rate': 0.0001620817843866171, 'epoch': 0.19}\n",
      "{'loss': 0.9714, 'grad_norm': 0.2485632300376892, 'learning_rate': 0.00016204637989024608, 'epoch': 0.19}\n",
      "{'loss': 0.6899, 'grad_norm': 0.2104385495185852, 'learning_rate': 0.00016201097539387502, 'epoch': 0.19}\n",
      "{'loss': 0.9154, 'grad_norm': 0.20788417756557465, 'learning_rate': 0.000161975570897504, 'epoch': 0.19}\n",
      "{'loss': 0.8107, 'grad_norm': 0.2187223881483078, 'learning_rate': 0.00016194016640113293, 'epoch': 0.19}\n",
      "{'loss': 0.5722, 'grad_norm': 0.16189026832580566, 'learning_rate': 0.00016190476190476192, 'epoch': 0.19}\n",
      "{'loss': 0.5434, 'grad_norm': 0.19573763012886047, 'learning_rate': 0.00016186935740839087, 'epoch': 0.19}\n",
      "{'loss': 0.5773, 'grad_norm': 0.20559222996234894, 'learning_rate': 0.00016183395291201983, 'epoch': 0.19}\n",
      "{'loss': 0.8418, 'grad_norm': 0.23850567638874054, 'learning_rate': 0.00016179854841564877, 'epoch': 0.19}\n",
      "{'loss': 0.758, 'grad_norm': 0.2966257929801941, 'learning_rate': 0.00016176314391927777, 'epoch': 0.19}\n",
      "{'loss': 0.6005, 'grad_norm': 0.24243520200252533, 'learning_rate': 0.0001617277394229067, 'epoch': 0.19}\n",
      "{'loss': 0.8522, 'grad_norm': 0.2508070468902588, 'learning_rate': 0.00016169233492653568, 'epoch': 0.19}\n",
      "{'loss': 0.7329, 'grad_norm': 0.19014397263526917, 'learning_rate': 0.00016165693043016462, 'epoch': 0.19}\n",
      "{'loss': 0.6736, 'grad_norm': 0.23984326422214508, 'learning_rate': 0.00016162152593379362, 'epoch': 0.19}\n",
      "{'loss': 0.904, 'grad_norm': 0.31414613127708435, 'learning_rate': 0.00016158612143742256, 'epoch': 0.19}\n",
      "{'loss': 0.6525, 'grad_norm': 0.20590610802173615, 'learning_rate': 0.00016155071694105153, 'epoch': 0.19}\n",
      "{'loss': 0.7183, 'grad_norm': 0.214579239487648, 'learning_rate': 0.00016151531244468047, 'epoch': 0.19}\n",
      "{'loss': 0.4383, 'grad_norm': 0.2445797324180603, 'learning_rate': 0.00016147990794830946, 'epoch': 0.19}\n",
      "{'loss': 0.7774, 'grad_norm': 0.2229814976453781, 'learning_rate': 0.0001614445034519384, 'epoch': 0.19}\n",
      "{'loss': 0.9362, 'grad_norm': 0.27555322647094727, 'learning_rate': 0.00016140909895556737, 'epoch': 0.19}\n",
      "{'loss': 0.9785, 'grad_norm': 0.2268361747264862, 'learning_rate': 0.0001613736944591963, 'epoch': 0.19}\n",
      "{'loss': 0.8362, 'grad_norm': 0.23292100429534912, 'learning_rate': 0.0001613382899628253, 'epoch': 0.19}\n",
      "{'loss': 0.9682, 'grad_norm': 0.2733649015426636, 'learning_rate': 0.00016130288546645425, 'epoch': 0.19}\n",
      "{'loss': 0.7063, 'grad_norm': 0.1927645355463028, 'learning_rate': 0.00016126748097008322, 'epoch': 0.19}\n",
      "{'loss': 0.6224, 'grad_norm': 0.20256638526916504, 'learning_rate': 0.00016123207647371216, 'epoch': 0.19}\n",
      "{'loss': 0.562, 'grad_norm': 0.19636696577072144, 'learning_rate': 0.00016119667197734113, 'epoch': 0.19}\n",
      "{'loss': 1.0042, 'grad_norm': 0.2107691764831543, 'learning_rate': 0.0001611612674809701, 'epoch': 0.19}\n",
      "{'loss': 0.747, 'grad_norm': 0.24468891322612762, 'learning_rate': 0.00016112586298459904, 'epoch': 0.2}\n",
      "{'loss': 0.6652, 'grad_norm': 0.2830463647842407, 'learning_rate': 0.000161090458488228, 'epoch': 0.2}\n",
      "{'loss': 0.8692, 'grad_norm': 0.20653313398361206, 'learning_rate': 0.00016105505399185697, 'epoch': 0.2}\n",
      "{'loss': 0.7518, 'grad_norm': 0.18763136863708496, 'learning_rate': 0.00016101964949548594, 'epoch': 0.2}\n",
      "{'loss': 0.8374, 'grad_norm': 0.20599514245986938, 'learning_rate': 0.00016098424499911488, 'epoch': 0.2}\n",
      "{'loss': 0.8866, 'grad_norm': 0.20433899760246277, 'learning_rate': 0.00016094884050274385, 'epoch': 0.2}\n",
      "{'loss': 0.7007, 'grad_norm': 0.20344384014606476, 'learning_rate': 0.00016091343600637282, 'epoch': 0.2}\n",
      "{'loss': 0.591, 'grad_norm': 0.1929827779531479, 'learning_rate': 0.0001608780315100018, 'epoch': 0.2}\n",
      "{'loss': 0.881, 'grad_norm': 0.23456335067749023, 'learning_rate': 0.00016084262701363073, 'epoch': 0.2}\n",
      "{'loss': 0.8827, 'grad_norm': 0.27562764286994934, 'learning_rate': 0.0001608072225172597, 'epoch': 0.2}\n",
      "{'loss': 0.9929, 'grad_norm': 0.2100261151790619, 'learning_rate': 0.00016077181802088867, 'epoch': 0.2}\n",
      "{'loss': 0.7854, 'grad_norm': 0.20452658832073212, 'learning_rate': 0.00016073641352451763, 'epoch': 0.2}\n",
      "{'loss': 0.9103, 'grad_norm': 0.24390597641468048, 'learning_rate': 0.00016070100902814658, 'epoch': 0.2}\n",
      "{'loss': 0.8424, 'grad_norm': 0.28252002596855164, 'learning_rate': 0.00016066560453177554, 'epoch': 0.2}\n",
      "{'loss': 0.6945, 'grad_norm': 0.2454022467136383, 'learning_rate': 0.00016063020003540449, 'epoch': 0.2}\n",
      "{'loss': 0.7481, 'grad_norm': 0.22161200642585754, 'learning_rate': 0.00016059479553903348, 'epoch': 0.2}\n",
      "{'loss': 0.4917, 'grad_norm': 0.19277732074260712, 'learning_rate': 0.00016055939104266242, 'epoch': 0.2}\n",
      "{'loss': 0.9785, 'grad_norm': 0.22200755774974823, 'learning_rate': 0.0001605239865462914, 'epoch': 0.2}\n",
      "{'loss': 0.7128, 'grad_norm': 0.24988611042499542, 'learning_rate': 0.00016048858204992033, 'epoch': 0.2}\n",
      "{'loss': 0.9432, 'grad_norm': 0.2200697511434555, 'learning_rate': 0.00016045317755354933, 'epoch': 0.2}\n",
      "{'loss': 0.8436, 'grad_norm': 0.23096641898155212, 'learning_rate': 0.00016041777305717827, 'epoch': 0.2}\n",
      "{'loss': 0.7899, 'grad_norm': 0.26936841011047363, 'learning_rate': 0.00016038236856080724, 'epoch': 0.2}\n",
      "{'loss': 0.8565, 'grad_norm': 0.18304073810577393, 'learning_rate': 0.00016034696406443618, 'epoch': 0.2}\n",
      "{'loss': 0.6622, 'grad_norm': 0.21576473116874695, 'learning_rate': 0.00016031155956806517, 'epoch': 0.2}\n",
      "{'loss': 0.7771, 'grad_norm': 0.21041597425937653, 'learning_rate': 0.00016027615507169411, 'epoch': 0.2}\n",
      "{'loss': 0.7675, 'grad_norm': 0.17193292081356049, 'learning_rate': 0.00016024075057532308, 'epoch': 0.2}\n",
      "{'loss': 0.7562, 'grad_norm': 0.22725136578083038, 'learning_rate': 0.00016020534607895202, 'epoch': 0.2}\n",
      "{'loss': 0.5263, 'grad_norm': 0.24025501310825348, 'learning_rate': 0.000160169941582581, 'epoch': 0.2}\n",
      "{'loss': 0.7742, 'grad_norm': 0.2831539213657379, 'learning_rate': 0.00016013453708620996, 'epoch': 0.2}\n",
      "{'loss': 0.5877, 'grad_norm': 0.17121969163417816, 'learning_rate': 0.0001600991325898389, 'epoch': 0.2}\n",
      "{'loss': 0.6206, 'grad_norm': 0.19267959892749786, 'learning_rate': 0.00016006372809346787, 'epoch': 0.2}\n",
      "{'loss': 0.7678, 'grad_norm': 0.22679565846920013, 'learning_rate': 0.00016002832359709684, 'epoch': 0.2}\n",
      "{'loss': 1.0773, 'grad_norm': 0.3436870574951172, 'learning_rate': 0.0001599929191007258, 'epoch': 0.2}\n",
      "{'loss': 1.0118, 'grad_norm': 0.28921404480934143, 'learning_rate': 0.00015995751460435475, 'epoch': 0.2}\n",
      "{'loss': 0.8067, 'grad_norm': 0.2532171607017517, 'learning_rate': 0.00015992211010798372, 'epoch': 0.2}\n",
      "{'loss': 1.0396, 'grad_norm': 0.2607223093509674, 'learning_rate': 0.00015988670561161268, 'epoch': 0.2}\n",
      "{'loss': 0.6513, 'grad_norm': 0.21936970949172974, 'learning_rate': 0.00015985130111524165, 'epoch': 0.2}\n",
      "{'loss': 0.8047, 'grad_norm': 0.26286619901657104, 'learning_rate': 0.0001598158966188706, 'epoch': 0.2}\n",
      "{'loss': 0.7947, 'grad_norm': 0.29515206813812256, 'learning_rate': 0.00015978049212249956, 'epoch': 0.2}\n",
      "{'loss': 0.6991, 'grad_norm': 0.2223813831806183, 'learning_rate': 0.00015974508762612853, 'epoch': 0.2}\n",
      "{'loss': 0.8527, 'grad_norm': 0.2542046010494232, 'learning_rate': 0.0001597096831297575, 'epoch': 0.2}\n",
      "{'loss': 0.9737, 'grad_norm': 0.21591556072235107, 'learning_rate': 0.00015967427863338644, 'epoch': 0.2}\n",
      "{'loss': 0.8669, 'grad_norm': 0.2639680504798889, 'learning_rate': 0.0001596388741370154, 'epoch': 0.2}\n",
      "{'loss': 1.0789, 'grad_norm': 0.2710952162742615, 'learning_rate': 0.00015960346964064438, 'epoch': 0.2}\n",
      "{'loss': 1.1114, 'grad_norm': 0.2536151111125946, 'learning_rate': 0.00015956806514427335, 'epoch': 0.2}\n",
      "{'loss': 0.9189, 'grad_norm': 0.24031147360801697, 'learning_rate': 0.0001595326606479023, 'epoch': 0.2}\n",
      "{'loss': 0.6492, 'grad_norm': 0.2778094708919525, 'learning_rate': 0.00015949725615153125, 'epoch': 0.2}\n",
      "{'loss': 0.8327, 'grad_norm': 0.22433027625083923, 'learning_rate': 0.00015946185165516022, 'epoch': 0.2}\n",
      "{'loss': 0.9847, 'grad_norm': 0.23074668645858765, 'learning_rate': 0.0001594264471587892, 'epoch': 0.2}\n",
      "{'loss': 0.9067, 'grad_norm': 0.21947090327739716, 'learning_rate': 0.00015939104266241813, 'epoch': 0.2}\n",
      "{'loss': 0.6388, 'grad_norm': 0.18665936589241028, 'learning_rate': 0.0001593556381660471, 'epoch': 0.2}\n",
      "{'loss': 1.108, 'grad_norm': 0.3068782389163971, 'learning_rate': 0.00015932023366967604, 'epoch': 0.2}\n",
      "{'loss': 1.0443, 'grad_norm': 0.29140958189964294, 'learning_rate': 0.00015928482917330504, 'epoch': 0.2}\n",
      "{'loss': 0.5665, 'grad_norm': 0.15973606705665588, 'learning_rate': 0.00015924942467693398, 'epoch': 0.2}\n",
      "{'loss': 1.2071, 'grad_norm': 0.2511746287345886, 'learning_rate': 0.00015921402018056292, 'epoch': 0.2}\n",
      "{'loss': 0.5266, 'grad_norm': 0.1919732689857483, 'learning_rate': 0.0001591786156841919, 'epoch': 0.2}\n",
      "{'loss': 0.818, 'grad_norm': 0.23216512799263, 'learning_rate': 0.00015914321118782086, 'epoch': 0.2}\n",
      "{'loss': 0.7252, 'grad_norm': 0.22236789762973785, 'learning_rate': 0.00015910780669144982, 'epoch': 0.21}\n",
      "{'loss': 0.8567, 'grad_norm': 0.25816747546195984, 'learning_rate': 0.00015907240219507877, 'epoch': 0.21}\n",
      "{'loss': 0.7435, 'grad_norm': 0.25566327571868896, 'learning_rate': 0.00015903699769870773, 'epoch': 0.21}\n",
      "{'loss': 0.6993, 'grad_norm': 0.2116285115480423, 'learning_rate': 0.0001590015932023367, 'epoch': 0.21}\n",
      "{'loss': 0.5368, 'grad_norm': 0.2635153532028198, 'learning_rate': 0.00015896618870596567, 'epoch': 0.21}\n",
      "{'loss': 0.7459, 'grad_norm': 0.24838699400424957, 'learning_rate': 0.0001589307842095946, 'epoch': 0.21}\n",
      "{'loss': 0.9471, 'grad_norm': 0.26212218403816223, 'learning_rate': 0.00015889537971322358, 'epoch': 0.21}\n",
      "{'loss': 0.9654, 'grad_norm': 0.24256989359855652, 'learning_rate': 0.00015885997521685255, 'epoch': 0.21}\n",
      "{'loss': 1.073, 'grad_norm': 0.2920837700366974, 'learning_rate': 0.00015882457072048152, 'epoch': 0.21}\n",
      "{'loss': 0.6815, 'grad_norm': 0.18396057188510895, 'learning_rate': 0.00015878916622411046, 'epoch': 0.21}\n",
      "{'loss': 0.9168, 'grad_norm': 0.26144951581954956, 'learning_rate': 0.00015875376172773943, 'epoch': 0.21}\n",
      "{'loss': 0.7116, 'grad_norm': 0.20542575418949127, 'learning_rate': 0.0001587183572313684, 'epoch': 0.21}\n",
      "{'loss': 0.762, 'grad_norm': 0.22421757876873016, 'learning_rate': 0.00015868295273499736, 'epoch': 0.21}\n",
      "{'loss': 0.653, 'grad_norm': 0.30147427320480347, 'learning_rate': 0.0001586475482386263, 'epoch': 0.21}\n",
      "{'loss': 0.7577, 'grad_norm': 0.20579156279563904, 'learning_rate': 0.00015861214374225527, 'epoch': 0.21}\n",
      "{'loss': 0.6229, 'grad_norm': 0.23291327059268951, 'learning_rate': 0.00015857673924588424, 'epoch': 0.21}\n",
      "{'loss': 0.8164, 'grad_norm': 0.20835906267166138, 'learning_rate': 0.0001585413347495132, 'epoch': 0.21}\n",
      "{'loss': 0.6085, 'grad_norm': 0.2355487197637558, 'learning_rate': 0.00015850593025314215, 'epoch': 0.21}\n",
      "{'loss': 0.706, 'grad_norm': 0.26493966579437256, 'learning_rate': 0.00015847052575677112, 'epoch': 0.21}\n",
      "{'loss': 0.5669, 'grad_norm': 0.19817014038562775, 'learning_rate': 0.0001584351212604001, 'epoch': 0.21}\n",
      "{'loss': 0.7008, 'grad_norm': 0.2077065408229828, 'learning_rate': 0.00015839971676402906, 'epoch': 0.21}\n",
      "{'loss': 0.6807, 'grad_norm': 0.2675512731075287, 'learning_rate': 0.000158364312267658, 'epoch': 0.21}\n",
      "{'loss': 1.2726, 'grad_norm': 0.2846753001213074, 'learning_rate': 0.00015832890777128697, 'epoch': 0.21}\n",
      "{'loss': 0.6704, 'grad_norm': 0.2047533094882965, 'learning_rate': 0.00015829350327491593, 'epoch': 0.21}\n",
      "{'loss': 0.651, 'grad_norm': 0.3025050461292267, 'learning_rate': 0.00015825809877854487, 'epoch': 0.21}\n",
      "{'loss': 1.0631, 'grad_norm': 0.22514282166957855, 'learning_rate': 0.00015822269428217384, 'epoch': 0.21}\n",
      "{'loss': 0.8976, 'grad_norm': 0.2045622020959854, 'learning_rate': 0.00015818728978580278, 'epoch': 0.21}\n",
      "{'loss': 0.6965, 'grad_norm': 0.23143090307712555, 'learning_rate': 0.00015815188528943178, 'epoch': 0.21}\n",
      "{'loss': 0.7541, 'grad_norm': 0.21923257410526276, 'learning_rate': 0.00015811648079306072, 'epoch': 0.21}\n",
      "{'loss': 0.7709, 'grad_norm': 0.3441268503665924, 'learning_rate': 0.0001580810762966897, 'epoch': 0.21}\n",
      "{'loss': 0.7712, 'grad_norm': 0.2633553445339203, 'learning_rate': 0.00015804567180031863, 'epoch': 0.21}\n",
      "{'loss': 0.8485, 'grad_norm': 0.19533942639827728, 'learning_rate': 0.0001580102673039476, 'epoch': 0.21}\n",
      "{'loss': 0.6115, 'grad_norm': 0.19765479862689972, 'learning_rate': 0.00015797486280757657, 'epoch': 0.21}\n",
      "{'loss': 0.6622, 'grad_norm': 0.28854429721832275, 'learning_rate': 0.00015793945831120554, 'epoch': 0.21}\n",
      "{'loss': 0.7275, 'grad_norm': 0.29365092515945435, 'learning_rate': 0.00015790405381483448, 'epoch': 0.21}\n",
      "{'loss': 0.9099, 'grad_norm': 0.27719298005104065, 'learning_rate': 0.00015786864931846345, 'epoch': 0.21}\n",
      "{'loss': 1.1045, 'grad_norm': 0.30834412574768066, 'learning_rate': 0.0001578332448220924, 'epoch': 0.21}\n",
      "{'loss': 0.7527, 'grad_norm': 0.20259669423103333, 'learning_rate': 0.00015779784032572138, 'epoch': 0.21}\n",
      "{'loss': 1.0248, 'grad_norm': 0.2529808282852173, 'learning_rate': 0.00015776243582935032, 'epoch': 0.21}\n",
      "{'loss': 0.7965, 'grad_norm': 0.19768379628658295, 'learning_rate': 0.0001577270313329793, 'epoch': 0.21}\n",
      "{'loss': 0.7859, 'grad_norm': 0.24143318831920624, 'learning_rate': 0.00015769162683660826, 'epoch': 0.21}\n",
      "{'loss': 0.684, 'grad_norm': 0.19884584844112396, 'learning_rate': 0.00015765622234023723, 'epoch': 0.21}\n",
      "{'loss': 0.8111, 'grad_norm': 0.21375896036624908, 'learning_rate': 0.00015762081784386617, 'epoch': 0.21}\n",
      "{'loss': 0.7477, 'grad_norm': 0.2899209260940552, 'learning_rate': 0.00015758541334749514, 'epoch': 0.21}\n",
      "{'loss': 0.6143, 'grad_norm': 0.3112017512321472, 'learning_rate': 0.0001575500088511241, 'epoch': 0.21}\n",
      "{'loss': 0.7364, 'grad_norm': 0.21402235329151154, 'learning_rate': 0.00015751460435475307, 'epoch': 0.21}\n",
      "{'loss': 0.5847, 'grad_norm': 0.24327291548252106, 'learning_rate': 0.00015747919985838202, 'epoch': 0.21}\n",
      "{'loss': 0.6201, 'grad_norm': 0.21631379425525665, 'learning_rate': 0.00015744379536201098, 'epoch': 0.21}\n",
      "{'loss': 0.5445, 'grad_norm': 0.17529062926769257, 'learning_rate': 0.00015740839086563995, 'epoch': 0.21}\n",
      "{'loss': 0.8282, 'grad_norm': 0.23000377416610718, 'learning_rate': 0.00015737298636926892, 'epoch': 0.21}\n",
      "{'loss': 0.5822, 'grad_norm': 0.2032228261232376, 'learning_rate': 0.00015733758187289786, 'epoch': 0.21}\n",
      "{'loss': 0.7655, 'grad_norm': 0.29774191975593567, 'learning_rate': 0.0001573021773765268, 'epoch': 0.21}\n",
      "{'loss': 0.7452, 'grad_norm': 0.18959318101406097, 'learning_rate': 0.0001572667728801558, 'epoch': 0.21}\n",
      "{'loss': 0.7072, 'grad_norm': 0.2542887032032013, 'learning_rate': 0.00015723136838378474, 'epoch': 0.21}\n",
      "{'loss': 0.7062, 'grad_norm': 0.27776676416397095, 'learning_rate': 0.0001571959638874137, 'epoch': 0.21}\n",
      "{'loss': 0.6611, 'grad_norm': 0.24625687301158905, 'learning_rate': 0.00015716055939104265, 'epoch': 0.21}\n",
      "{'loss': 0.5193, 'grad_norm': 0.23340831696987152, 'learning_rate': 0.00015712515489467164, 'epoch': 0.22}\n",
      "{'loss': 0.9265, 'grad_norm': 0.2522214353084564, 'learning_rate': 0.00015708975039830059, 'epoch': 0.22}\n",
      "{'loss': 0.6964, 'grad_norm': 0.17764616012573242, 'learning_rate': 0.00015705434590192955, 'epoch': 0.22}\n",
      "{'loss': 0.8583, 'grad_norm': 0.24129746854305267, 'learning_rate': 0.0001570189414055585, 'epoch': 0.22}\n",
      "{'loss': 0.9369, 'grad_norm': 0.24936263263225555, 'learning_rate': 0.0001569835369091875, 'epoch': 0.22}\n",
      "{'loss': 0.8786, 'grad_norm': 0.2184133678674698, 'learning_rate': 0.00015694813241281643, 'epoch': 0.22}\n",
      "{'loss': 0.8142, 'grad_norm': 0.21921852231025696, 'learning_rate': 0.0001569127279164454, 'epoch': 0.22}\n",
      "{'loss': 0.9079, 'grad_norm': 0.2128852754831314, 'learning_rate': 0.00015687732342007434, 'epoch': 0.22}\n",
      "{'loss': 0.6916, 'grad_norm': 0.20468789339065552, 'learning_rate': 0.00015684191892370334, 'epoch': 0.22}\n",
      "{'loss': 0.6404, 'grad_norm': 0.18690994381904602, 'learning_rate': 0.00015680651442733228, 'epoch': 0.22}\n",
      "{'loss': 0.6709, 'grad_norm': 0.2726806402206421, 'learning_rate': 0.00015677110993096125, 'epoch': 0.22}\n",
      "{'loss': 0.885, 'grad_norm': 0.23507453501224518, 'learning_rate': 0.0001567357054345902, 'epoch': 0.22}\n",
      "{'loss': 0.5794, 'grad_norm': 0.21619927883148193, 'learning_rate': 0.00015670030093821916, 'epoch': 0.22}\n",
      "{'loss': 0.6582, 'grad_norm': 0.22402366995811462, 'learning_rate': 0.00015666489644184812, 'epoch': 0.22}\n",
      "{'loss': 0.9584, 'grad_norm': 0.3622370958328247, 'learning_rate': 0.0001566294919454771, 'epoch': 0.22}\n",
      "{'loss': 0.4895, 'grad_norm': 0.17198899388313293, 'learning_rate': 0.00015659408744910603, 'epoch': 0.22}\n",
      "{'loss': 0.5237, 'grad_norm': 0.18518631160259247, 'learning_rate': 0.000156558682952735, 'epoch': 0.22}\n",
      "{'loss': 0.6255, 'grad_norm': 0.1979614496231079, 'learning_rate': 0.00015652327845636397, 'epoch': 0.22}\n",
      "{'loss': 0.5685, 'grad_norm': 0.250362366437912, 'learning_rate': 0.00015648787395999294, 'epoch': 0.22}\n",
      "{'loss': 0.6246, 'grad_norm': 0.21850913763046265, 'learning_rate': 0.00015645246946362188, 'epoch': 0.22}\n",
      "{'loss': 0.7018, 'grad_norm': 0.2951694130897522, 'learning_rate': 0.00015641706496725085, 'epoch': 0.22}\n",
      "{'loss': 0.6748, 'grad_norm': 0.22280274331569672, 'learning_rate': 0.00015638166047087982, 'epoch': 0.22}\n",
      "{'loss': 0.8469, 'grad_norm': 0.28316977620124817, 'learning_rate': 0.00015634625597450876, 'epoch': 0.22}\n",
      "{'loss': 0.5346, 'grad_norm': 0.27877935767173767, 'learning_rate': 0.00015631085147813773, 'epoch': 0.22}\n",
      "{'loss': 0.6256, 'grad_norm': 0.18611718714237213, 'learning_rate': 0.0001562754469817667, 'epoch': 0.22}\n",
      "{'loss': 0.7754, 'grad_norm': 0.2561894357204437, 'learning_rate': 0.00015624004248539566, 'epoch': 0.22}\n",
      "{'loss': 0.9121, 'grad_norm': 0.2406410127878189, 'learning_rate': 0.0001562046379890246, 'epoch': 0.22}\n",
      "{'loss': 0.9303, 'grad_norm': 0.2074602097272873, 'learning_rate': 0.00015616923349265357, 'epoch': 0.22}\n",
      "{'loss': 1.0299, 'grad_norm': 0.2812729477882385, 'learning_rate': 0.00015613382899628254, 'epoch': 0.22}\n",
      "{'loss': 0.5784, 'grad_norm': 0.19689960777759552, 'learning_rate': 0.0001560984244999115, 'epoch': 0.22}\n",
      "{'loss': 0.7114, 'grad_norm': 0.2689157724380493, 'learning_rate': 0.00015606302000354045, 'epoch': 0.22}\n",
      "{'loss': 0.5038, 'grad_norm': 0.16099151968955994, 'learning_rate': 0.00015602761550716942, 'epoch': 0.22}\n",
      "{'loss': 0.6738, 'grad_norm': 0.2191620171070099, 'learning_rate': 0.00015599221101079836, 'epoch': 0.22}\n",
      "{'loss': 0.9795, 'grad_norm': 0.2709907591342926, 'learning_rate': 0.00015595680651442735, 'epoch': 0.22}\n",
      "{'loss': 0.6089, 'grad_norm': 0.21062137186527252, 'learning_rate': 0.0001559214020180563, 'epoch': 0.22}\n",
      "{'loss': 1.0158, 'grad_norm': 0.28805091977119446, 'learning_rate': 0.00015588599752168526, 'epoch': 0.22}\n",
      "{'loss': 0.7855, 'grad_norm': 0.21295440196990967, 'learning_rate': 0.0001558505930253142, 'epoch': 0.22}\n",
      "{'loss': 0.6165, 'grad_norm': 0.22163765132427216, 'learning_rate': 0.0001558151885289432, 'epoch': 0.22}\n",
      "{'loss': 0.7737, 'grad_norm': 0.2331346571445465, 'learning_rate': 0.00015577978403257214, 'epoch': 0.22}\n",
      "{'loss': 0.7573, 'grad_norm': 0.18496446311473846, 'learning_rate': 0.0001557443795362011, 'epoch': 0.22}\n",
      "{'loss': 0.7051, 'grad_norm': 0.23200276494026184, 'learning_rate': 0.00015570897503983005, 'epoch': 0.22}\n",
      "{'loss': 0.5806, 'grad_norm': 0.1849365383386612, 'learning_rate': 0.00015567357054345905, 'epoch': 0.22}\n",
      "{'loss': 0.7697, 'grad_norm': 0.2979569435119629, 'learning_rate': 0.000155638166047088, 'epoch': 0.22}\n",
      "{'loss': 0.8725, 'grad_norm': 0.2589607238769531, 'learning_rate': 0.00015560276155071696, 'epoch': 0.22}\n",
      "{'loss': 0.739, 'grad_norm': 0.201655313372612, 'learning_rate': 0.0001555673570543459, 'epoch': 0.22}\n",
      "{'loss': 1.1582, 'grad_norm': 0.3160325884819031, 'learning_rate': 0.0001555319525579749, 'epoch': 0.22}\n",
      "{'loss': 0.6203, 'grad_norm': 0.19877800345420837, 'learning_rate': 0.00015549654806160383, 'epoch': 0.22}\n",
      "{'loss': 0.8291, 'grad_norm': 0.2136562615633011, 'learning_rate': 0.0001554611435652328, 'epoch': 0.22}\n",
      "{'loss': 0.5597, 'grad_norm': 0.22947251796722412, 'learning_rate': 0.00015542573906886174, 'epoch': 0.22}\n",
      "{'loss': 0.6641, 'grad_norm': 0.23641158640384674, 'learning_rate': 0.0001553903345724907, 'epoch': 0.22}\n",
      "{'loss': 0.7471, 'grad_norm': 0.23161554336547852, 'learning_rate': 0.00015535493007611968, 'epoch': 0.22}\n",
      "{'loss': 0.9177, 'grad_norm': 0.2008301168680191, 'learning_rate': 0.00015531952557974862, 'epoch': 0.22}\n",
      "{'loss': 0.7487, 'grad_norm': 0.25928518176078796, 'learning_rate': 0.0001552841210833776, 'epoch': 0.22}\n",
      "{'loss': 0.7446, 'grad_norm': 0.1997520625591278, 'learning_rate': 0.00015524871658700656, 'epoch': 0.22}\n",
      "{'loss': 0.8163, 'grad_norm': 0.2633386552333832, 'learning_rate': 0.00015521331209063553, 'epoch': 0.22}\n",
      "{'loss': 0.8101, 'grad_norm': 0.2008240818977356, 'learning_rate': 0.00015517790759426447, 'epoch': 0.22}\n",
      "{'loss': 0.8979, 'grad_norm': 0.30229777097702026, 'learning_rate': 0.00015514250309789344, 'epoch': 0.22}\n",
      "{'loss': 0.6707, 'grad_norm': 0.22680926322937012, 'learning_rate': 0.0001551070986015224, 'epoch': 0.23}\n",
      "{'loss': 0.6833, 'grad_norm': 0.25092393159866333, 'learning_rate': 0.00015507169410515137, 'epoch': 0.23}\n",
      "{'loss': 0.7561, 'grad_norm': 0.27725234627723694, 'learning_rate': 0.00015503628960878031, 'epoch': 0.23}\n",
      "{'loss': 0.6519, 'grad_norm': 0.23117031157016754, 'learning_rate': 0.00015500088511240928, 'epoch': 0.23}\n",
      "{'loss': 0.6279, 'grad_norm': 0.24607513844966888, 'learning_rate': 0.00015496548061603825, 'epoch': 0.23}\n",
      "{'loss': 0.5803, 'grad_norm': 0.23036444187164307, 'learning_rate': 0.00015493007611966722, 'epoch': 0.23}\n",
      "{'loss': 0.8832, 'grad_norm': 0.1900000423192978, 'learning_rate': 0.00015489467162329616, 'epoch': 0.23}\n",
      "{'loss': 0.6101, 'grad_norm': 0.23194344341754913, 'learning_rate': 0.00015485926712692513, 'epoch': 0.23}\n",
      "{'loss': 0.8722, 'grad_norm': 0.2677258849143982, 'learning_rate': 0.0001548238626305541, 'epoch': 0.23}\n",
      "{'loss': 0.7797, 'grad_norm': 0.2539367377758026, 'learning_rate': 0.00015478845813418307, 'epoch': 0.23}\n",
      "{'loss': 0.836, 'grad_norm': 0.24234969913959503, 'learning_rate': 0.000154753053637812, 'epoch': 0.23}\n",
      "{'loss': 0.5506, 'grad_norm': 0.2713561952114105, 'learning_rate': 0.00015471764914144097, 'epoch': 0.23}\n",
      "{'loss': 0.6078, 'grad_norm': 0.22109422087669373, 'learning_rate': 0.00015468224464506992, 'epoch': 0.23}\n",
      "{'loss': 0.6053, 'grad_norm': 0.21315667033195496, 'learning_rate': 0.0001546468401486989, 'epoch': 0.23}\n",
      "{'loss': 0.8313, 'grad_norm': 0.24095532298088074, 'learning_rate': 0.00015461143565232785, 'epoch': 0.23}\n",
      "{'loss': 0.6421, 'grad_norm': 0.19682729244232178, 'learning_rate': 0.00015457603115595682, 'epoch': 0.23}\n",
      "{'loss': 0.6703, 'grad_norm': 0.1825176179409027, 'learning_rate': 0.00015454062665958576, 'epoch': 0.23}\n",
      "{'loss': 0.6421, 'grad_norm': 0.19413091242313385, 'learning_rate': 0.00015450522216321476, 'epoch': 0.23}\n",
      "{'loss': 0.8495, 'grad_norm': 0.2636794447898865, 'learning_rate': 0.0001544698176668437, 'epoch': 0.23}\n",
      "{'loss': 0.877, 'grad_norm': 0.26895129680633545, 'learning_rate': 0.00015443441317047264, 'epoch': 0.23}\n",
      "{'loss': 0.6107, 'grad_norm': 0.20733915269374847, 'learning_rate': 0.0001543990086741016, 'epoch': 0.23}\n",
      "{'loss': 0.6831, 'grad_norm': 0.19173486530780792, 'learning_rate': 0.00015436360417773058, 'epoch': 0.23}\n",
      "{'loss': 0.6287, 'grad_norm': 0.2881137728691101, 'learning_rate': 0.00015432819968135955, 'epoch': 0.23}\n",
      "{'loss': 0.6139, 'grad_norm': 0.2232624590396881, 'learning_rate': 0.00015429279518498849, 'epoch': 0.23}\n",
      "{'loss': 0.8519, 'grad_norm': 0.2337712049484253, 'learning_rate': 0.00015425739068861745, 'epoch': 0.23}\n",
      "{'loss': 0.9551, 'grad_norm': 0.31082770228385925, 'learning_rate': 0.00015422198619224642, 'epoch': 0.23}\n",
      "{'loss': 0.8035, 'grad_norm': 0.24827834963798523, 'learning_rate': 0.0001541865816958754, 'epoch': 0.23}\n",
      "{'loss': 0.6806, 'grad_norm': 0.25723785161972046, 'learning_rate': 0.00015415117719950433, 'epoch': 0.23}\n",
      "{'loss': 0.8268, 'grad_norm': 0.28074780106544495, 'learning_rate': 0.0001541157727031333, 'epoch': 0.23}\n",
      "{'loss': 0.9796, 'grad_norm': 0.23862089216709137, 'learning_rate': 0.00015408036820676227, 'epoch': 0.23}\n",
      "{'loss': 0.6736, 'grad_norm': 0.21815764904022217, 'learning_rate': 0.00015404496371039124, 'epoch': 0.23}\n",
      "{'loss': 0.5859, 'grad_norm': 0.17755715548992157, 'learning_rate': 0.00015400955921402018, 'epoch': 0.23}\n",
      "{'loss': 0.7252, 'grad_norm': 0.2050037831068039, 'learning_rate': 0.00015397415471764915, 'epoch': 0.23}\n",
      "{'loss': 0.6268, 'grad_norm': 0.26778167486190796, 'learning_rate': 0.00015393875022127812, 'epoch': 0.23}\n",
      "{'loss': 0.805, 'grad_norm': 0.26898807287216187, 'learning_rate': 0.00015390334572490708, 'epoch': 0.23}\n",
      "{'loss': 0.7487, 'grad_norm': 0.1807631403207779, 'learning_rate': 0.00015386794122853602, 'epoch': 0.23}\n",
      "{'loss': 0.5651, 'grad_norm': 0.24506960809230804, 'learning_rate': 0.000153832536732165, 'epoch': 0.23}\n",
      "{'loss': 0.7567, 'grad_norm': 0.2321777045726776, 'learning_rate': 0.00015379713223579396, 'epoch': 0.23}\n",
      "{'loss': 0.4866, 'grad_norm': 0.21669460833072662, 'learning_rate': 0.00015376172773942293, 'epoch': 0.23}\n",
      "{'loss': 0.6007, 'grad_norm': 0.21263493597507477, 'learning_rate': 0.00015372632324305187, 'epoch': 0.23}\n",
      "{'loss': 0.9049, 'grad_norm': 0.246636301279068, 'learning_rate': 0.00015369091874668084, 'epoch': 0.23}\n",
      "{'loss': 0.8652, 'grad_norm': 0.2559899687767029, 'learning_rate': 0.0001536555142503098, 'epoch': 0.23}\n",
      "{'loss': 0.7871, 'grad_norm': 0.25113940238952637, 'learning_rate': 0.00015362010975393878, 'epoch': 0.23}\n",
      "{'loss': 1.1621, 'grad_norm': 0.3244817554950714, 'learning_rate': 0.00015358470525756772, 'epoch': 0.23}\n",
      "{'loss': 0.5779, 'grad_norm': 0.2389388531446457, 'learning_rate': 0.00015354930076119666, 'epoch': 0.23}\n",
      "{'loss': 0.748, 'grad_norm': 0.27000993490219116, 'learning_rate': 0.00015351389626482565, 'epoch': 0.23}\n",
      "{'loss': 0.7377, 'grad_norm': 0.21634121239185333, 'learning_rate': 0.0001534784917684546, 'epoch': 0.23}\n",
      "{'loss': 0.8608, 'grad_norm': 0.21704727411270142, 'learning_rate': 0.00015344308727208356, 'epoch': 0.23}\n",
      "{'loss': 0.88, 'grad_norm': 0.22909605503082275, 'learning_rate': 0.0001534076827757125, 'epoch': 0.23}\n",
      "{'loss': 0.5329, 'grad_norm': 0.220509335398674, 'learning_rate': 0.00015337227827934147, 'epoch': 0.23}\n",
      "{'loss': 0.9199, 'grad_norm': 0.28920137882232666, 'learning_rate': 0.00015333687378297044, 'epoch': 0.23}\n",
      "{'loss': 0.8054, 'grad_norm': 0.23710595071315765, 'learning_rate': 0.0001533014692865994, 'epoch': 0.23}\n",
      "{'loss': 0.7076, 'grad_norm': 0.20955637097358704, 'learning_rate': 0.00015326606479022835, 'epoch': 0.23}\n",
      "{'loss': 0.7795, 'grad_norm': 0.23913666605949402, 'learning_rate': 0.00015323066029385732, 'epoch': 0.23}\n",
      "{'loss': 0.6674, 'grad_norm': 0.2807677984237671, 'learning_rate': 0.0001531952557974863, 'epoch': 0.23}\n",
      "{'loss': 0.5663, 'grad_norm': 0.24305777251720428, 'learning_rate': 0.00015315985130111526, 'epoch': 0.23}\n",
      "{'loss': 0.6628, 'grad_norm': 0.26259708404541016, 'learning_rate': 0.0001531244468047442, 'epoch': 0.24}\n",
      "{'loss': 0.7776, 'grad_norm': 0.25915899872779846, 'learning_rate': 0.00015308904230837317, 'epoch': 0.24}\n",
      "{'loss': 0.7673, 'grad_norm': 0.3159503936767578, 'learning_rate': 0.00015305363781200213, 'epoch': 0.24}\n",
      "{'loss': 0.492, 'grad_norm': 0.15653449296951294, 'learning_rate': 0.0001530182333156311, 'epoch': 0.24}\n",
      "{'loss': 0.8417, 'grad_norm': 0.2513555586338043, 'learning_rate': 0.00015298282881926004, 'epoch': 0.24}\n",
      "{'loss': 0.801, 'grad_norm': 0.18889988958835602, 'learning_rate': 0.000152947424322889, 'epoch': 0.24}\n",
      "{'loss': 0.9537, 'grad_norm': 0.24049149453639984, 'learning_rate': 0.00015291201982651798, 'epoch': 0.24}\n",
      "{'loss': 0.8027, 'grad_norm': 0.2363480031490326, 'learning_rate': 0.00015287661533014695, 'epoch': 0.24}\n",
      "{'loss': 0.6445, 'grad_norm': 0.23260654509067535, 'learning_rate': 0.0001528412108337759, 'epoch': 0.24}\n",
      "{'loss': 0.7192, 'grad_norm': 0.22163517773151398, 'learning_rate': 0.00015280580633740486, 'epoch': 0.24}\n",
      "{'loss': 0.7181, 'grad_norm': 0.3095880448818207, 'learning_rate': 0.00015277040184103383, 'epoch': 0.24}\n",
      "{'loss': 0.6556, 'grad_norm': 0.19384115934371948, 'learning_rate': 0.0001527349973446628, 'epoch': 0.24}\n",
      "{'loss': 0.7877, 'grad_norm': 0.2595933973789215, 'learning_rate': 0.00015269959284829174, 'epoch': 0.24}\n",
      "{'loss': 0.7678, 'grad_norm': 0.22487549483776093, 'learning_rate': 0.0001526641883519207, 'epoch': 0.24}\n",
      "{'loss': 0.5716, 'grad_norm': 0.19884121417999268, 'learning_rate': 0.00015262878385554967, 'epoch': 0.24}\n",
      "{'loss': 0.6944, 'grad_norm': 0.2216484248638153, 'learning_rate': 0.0001525933793591786, 'epoch': 0.24}\n",
      "{'loss': 0.7692, 'grad_norm': 0.25213173031806946, 'learning_rate': 0.00015255797486280758, 'epoch': 0.24}\n",
      "{'loss': 0.8949, 'grad_norm': 0.2548965811729431, 'learning_rate': 0.00015252257036643652, 'epoch': 0.24}\n",
      "{'loss': 0.778, 'grad_norm': 0.19392001628875732, 'learning_rate': 0.00015248716587006552, 'epoch': 0.24}\n",
      "{'loss': 0.7812, 'grad_norm': 0.17725342512130737, 'learning_rate': 0.00015245176137369446, 'epoch': 0.24}\n",
      "{'loss': 0.959, 'grad_norm': 0.20382378995418549, 'learning_rate': 0.00015241635687732343, 'epoch': 0.24}\n",
      "{'loss': 0.8151, 'grad_norm': 0.20447929203510284, 'learning_rate': 0.00015238095238095237, 'epoch': 0.24}\n",
      "{'loss': 0.8231, 'grad_norm': 0.31503716111183167, 'learning_rate': 0.00015234554788458136, 'epoch': 0.24}\n",
      "{'loss': 0.6695, 'grad_norm': 0.2088373899459839, 'learning_rate': 0.0001523101433882103, 'epoch': 0.24}\n",
      "{'loss': 0.7926, 'grad_norm': 0.20738397538661957, 'learning_rate': 0.00015227473889183927, 'epoch': 0.24}\n",
      "{'loss': 0.7202, 'grad_norm': 0.22426554560661316, 'learning_rate': 0.00015223933439546822, 'epoch': 0.24}\n",
      "{'loss': 0.8495, 'grad_norm': 0.32109642028808594, 'learning_rate': 0.0001522039298990972, 'epoch': 0.24}\n",
      "{'loss': 0.6742, 'grad_norm': 0.18947917222976685, 'learning_rate': 0.00015216852540272615, 'epoch': 0.24}\n",
      "{'loss': 0.9062, 'grad_norm': 0.25273793935775757, 'learning_rate': 0.00015213312090635512, 'epoch': 0.24}\n",
      "{'loss': 0.6478, 'grad_norm': 0.20915786921977997, 'learning_rate': 0.00015209771640998406, 'epoch': 0.24}\n",
      "{'loss': 0.6846, 'grad_norm': 0.18806108832359314, 'learning_rate': 0.00015206231191361303, 'epoch': 0.24}\n",
      "{'loss': 0.5093, 'grad_norm': 0.17554768919944763, 'learning_rate': 0.000152026907417242, 'epoch': 0.24}\n",
      "{'loss': 0.6355, 'grad_norm': 0.20300188660621643, 'learning_rate': 0.00015199150292087097, 'epoch': 0.24}\n",
      "{'loss': 0.6862, 'grad_norm': 0.18160639703273773, 'learning_rate': 0.0001519560984244999, 'epoch': 0.24}\n",
      "{'loss': 0.5461, 'grad_norm': 0.22796645760536194, 'learning_rate': 0.00015192069392812888, 'epoch': 0.24}\n",
      "{'loss': 0.7379, 'grad_norm': 0.17654204368591309, 'learning_rate': 0.00015188528943175784, 'epoch': 0.24}\n",
      "{'loss': 0.9437, 'grad_norm': 0.25487563014030457, 'learning_rate': 0.0001518498849353868, 'epoch': 0.24}\n",
      "{'loss': 0.8335, 'grad_norm': 0.23323793709278107, 'learning_rate': 0.00015181448043901575, 'epoch': 0.24}\n",
      "{'loss': 0.7319, 'grad_norm': 0.2302442193031311, 'learning_rate': 0.00015177907594264472, 'epoch': 0.24}\n",
      "{'loss': 0.8107, 'grad_norm': 0.2400895059108734, 'learning_rate': 0.0001517436714462737, 'epoch': 0.24}\n",
      "{'loss': 0.9653, 'grad_norm': 0.295159250497818, 'learning_rate': 0.00015170826694990266, 'epoch': 0.24}\n",
      "{'loss': 0.7012, 'grad_norm': 0.23350466787815094, 'learning_rate': 0.0001516728624535316, 'epoch': 0.24}\n",
      "{'loss': 0.7574, 'grad_norm': 0.20318004488945007, 'learning_rate': 0.00015163745795716057, 'epoch': 0.24}\n",
      "{'loss': 0.8159, 'grad_norm': 0.24481618404388428, 'learning_rate': 0.00015160205346078954, 'epoch': 0.24}\n",
      "{'loss': 0.5771, 'grad_norm': 0.1874368041753769, 'learning_rate': 0.00015156664896441848, 'epoch': 0.24}\n",
      "{'loss': 0.7555, 'grad_norm': 0.17178291082382202, 'learning_rate': 0.00015153124446804745, 'epoch': 0.24}\n",
      "{'loss': 0.7262, 'grad_norm': 0.2865058481693268, 'learning_rate': 0.00015149583997167641, 'epoch': 0.24}\n",
      "{'loss': 0.8276, 'grad_norm': 0.1951821893453598, 'learning_rate': 0.00015146043547530538, 'epoch': 0.24}\n",
      "{'loss': 0.5738, 'grad_norm': 0.1722281575202942, 'learning_rate': 0.00015142503097893432, 'epoch': 0.24}\n",
      "{'loss': 0.6345, 'grad_norm': 0.21682344377040863, 'learning_rate': 0.0001513896264825633, 'epoch': 0.24}\n",
      "{'loss': 0.8653, 'grad_norm': 0.27635687589645386, 'learning_rate': 0.00015135422198619223, 'epoch': 0.24}\n",
      "{'loss': 0.6336, 'grad_norm': 0.19331488013267517, 'learning_rate': 0.00015131881748982123, 'epoch': 0.24}\n",
      "{'loss': 0.7887, 'grad_norm': 0.23733627796173096, 'learning_rate': 0.00015128341299345017, 'epoch': 0.24}\n",
      "{'loss': 0.7263, 'grad_norm': 0.2572394907474518, 'learning_rate': 0.00015124800849707914, 'epoch': 0.24}\n",
      "{'loss': 0.5182, 'grad_norm': 0.17544440925121307, 'learning_rate': 0.00015121260400070808, 'epoch': 0.24}\n",
      "{'loss': 0.9188, 'grad_norm': 0.28596144914627075, 'learning_rate': 0.00015117719950433707, 'epoch': 0.24}\n",
      "{'loss': 0.6512, 'grad_norm': 0.2078787386417389, 'learning_rate': 0.00015114179500796602, 'epoch': 0.24}\n",
      "{'loss': 0.7339, 'grad_norm': 0.19415001571178436, 'learning_rate': 0.00015110639051159498, 'epoch': 0.25}\n",
      "{'loss': 0.8057, 'grad_norm': 0.26143020391464233, 'learning_rate': 0.00015107098601522393, 'epoch': 0.25}\n",
      "{'loss': 1.1057, 'grad_norm': 0.21193507313728333, 'learning_rate': 0.00015103558151885292, 'epoch': 0.25}\n",
      "{'loss': 0.6592, 'grad_norm': 0.19480280578136444, 'learning_rate': 0.00015100017702248186, 'epoch': 0.25}\n",
      "{'loss': 0.6718, 'grad_norm': 0.1986784189939499, 'learning_rate': 0.00015096477252611083, 'epoch': 0.25}\n",
      "{'loss': 0.7054, 'grad_norm': 0.26002606749534607, 'learning_rate': 0.00015092936802973977, 'epoch': 0.25}\n",
      "{'loss': 0.81, 'grad_norm': 0.31136059761047363, 'learning_rate': 0.00015089396353336877, 'epoch': 0.25}\n",
      "{'loss': 0.9868, 'grad_norm': 0.23808333277702332, 'learning_rate': 0.0001508585590369977, 'epoch': 0.25}\n",
      "{'loss': 0.9263, 'grad_norm': 0.23278823494911194, 'learning_rate': 0.00015082315454062668, 'epoch': 0.25}\n",
      "{'loss': 0.8001, 'grad_norm': 0.18351022899150848, 'learning_rate': 0.00015078775004425562, 'epoch': 0.25}\n",
      "{'loss': 0.6447, 'grad_norm': 0.20111481845378876, 'learning_rate': 0.00015075234554788459, 'epoch': 0.25}\n",
      "{'loss': 0.6984, 'grad_norm': 0.227918341755867, 'learning_rate': 0.00015071694105151355, 'epoch': 0.25}\n",
      "{'loss': 0.8266, 'grad_norm': 0.24787935614585876, 'learning_rate': 0.0001506815365551425, 'epoch': 0.25}\n",
      "{'loss': 0.5391, 'grad_norm': 0.1794378012418747, 'learning_rate': 0.00015064613205877146, 'epoch': 0.25}\n",
      "{'loss': 1.4357, 'grad_norm': 0.289655864238739, 'learning_rate': 0.00015061072756240043, 'epoch': 0.25}\n",
      "{'loss': 0.7433, 'grad_norm': 0.22710508108139038, 'learning_rate': 0.0001505753230660294, 'epoch': 0.25}\n",
      "{'loss': 0.8323, 'grad_norm': 0.24063558876514435, 'learning_rate': 0.00015053991856965834, 'epoch': 0.25}\n",
      "{'loss': 0.6047, 'grad_norm': 0.18113504350185394, 'learning_rate': 0.0001505045140732873, 'epoch': 0.25}\n",
      "{'loss': 0.6495, 'grad_norm': 0.2413153201341629, 'learning_rate': 0.00015046910957691628, 'epoch': 0.25}\n",
      "{'loss': 0.8223, 'grad_norm': 0.20053690671920776, 'learning_rate': 0.00015043370508054525, 'epoch': 0.25}\n",
      "{'loss': 0.7757, 'grad_norm': 0.22993819415569305, 'learning_rate': 0.0001503983005841742, 'epoch': 0.25}\n",
      "{'loss': 0.6787, 'grad_norm': 0.2364305704832077, 'learning_rate': 0.00015036289608780316, 'epoch': 0.25}\n",
      "{'loss': 0.6851, 'grad_norm': 0.2016603797674179, 'learning_rate': 0.00015032749159143212, 'epoch': 0.25}\n",
      "{'loss': 0.9312, 'grad_norm': 0.21319365501403809, 'learning_rate': 0.0001502920870950611, 'epoch': 0.25}\n",
      "{'loss': 0.9362, 'grad_norm': 0.2761240303516388, 'learning_rate': 0.00015025668259869003, 'epoch': 0.25}\n",
      "{'loss': 0.7519, 'grad_norm': 0.21170543134212494, 'learning_rate': 0.000150221278102319, 'epoch': 0.25}\n",
      "{'loss': 0.5463, 'grad_norm': 0.2782925069332123, 'learning_rate': 0.00015018587360594797, 'epoch': 0.25}\n",
      "{'loss': 0.6282, 'grad_norm': 0.229813352227211, 'learning_rate': 0.00015015046910957694, 'epoch': 0.25}\n",
      "{'loss': 0.5993, 'grad_norm': 0.37315934896469116, 'learning_rate': 0.00015011506461320588, 'epoch': 0.25}\n",
      "{'loss': 0.73, 'grad_norm': 0.2072601467370987, 'learning_rate': 0.00015007966011683485, 'epoch': 0.25}\n",
      "{'loss': 0.6102, 'grad_norm': 0.21364066004753113, 'learning_rate': 0.0001500442556204638, 'epoch': 0.25}\n",
      "{'loss': 1.0373, 'grad_norm': 0.2587992548942566, 'learning_rate': 0.00015000885112409279, 'epoch': 0.25}\n",
      "{'loss': 0.515, 'grad_norm': 0.2345203012228012, 'learning_rate': 0.00014997344662772173, 'epoch': 0.25}\n",
      "{'loss': 1.104, 'grad_norm': 0.22064852714538574, 'learning_rate': 0.0001499380421313507, 'epoch': 0.25}\n",
      "{'loss': 0.7527, 'grad_norm': 0.23915763199329376, 'learning_rate': 0.00014990263763497964, 'epoch': 0.25}\n",
      "{'loss': 0.6117, 'grad_norm': 0.21806779503822327, 'learning_rate': 0.00014986723313860863, 'epoch': 0.25}\n",
      "{'loss': 0.6583, 'grad_norm': 0.2682933211326599, 'learning_rate': 0.00014983182864223757, 'epoch': 0.25}\n",
      "{'loss': 0.9452, 'grad_norm': 0.2197882980108261, 'learning_rate': 0.00014979642414586654, 'epoch': 0.25}\n",
      "{'loss': 0.819, 'grad_norm': 0.15732930600643158, 'learning_rate': 0.00014976101964949548, 'epoch': 0.25}\n",
      "{'loss': 0.7506, 'grad_norm': 0.22513070702552795, 'learning_rate': 0.00014972561515312445, 'epoch': 0.25}\n",
      "{'loss': 0.8287, 'grad_norm': 0.20115657150745392, 'learning_rate': 0.00014969021065675342, 'epoch': 0.25}\n",
      "{'loss': 0.92, 'grad_norm': 0.2520919442176819, 'learning_rate': 0.00014965480616038236, 'epoch': 0.25}\n",
      "{'loss': 0.916, 'grad_norm': 0.31276533007621765, 'learning_rate': 0.00014961940166401133, 'epoch': 0.25}\n",
      "{'loss': 0.9877, 'grad_norm': 0.29761677980422974, 'learning_rate': 0.0001495839971676403, 'epoch': 0.25}\n",
      "{'loss': 0.5103, 'grad_norm': 0.19049406051635742, 'learning_rate': 0.00014954859267126927, 'epoch': 0.25}\n",
      "{'loss': 0.9447, 'grad_norm': 0.2478533685207367, 'learning_rate': 0.0001495131881748982, 'epoch': 0.25}\n",
      "{'loss': 0.8018, 'grad_norm': 0.22682791948318481, 'learning_rate': 0.00014947778367852717, 'epoch': 0.25}\n",
      "{'loss': 0.8668, 'grad_norm': 0.24378004670143127, 'learning_rate': 0.00014944237918215614, 'epoch': 0.25}\n",
      "{'loss': 0.6025, 'grad_norm': 0.16355912387371063, 'learning_rate': 0.0001494069746857851, 'epoch': 0.25}\n",
      "{'loss': 0.7081, 'grad_norm': 0.1857200413942337, 'learning_rate': 0.00014937157018941405, 'epoch': 0.25}\n",
      "{'loss': 0.5057, 'grad_norm': 0.19468708336353302, 'learning_rate': 0.00014933616569304302, 'epoch': 0.25}\n",
      "{'loss': 1.2004, 'grad_norm': 0.19885267317295074, 'learning_rate': 0.000149300761196672, 'epoch': 0.25}\n",
      "{'loss': 0.6344, 'grad_norm': 0.20633935928344727, 'learning_rate': 0.00014926535670030096, 'epoch': 0.25}\n",
      "{'loss': 1.26, 'grad_norm': 0.2982032597064972, 'learning_rate': 0.0001492299522039299, 'epoch': 0.25}\n",
      "{'loss': 0.6677, 'grad_norm': 0.2201911062002182, 'learning_rate': 0.00014919454770755887, 'epoch': 0.25}\n",
      "{'loss': 0.6956, 'grad_norm': 0.2717716693878174, 'learning_rate': 0.00014915914321118784, 'epoch': 0.25}\n",
      "{'loss': 0.7256, 'grad_norm': 0.19060009717941284, 'learning_rate': 0.0001491237387148168, 'epoch': 0.26}\n",
      "{'loss': 0.6299, 'grad_norm': 0.19019639492034912, 'learning_rate': 0.00014908833421844574, 'epoch': 0.26}\n",
      "{'loss': 0.6882, 'grad_norm': 0.20794561505317688, 'learning_rate': 0.0001490529297220747, 'epoch': 0.26}\n",
      "{'loss': 1.2658, 'grad_norm': 0.3012785315513611, 'learning_rate': 0.00014901752522570368, 'epoch': 0.26}\n",
      "{'loss': 0.6358, 'grad_norm': 0.21975435316562653, 'learning_rate': 0.00014898212072933265, 'epoch': 0.26}\n",
      "{'loss': 0.5398, 'grad_norm': 0.16343581676483154, 'learning_rate': 0.0001489467162329616, 'epoch': 0.26}\n",
      "{'loss': 0.4986, 'grad_norm': 0.16808456182479858, 'learning_rate': 0.00014891131173659056, 'epoch': 0.26}\n",
      "{'loss': 1.1631, 'grad_norm': 0.23097103834152222, 'learning_rate': 0.00014887590724021953, 'epoch': 0.26}\n",
      "{'loss': 0.9038, 'grad_norm': 0.2272012084722519, 'learning_rate': 0.0001488405027438485, 'epoch': 0.26}\n",
      "{'loss': 0.6523, 'grad_norm': 0.23749715089797974, 'learning_rate': 0.00014880509824747744, 'epoch': 0.26}\n",
      "{'loss': 0.5652, 'grad_norm': 0.19588400423526764, 'learning_rate': 0.00014876969375110638, 'epoch': 0.26}\n",
      "{'loss': 0.5776, 'grad_norm': 0.19843155145645142, 'learning_rate': 0.00014873428925473535, 'epoch': 0.26}\n",
      "{'loss': 0.745, 'grad_norm': 0.22420960664749146, 'learning_rate': 0.00014869888475836432, 'epoch': 0.26}\n",
      "{'loss': 0.6733, 'grad_norm': 0.1969747245311737, 'learning_rate': 0.00014866348026199328, 'epoch': 0.26}\n",
      "{'loss': 0.6783, 'grad_norm': 0.28818202018737793, 'learning_rate': 0.00014862807576562222, 'epoch': 0.26}\n",
      "{'loss': 0.701, 'grad_norm': 0.2138931155204773, 'learning_rate': 0.0001485926712692512, 'epoch': 0.26}\n",
      "{'loss': 0.5156, 'grad_norm': 0.1858362853527069, 'learning_rate': 0.00014855726677288016, 'epoch': 0.26}\n",
      "{'loss': 0.8019, 'grad_norm': 0.26061829924583435, 'learning_rate': 0.00014852186227650913, 'epoch': 0.26}\n",
      "{'loss': 0.6176, 'grad_norm': 0.19014517962932587, 'learning_rate': 0.00014848645778013807, 'epoch': 0.26}\n",
      "{'loss': 1.0422, 'grad_norm': 0.19676053524017334, 'learning_rate': 0.00014845105328376704, 'epoch': 0.26}\n",
      "{'loss': 0.7948, 'grad_norm': 0.2546751797199249, 'learning_rate': 0.000148415648787396, 'epoch': 0.26}\n",
      "{'loss': 0.5517, 'grad_norm': 0.20766288042068481, 'learning_rate': 0.00014838024429102498, 'epoch': 0.26}\n",
      "{'loss': 0.6525, 'grad_norm': 0.22394369542598724, 'learning_rate': 0.00014834483979465392, 'epoch': 0.26}\n",
      "{'loss': 1.003, 'grad_norm': 0.2765600085258484, 'learning_rate': 0.00014830943529828289, 'epoch': 0.26}\n",
      "{'loss': 0.6088, 'grad_norm': 0.21075068414211273, 'learning_rate': 0.00014827403080191185, 'epoch': 0.26}\n",
      "{'loss': 0.7716, 'grad_norm': 0.2147427201271057, 'learning_rate': 0.00014823862630554082, 'epoch': 0.26}\n",
      "{'loss': 0.634, 'grad_norm': 0.20641294121742249, 'learning_rate': 0.00014820322180916976, 'epoch': 0.26}\n",
      "{'loss': 0.8639, 'grad_norm': 0.21944548189640045, 'learning_rate': 0.00014816781731279873, 'epoch': 0.26}\n",
      "{'loss': 0.8067, 'grad_norm': 0.2563226521015167, 'learning_rate': 0.0001481324128164277, 'epoch': 0.26}\n",
      "{'loss': 0.7825, 'grad_norm': 0.30132412910461426, 'learning_rate': 0.00014809700832005667, 'epoch': 0.26}\n",
      "{'loss': 0.5199, 'grad_norm': 0.2202112078666687, 'learning_rate': 0.0001480616038236856, 'epoch': 0.26}\n",
      "{'loss': 1.1372, 'grad_norm': 0.32242244482040405, 'learning_rate': 0.00014802619932731458, 'epoch': 0.26}\n",
      "{'loss': 0.6841, 'grad_norm': 0.21563470363616943, 'learning_rate': 0.00014799079483094355, 'epoch': 0.26}\n",
      "{'loss': 0.6412, 'grad_norm': 0.21070536971092224, 'learning_rate': 0.00014795539033457251, 'epoch': 0.26}\n",
      "{'loss': 0.7622, 'grad_norm': 0.2191610187292099, 'learning_rate': 0.00014791998583820146, 'epoch': 0.26}\n",
      "{'loss': 0.7716, 'grad_norm': 0.2688773274421692, 'learning_rate': 0.00014788458134183042, 'epoch': 0.26}\n",
      "{'loss': 0.525, 'grad_norm': 0.2756035625934601, 'learning_rate': 0.0001478491768454594, 'epoch': 0.26}\n",
      "{'loss': 0.8104, 'grad_norm': 0.2534715533256531, 'learning_rate': 0.00014781377234908833, 'epoch': 0.26}\n",
      "{'loss': 0.9206, 'grad_norm': 0.31133684515953064, 'learning_rate': 0.0001477783678527173, 'epoch': 0.26}\n",
      "{'loss': 0.8404, 'grad_norm': 0.24748185276985168, 'learning_rate': 0.00014774296335634624, 'epoch': 0.26}\n",
      "{'loss': 0.5912, 'grad_norm': 0.22835303843021393, 'learning_rate': 0.00014770755885997524, 'epoch': 0.26}\n",
      "{'loss': 0.9533, 'grad_norm': 0.1916041076183319, 'learning_rate': 0.00014767215436360418, 'epoch': 0.26}\n",
      "{'loss': 0.6569, 'grad_norm': 0.2549031674861908, 'learning_rate': 0.00014763674986723315, 'epoch': 0.26}\n",
      "{'loss': 0.5825, 'grad_norm': 0.24030719697475433, 'learning_rate': 0.0001476013453708621, 'epoch': 0.26}\n",
      "{'loss': 0.6352, 'grad_norm': 0.2044142782688141, 'learning_rate': 0.00014756594087449108, 'epoch': 0.26}\n",
      "{'loss': 0.6369, 'grad_norm': 0.1935693621635437, 'learning_rate': 0.00014753053637812003, 'epoch': 0.26}\n",
      "{'loss': 0.6669, 'grad_norm': 0.21834668517112732, 'learning_rate': 0.000147495131881749, 'epoch': 0.26}\n",
      "{'loss': 0.7199, 'grad_norm': 0.26235586404800415, 'learning_rate': 0.00014745972738537794, 'epoch': 0.26}\n",
      "{'loss': 0.9813, 'grad_norm': 0.20112545788288116, 'learning_rate': 0.0001474243228890069, 'epoch': 0.26}\n",
      "{'loss': 0.6849, 'grad_norm': 0.24790897965431213, 'learning_rate': 0.00014738891839263587, 'epoch': 0.26}\n",
      "{'loss': 0.5339, 'grad_norm': 0.21617712080478668, 'learning_rate': 0.00014735351389626484, 'epoch': 0.26}\n",
      "{'loss': 0.6971, 'grad_norm': 0.1775180548429489, 'learning_rate': 0.00014731810939989378, 'epoch': 0.26}\n",
      "{'loss': 0.811, 'grad_norm': 0.19421431422233582, 'learning_rate': 0.00014728270490352275, 'epoch': 0.26}\n",
      "{'loss': 0.9707, 'grad_norm': 0.21333888173103333, 'learning_rate': 0.00014724730040715172, 'epoch': 0.26}\n",
      "{'loss': 0.9258, 'grad_norm': 0.21993304789066315, 'learning_rate': 0.00014721189591078069, 'epoch': 0.26}\n",
      "{'loss': 0.89, 'grad_norm': 0.2298220992088318, 'learning_rate': 0.00014717649141440963, 'epoch': 0.26}\n",
      "{'loss': 0.9483, 'grad_norm': 0.25218555331230164, 'learning_rate': 0.0001471410869180386, 'epoch': 0.26}\n",
      "{'loss': 0.6141, 'grad_norm': 0.1789647340774536, 'learning_rate': 0.00014710568242166756, 'epoch': 0.27}\n",
      "{'loss': 0.677, 'grad_norm': 0.288944274187088, 'learning_rate': 0.00014707027792529653, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee37608ac0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: bc69cf88-8dc2-4611-a562-b59eed706e07)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8498, 'grad_norm': 0.2456086277961731, 'learning_rate': 0.00014703487342892547, 'epoch': 0.27}\n",
      "{'loss': 0.7344, 'grad_norm': 0.23290085792541504, 'learning_rate': 0.00014699946893255444, 'epoch': 0.27}\n",
      "{'loss': 0.7642, 'grad_norm': 0.20329609513282776, 'learning_rate': 0.0001469640644361834, 'epoch': 0.27}\n",
      "{'loss': 0.6411, 'grad_norm': 0.2410823106765747, 'learning_rate': 0.00014692865993981238, 'epoch': 0.27}\n",
      "{'loss': 0.9118, 'grad_norm': 0.2228451520204544, 'learning_rate': 0.00014689325544344132, 'epoch': 0.27}\n",
      "{'loss': 0.7991, 'grad_norm': 0.3148958384990692, 'learning_rate': 0.00014685785094707026, 'epoch': 0.27}\n",
      "{'loss': 0.8972, 'grad_norm': 0.21027664840221405, 'learning_rate': 0.00014682244645069926, 'epoch': 0.27}\n",
      "{'loss': 0.6111, 'grad_norm': 0.258932888507843, 'learning_rate': 0.0001467870419543282, 'epoch': 0.27}\n",
      "{'loss': 0.6503, 'grad_norm': 0.20580044388771057, 'learning_rate': 0.00014675163745795717, 'epoch': 0.27}\n",
      "{'loss': 0.7234, 'grad_norm': 0.2476489096879959, 'learning_rate': 0.0001467162329615861, 'epoch': 0.27}\n",
      "{'loss': 0.8019, 'grad_norm': 0.21284618973731995, 'learning_rate': 0.0001466808284652151, 'epoch': 0.27}\n",
      "{'loss': 0.6929, 'grad_norm': 0.17199921607971191, 'learning_rate': 0.00014664542396884404, 'epoch': 0.27}\n",
      "{'loss': 0.7792, 'grad_norm': 0.2815849184989929, 'learning_rate': 0.000146610019472473, 'epoch': 0.27}\n",
      "{'loss': 0.6289, 'grad_norm': 0.19593006372451782, 'learning_rate': 0.00014657461497610195, 'epoch': 0.27}\n",
      "{'loss': 1.0243, 'grad_norm': 0.23526529967784882, 'learning_rate': 0.00014653921047973095, 'epoch': 0.27}\n",
      "{'loss': 0.7604, 'grad_norm': 0.2913140058517456, 'learning_rate': 0.0001465038059833599, 'epoch': 0.27}\n",
      "{'loss': 0.7747, 'grad_norm': 0.27498680353164673, 'learning_rate': 0.00014646840148698886, 'epoch': 0.27}\n",
      "{'loss': 0.7021, 'grad_norm': 0.21991923451423645, 'learning_rate': 0.0001464329969906178, 'epoch': 0.27}\n",
      "{'loss': 0.7044, 'grad_norm': 0.2559138834476471, 'learning_rate': 0.0001463975924942468, 'epoch': 0.27}\n",
      "{'loss': 0.826, 'grad_norm': 0.27795466780662537, 'learning_rate': 0.00014636218799787574, 'epoch': 0.27}\n",
      "{'loss': 0.7485, 'grad_norm': 0.2232242226600647, 'learning_rate': 0.0001463267835015047, 'epoch': 0.27}\n",
      "{'loss': 0.9057, 'grad_norm': 0.25255653262138367, 'learning_rate': 0.00014629137900513365, 'epoch': 0.27}\n",
      "{'loss': 0.7686, 'grad_norm': 0.25514036417007446, 'learning_rate': 0.00014625597450876264, 'epoch': 0.27}\n",
      "{'loss': 0.5382, 'grad_norm': 0.16857407987117767, 'learning_rate': 0.00014622057001239158, 'epoch': 0.27}\n",
      "{'loss': 1.0709, 'grad_norm': 0.21305523812770844, 'learning_rate': 0.00014618516551602055, 'epoch': 0.27}\n",
      "{'loss': 0.762, 'grad_norm': 0.24888622760772705, 'learning_rate': 0.0001461497610196495, 'epoch': 0.27}\n",
      "{'loss': 0.8665, 'grad_norm': 0.19444358348846436, 'learning_rate': 0.00014611435652327846, 'epoch': 0.27}\n",
      "{'loss': 0.7384, 'grad_norm': 0.2249828279018402, 'learning_rate': 0.00014607895202690743, 'epoch': 0.27}\n",
      "{'loss': 0.807, 'grad_norm': 0.23381352424621582, 'learning_rate': 0.0001460435475305364, 'epoch': 0.27}\n",
      "{'loss': 0.797, 'grad_norm': 0.2795560956001282, 'learning_rate': 0.00014600814303416534, 'epoch': 0.27}\n",
      "{'loss': 0.7653, 'grad_norm': 0.19977925717830658, 'learning_rate': 0.0001459727385377943, 'epoch': 0.27}\n",
      "{'loss': 0.9511, 'grad_norm': 0.2796512842178345, 'learning_rate': 0.00014593733404142327, 'epoch': 0.27}\n",
      "{'loss': 0.6879, 'grad_norm': 0.3077372610569, 'learning_rate': 0.00014590192954505222, 'epoch': 0.27}\n",
      "{'loss': 0.6337, 'grad_norm': 0.1894015222787857, 'learning_rate': 0.00014586652504868118, 'epoch': 0.27}\n",
      "{'loss': 0.7558, 'grad_norm': 0.23691397905349731, 'learning_rate': 0.00014583112055231015, 'epoch': 0.27}\n",
      "{'loss': 0.7019, 'grad_norm': 0.20240923762321472, 'learning_rate': 0.00014579571605593912, 'epoch': 0.27}\n",
      "{'loss': 0.7064, 'grad_norm': 0.31922221183776855, 'learning_rate': 0.00014576031155956806, 'epoch': 0.27}\n",
      "{'loss': 0.8339, 'grad_norm': 0.24760697782039642, 'learning_rate': 0.00014572490706319703, 'epoch': 0.27}\n",
      "{'loss': 0.5823, 'grad_norm': 0.1836235374212265, 'learning_rate': 0.000145689502566826, 'epoch': 0.27}\n",
      "{'loss': 0.6095, 'grad_norm': 0.25431370735168457, 'learning_rate': 0.00014565409807045497, 'epoch': 0.27}\n",
      "{'loss': 0.9216, 'grad_norm': 0.22469662129878998, 'learning_rate': 0.0001456186935740839, 'epoch': 0.27}\n",
      "{'loss': 0.6456, 'grad_norm': 0.21366795897483826, 'learning_rate': 0.00014558328907771288, 'epoch': 0.27}\n",
      "{'loss': 0.608, 'grad_norm': 0.2262149602174759, 'learning_rate': 0.00014554788458134182, 'epoch': 0.27}\n",
      "{'loss': 0.9108, 'grad_norm': 0.23435011506080627, 'learning_rate': 0.0001455124800849708, 'epoch': 0.27}\n",
      "{'loss': 0.8825, 'grad_norm': 0.23121659457683563, 'learning_rate': 0.00014547707558859975, 'epoch': 0.27}\n",
      "{'loss': 0.6433, 'grad_norm': 0.32784900069236755, 'learning_rate': 0.00014544167109222872, 'epoch': 0.27}\n",
      "{'loss': 0.8978, 'grad_norm': 0.3151567876338959, 'learning_rate': 0.00014540626659585766, 'epoch': 0.27}\n",
      "{'loss': 0.724, 'grad_norm': 0.2605477571487427, 'learning_rate': 0.00014537086209948666, 'epoch': 0.27}\n",
      "{'loss': 0.8519, 'grad_norm': 0.29219934344291687, 'learning_rate': 0.0001453354576031156, 'epoch': 0.27}\n",
      "{'loss': 0.6615, 'grad_norm': 0.24630697071552277, 'learning_rate': 0.00014530005310674457, 'epoch': 0.27}\n",
      "{'loss': 0.6471, 'grad_norm': 0.17746950685977936, 'learning_rate': 0.0001452646486103735, 'epoch': 0.27}\n",
      "{'loss': 0.915, 'grad_norm': 0.25025254487991333, 'learning_rate': 0.0001452292441140025, 'epoch': 0.27}\n",
      "{'loss': 0.8993, 'grad_norm': 0.29837992787361145, 'learning_rate': 0.00014519383961763145, 'epoch': 0.27}\n",
      "{'loss': 1.1472, 'grad_norm': 0.26943516731262207, 'learning_rate': 0.00014515843512126042, 'epoch': 0.27}\n",
      "{'loss': 0.6013, 'grad_norm': 0.16892801225185394, 'learning_rate': 0.00014512303062488936, 'epoch': 0.28}\n",
      "{'loss': 0.9221, 'grad_norm': 0.3091247081756592, 'learning_rate': 0.00014508762612851835, 'epoch': 0.28}\n",
      "{'loss': 0.4015, 'grad_norm': 0.26314014196395874, 'learning_rate': 0.0001450522216321473, 'epoch': 0.28}\n",
      "{'loss': 0.6742, 'grad_norm': 0.24561819434165955, 'learning_rate': 0.00014501681713577623, 'epoch': 0.28}\n",
      "{'loss': 0.5547, 'grad_norm': 0.2312760055065155, 'learning_rate': 0.0001449814126394052, 'epoch': 0.28}\n",
      "{'loss': 0.6064, 'grad_norm': 0.23572877049446106, 'learning_rate': 0.00014494600814303417, 'epoch': 0.28}\n",
      "{'loss': 0.9239, 'grad_norm': 0.21921421587467194, 'learning_rate': 0.00014491060364666314, 'epoch': 0.28}\n",
      "{'loss': 0.7003, 'grad_norm': 0.23626965284347534, 'learning_rate': 0.00014487519915029208, 'epoch': 0.28}\n",
      "{'loss': 0.9296, 'grad_norm': 0.2717006206512451, 'learning_rate': 0.00014483979465392105, 'epoch': 0.28}\n",
      "{'loss': 0.9277, 'grad_norm': 0.2694796919822693, 'learning_rate': 0.00014480439015755002, 'epoch': 0.28}\n",
      "{'loss': 0.9152, 'grad_norm': 0.3064766228199005, 'learning_rate': 0.00014476898566117899, 'epoch': 0.28}\n",
      "{'loss': 0.7473, 'grad_norm': 0.19890302419662476, 'learning_rate': 0.00014473358116480793, 'epoch': 0.28}\n",
      "{'loss': 0.6675, 'grad_norm': 0.23209945857524872, 'learning_rate': 0.0001446981766684369, 'epoch': 0.28}\n",
      "{'loss': 0.6821, 'grad_norm': 0.21784335374832153, 'learning_rate': 0.00014466277217206586, 'epoch': 0.28}\n",
      "{'loss': 0.6922, 'grad_norm': 0.1933836042881012, 'learning_rate': 0.00014462736767569483, 'epoch': 0.28}\n",
      "{'loss': 0.985, 'grad_norm': 0.27191680669784546, 'learning_rate': 0.00014459196317932377, 'epoch': 0.28}\n",
      "{'loss': 0.553, 'grad_norm': 0.16222774982452393, 'learning_rate': 0.00014455655868295274, 'epoch': 0.28}\n",
      "{'loss': 0.6725, 'grad_norm': 0.2526126503944397, 'learning_rate': 0.0001445211541865817, 'epoch': 0.28}\n",
      "{'loss': 0.8505, 'grad_norm': 0.28760138154029846, 'learning_rate': 0.00014448574969021068, 'epoch': 0.28}\n",
      "{'loss': 0.6722, 'grad_norm': 0.2337302416563034, 'learning_rate': 0.00014445034519383962, 'epoch': 0.28}\n",
      "{'loss': 0.6822, 'grad_norm': 0.2302510142326355, 'learning_rate': 0.0001444149406974686, 'epoch': 0.28}\n",
      "{'loss': 0.7386, 'grad_norm': 0.2231627106666565, 'learning_rate': 0.00014437953620109756, 'epoch': 0.28}\n",
      "{'loss': 0.8696, 'grad_norm': 0.21600441634655, 'learning_rate': 0.00014434413170472652, 'epoch': 0.28}\n",
      "{'loss': 1.2084, 'grad_norm': 0.28059670329093933, 'learning_rate': 0.00014430872720835546, 'epoch': 0.28}\n",
      "{'loss': 0.9368, 'grad_norm': 0.194243922829628, 'learning_rate': 0.00014427332271198443, 'epoch': 0.28}\n",
      "{'loss': 0.6044, 'grad_norm': 0.2327588051557541, 'learning_rate': 0.00014423791821561337, 'epoch': 0.28}\n",
      "{'loss': 0.7199, 'grad_norm': 0.2637649178504944, 'learning_rate': 0.00014420251371924237, 'epoch': 0.28}\n",
      "{'loss': 1.1537, 'grad_norm': 0.23391243815422058, 'learning_rate': 0.0001441671092228713, 'epoch': 0.28}\n",
      "{'loss': 0.7083, 'grad_norm': 0.25398963689804077, 'learning_rate': 0.00014413170472650028, 'epoch': 0.28}\n",
      "{'loss': 0.884, 'grad_norm': 0.21681709587574005, 'learning_rate': 0.00014409630023012922, 'epoch': 0.28}\n",
      "{'loss': 0.5492, 'grad_norm': 0.1926400512456894, 'learning_rate': 0.0001440608957337582, 'epoch': 0.28}\n",
      "{'loss': 0.8503, 'grad_norm': 0.23882608115673065, 'learning_rate': 0.00014402549123738716, 'epoch': 0.28}\n",
      "{'loss': 0.7582, 'grad_norm': 0.20843680202960968, 'learning_rate': 0.0001439900867410161, 'epoch': 0.28}\n",
      "{'loss': 0.5149, 'grad_norm': 0.22804009914398193, 'learning_rate': 0.00014395468224464507, 'epoch': 0.28}\n",
      "{'loss': 0.5724, 'grad_norm': 0.2646639943122864, 'learning_rate': 0.00014391927774827404, 'epoch': 0.28}\n",
      "{'loss': 0.7144, 'grad_norm': 0.21292798221111298, 'learning_rate': 0.000143883873251903, 'epoch': 0.28}\n",
      "{'loss': 0.9974, 'grad_norm': 0.24454759061336517, 'learning_rate': 0.00014384846875553194, 'epoch': 0.28}\n",
      "{'loss': 0.6357, 'grad_norm': 0.2710694670677185, 'learning_rate': 0.0001438130642591609, 'epoch': 0.28}\n",
      "{'loss': 0.687, 'grad_norm': 0.2617715001106262, 'learning_rate': 0.00014377765976278988, 'epoch': 0.28}\n",
      "{'loss': 0.6611, 'grad_norm': 0.21881842613220215, 'learning_rate': 0.00014374225526641885, 'epoch': 0.28}\n",
      "{'loss': 1.0198, 'grad_norm': 0.21382473409175873, 'learning_rate': 0.0001437068507700478, 'epoch': 0.28}\n",
      "{'loss': 0.9479, 'grad_norm': 0.24218304455280304, 'learning_rate': 0.00014367144627367676, 'epoch': 0.28}\n",
      "{'loss': 0.6711, 'grad_norm': 0.21373413503170013, 'learning_rate': 0.00014363604177730573, 'epoch': 0.28}\n",
      "{'loss': 0.7929, 'grad_norm': 0.2399468570947647, 'learning_rate': 0.0001436006372809347, 'epoch': 0.28}\n",
      "{'loss': 0.8346, 'grad_norm': 0.2562178671360016, 'learning_rate': 0.00014356523278456364, 'epoch': 0.28}\n",
      "{'loss': 0.7722, 'grad_norm': 0.30983805656433105, 'learning_rate': 0.0001435298282881926, 'epoch': 0.28}\n",
      "{'loss': 0.7518, 'grad_norm': 0.3172582685947418, 'learning_rate': 0.00014349442379182157, 'epoch': 0.28}\n",
      "{'loss': 0.5209, 'grad_norm': 0.4769728481769562, 'learning_rate': 0.00014345901929545054, 'epoch': 0.28}\n",
      "{'loss': 0.6671, 'grad_norm': 0.21161924302577972, 'learning_rate': 0.00014342361479907948, 'epoch': 0.28}\n",
      "{'loss': 0.7359, 'grad_norm': 0.20165295898914337, 'learning_rate': 0.00014338821030270845, 'epoch': 0.28}\n",
      "{'loss': 0.5011, 'grad_norm': 0.23482319712638855, 'learning_rate': 0.00014335280580633742, 'epoch': 0.28}\n",
      "{'loss': 1.2436, 'grad_norm': 0.17622345685958862, 'learning_rate': 0.0001433174013099664, 'epoch': 0.28}\n",
      "{'loss': 1.0309, 'grad_norm': 0.2788161039352417, 'learning_rate': 0.00014328199681359533, 'epoch': 0.28}\n",
      "{'loss': 0.7027, 'grad_norm': 0.2064535766839981, 'learning_rate': 0.0001432465923172243, 'epoch': 0.28}\n",
      "{'loss': 0.708, 'grad_norm': 0.2015925943851471, 'learning_rate': 0.00014321118782085327, 'epoch': 0.28}\n",
      "{'loss': 0.6942, 'grad_norm': 0.19003580510616302, 'learning_rate': 0.00014317578332448223, 'epoch': 0.28}\n",
      "{'loss': 0.8432, 'grad_norm': 0.24004754424095154, 'learning_rate': 0.00014314037882811118, 'epoch': 0.28}\n",
      "{'loss': 0.7469, 'grad_norm': 0.2222999483346939, 'learning_rate': 0.00014310497433174012, 'epoch': 0.29}\n",
      "{'loss': 0.491, 'grad_norm': 0.20504087209701538, 'learning_rate': 0.0001430695698353691, 'epoch': 0.29}\n",
      "{'loss': 0.7532, 'grad_norm': 0.23801495134830475, 'learning_rate': 0.00014303416533899805, 'epoch': 0.29}\n",
      "{'loss': 0.9568, 'grad_norm': 0.21922467648983002, 'learning_rate': 0.00014299876084262702, 'epoch': 0.29}\n",
      "{'loss': 0.6637, 'grad_norm': 0.23989291489124298, 'learning_rate': 0.00014296335634625596, 'epoch': 0.29}\n",
      "{'loss': 1.0002, 'grad_norm': 0.38419923186302185, 'learning_rate': 0.00014292795184988493, 'epoch': 0.29}\n",
      "{'loss': 0.653, 'grad_norm': 0.23784203827381134, 'learning_rate': 0.0001428925473535139, 'epoch': 0.29}\n",
      "{'loss': 0.8055, 'grad_norm': 0.30425596237182617, 'learning_rate': 0.00014285714285714287, 'epoch': 0.29}\n",
      "{'loss': 0.7369, 'grad_norm': 0.21247293055057526, 'learning_rate': 0.0001428217383607718, 'epoch': 0.29}\n",
      "{'loss': 1.0515, 'grad_norm': 0.26532217860221863, 'learning_rate': 0.00014278633386440078, 'epoch': 0.29}\n",
      "{'loss': 0.7283, 'grad_norm': 0.209381103515625, 'learning_rate': 0.00014275092936802975, 'epoch': 0.29}\n",
      "{'loss': 0.6143, 'grad_norm': 0.18322713673114777, 'learning_rate': 0.00014271552487165871, 'epoch': 0.29}\n",
      "{'loss': 0.5761, 'grad_norm': 0.19996191561222076, 'learning_rate': 0.00014268012037528766, 'epoch': 0.29}\n",
      "{'loss': 0.7454, 'grad_norm': 0.20394085347652435, 'learning_rate': 0.00014264471587891662, 'epoch': 0.29}\n",
      "{'loss': 0.5812, 'grad_norm': 0.2626783549785614, 'learning_rate': 0.0001426093113825456, 'epoch': 0.29}\n",
      "{'loss': 1.0602, 'grad_norm': 0.3121308386325836, 'learning_rate': 0.00014257390688617456, 'epoch': 0.29}\n",
      "{'loss': 1.3, 'grad_norm': 0.27930450439453125, 'learning_rate': 0.0001425385023898035, 'epoch': 0.29}\n",
      "{'loss': 0.6509, 'grad_norm': 0.26971080899238586, 'learning_rate': 0.00014250309789343247, 'epoch': 0.29}\n",
      "{'loss': 0.6747, 'grad_norm': 0.2124224454164505, 'learning_rate': 0.00014246769339706144, 'epoch': 0.29}\n",
      "{'loss': 0.7839, 'grad_norm': 0.27656522393226624, 'learning_rate': 0.0001424322889006904, 'epoch': 0.29}\n",
      "{'loss': 0.485, 'grad_norm': 0.19462823867797852, 'learning_rate': 0.00014239688440431935, 'epoch': 0.29}\n",
      "{'loss': 0.6619, 'grad_norm': 0.1744089424610138, 'learning_rate': 0.00014236147990794832, 'epoch': 0.29}\n",
      "{'loss': 0.6707, 'grad_norm': 0.20888620615005493, 'learning_rate': 0.00014232607541157728, 'epoch': 0.29}\n",
      "{'loss': 0.6839, 'grad_norm': 0.20849327743053436, 'learning_rate': 0.00014229067091520625, 'epoch': 0.29}\n",
      "{'loss': 0.7494, 'grad_norm': 0.2662510573863983, 'learning_rate': 0.0001422552664188352, 'epoch': 0.29}\n",
      "{'loss': 0.6906, 'grad_norm': 0.23268909752368927, 'learning_rate': 0.00014221986192246416, 'epoch': 0.29}\n",
      "{'loss': 0.6434, 'grad_norm': 0.18543927371501923, 'learning_rate': 0.00014218445742609313, 'epoch': 0.29}\n",
      "{'loss': 0.8715, 'grad_norm': 0.24596744775772095, 'learning_rate': 0.00014214905292972207, 'epoch': 0.29}\n",
      "{'loss': 0.4838, 'grad_norm': 0.19547808170318604, 'learning_rate': 0.00014211364843335104, 'epoch': 0.29}\n",
      "{'loss': 0.8831, 'grad_norm': 0.2694692313671112, 'learning_rate': 0.00014207824393697998, 'epoch': 0.29}\n",
      "{'loss': 0.9173, 'grad_norm': 0.24811361730098724, 'learning_rate': 0.00014204283944060898, 'epoch': 0.29}\n",
      "{'loss': 0.6934, 'grad_norm': 0.2759273648262024, 'learning_rate': 0.00014200743494423792, 'epoch': 0.29}\n",
      "{'loss': 0.8, 'grad_norm': 0.18040971457958221, 'learning_rate': 0.00014197203044786689, 'epoch': 0.29}\n",
      "{'loss': 0.6024, 'grad_norm': 0.21873502433300018, 'learning_rate': 0.00014193662595149583, 'epoch': 0.29}\n",
      "{'loss': 0.5753, 'grad_norm': 0.26406875252723694, 'learning_rate': 0.00014190122145512482, 'epoch': 0.29}\n",
      "{'loss': 0.6048, 'grad_norm': 0.1806315779685974, 'learning_rate': 0.00014186581695875376, 'epoch': 0.29}\n",
      "{'loss': 0.561, 'grad_norm': 0.19201216101646423, 'learning_rate': 0.00014183041246238273, 'epoch': 0.29}\n",
      "{'loss': 0.6656, 'grad_norm': 0.22697794437408447, 'learning_rate': 0.00014179500796601167, 'epoch': 0.29}\n",
      "{'loss': 0.7714, 'grad_norm': 0.24663734436035156, 'learning_rate': 0.00014175960346964067, 'epoch': 0.29}\n",
      "{'loss': 0.6424, 'grad_norm': 0.21498063206672668, 'learning_rate': 0.0001417241989732696, 'epoch': 0.29}\n",
      "{'loss': 0.7919, 'grad_norm': 0.2456495463848114, 'learning_rate': 0.00014168879447689858, 'epoch': 0.29}\n",
      "{'loss': 0.7041, 'grad_norm': 0.21193061769008636, 'learning_rate': 0.00014165338998052752, 'epoch': 0.29}\n",
      "{'loss': 0.7721, 'grad_norm': 0.23178711533546448, 'learning_rate': 0.0001416179854841565, 'epoch': 0.29}\n",
      "{'loss': 1.0091, 'grad_norm': 0.3159460723400116, 'learning_rate': 0.00014158258098778546, 'epoch': 0.29}\n",
      "{'loss': 0.5877, 'grad_norm': 0.18954581022262573, 'learning_rate': 0.00014154717649141442, 'epoch': 0.29}\n",
      "{'loss': 0.723, 'grad_norm': 0.25924012064933777, 'learning_rate': 0.00014151177199504337, 'epoch': 0.29}\n",
      "{'loss': 0.73, 'grad_norm': 0.20452453196048737, 'learning_rate': 0.00014147636749867233, 'epoch': 0.29}\n",
      "{'loss': 0.7097, 'grad_norm': 0.17719656229019165, 'learning_rate': 0.0001414409630023013, 'epoch': 0.29}\n",
      "{'loss': 0.6782, 'grad_norm': 0.2211499810218811, 'learning_rate': 0.00014140555850593027, 'epoch': 0.29}\n",
      "{'loss': 0.8664, 'grad_norm': 0.2499416023492813, 'learning_rate': 0.0001413701540095592, 'epoch': 0.29}\n",
      "{'loss': 0.7827, 'grad_norm': 0.24736246466636658, 'learning_rate': 0.00014133474951318818, 'epoch': 0.29}\n",
      "{'loss': 0.6736, 'grad_norm': 0.17662082612514496, 'learning_rate': 0.00014129934501681715, 'epoch': 0.29}\n",
      "{'loss': 0.6656, 'grad_norm': 0.22083231806755066, 'learning_rate': 0.00014126394052044612, 'epoch': 0.29}\n",
      "{'loss': 0.77, 'grad_norm': 0.2595113515853882, 'learning_rate': 0.00014122853602407506, 'epoch': 0.29}\n",
      "{'loss': 0.7148, 'grad_norm': 0.2253689020872116, 'learning_rate': 0.00014119313152770403, 'epoch': 0.29}\n",
      "{'loss': 0.7683, 'grad_norm': 0.2031646966934204, 'learning_rate': 0.000141157727031333, 'epoch': 0.29}\n",
      "{'loss': 0.86, 'grad_norm': 0.24624113738536835, 'learning_rate': 0.00014112232253496194, 'epoch': 0.3}\n",
      "{'loss': 0.5334, 'grad_norm': 0.18751376867294312, 'learning_rate': 0.0001410869180385909, 'epoch': 0.3}\n",
      "{'loss': 0.7701, 'grad_norm': 0.24726074934005737, 'learning_rate': 0.00014105151354221987, 'epoch': 0.3}\n",
      "{'loss': 0.5885, 'grad_norm': 0.2037794291973114, 'learning_rate': 0.00014101610904584884, 'epoch': 0.3}\n",
      "{'loss': 0.8871, 'grad_norm': 0.25792989134788513, 'learning_rate': 0.00014098070454947778, 'epoch': 0.3}\n",
      "{'loss': 0.6398, 'grad_norm': 0.23127864301204681, 'learning_rate': 0.00014094530005310675, 'epoch': 0.3}\n",
      "{'loss': 0.6179, 'grad_norm': 0.270429402589798, 'learning_rate': 0.0001409098955567357, 'epoch': 0.3}\n",
      "{'loss': 0.8489, 'grad_norm': 0.3348079323768616, 'learning_rate': 0.0001408744910603647, 'epoch': 0.3}\n",
      "{'loss': 0.6781, 'grad_norm': 0.27079349756240845, 'learning_rate': 0.00014083908656399363, 'epoch': 0.3}\n",
      "{'loss': 0.7494, 'grad_norm': 0.23582464456558228, 'learning_rate': 0.0001408036820676226, 'epoch': 0.3}\n",
      "{'loss': 0.7299, 'grad_norm': 0.2476568967103958, 'learning_rate': 0.00014076827757125154, 'epoch': 0.3}\n",
      "{'loss': 0.9689, 'grad_norm': 0.1998959481716156, 'learning_rate': 0.00014073287307488053, 'epoch': 0.3}\n",
      "{'loss': 1.2957, 'grad_norm': 0.2594520151615143, 'learning_rate': 0.00014069746857850947, 'epoch': 0.3}\n",
      "{'loss': 0.7541, 'grad_norm': 0.17326423525810242, 'learning_rate': 0.00014066206408213844, 'epoch': 0.3}\n",
      "{'loss': 0.4817, 'grad_norm': 0.21975521743297577, 'learning_rate': 0.00014062665958576738, 'epoch': 0.3}\n",
      "{'loss': 0.6205, 'grad_norm': 0.2024867832660675, 'learning_rate': 0.00014059125508939638, 'epoch': 0.3}\n",
      "{'loss': 0.5475, 'grad_norm': 0.1848854273557663, 'learning_rate': 0.00014055585059302532, 'epoch': 0.3}\n",
      "{'loss': 0.8439, 'grad_norm': 0.23171144723892212, 'learning_rate': 0.0001405204460966543, 'epoch': 0.3}\n",
      "{'loss': 0.7331, 'grad_norm': 0.18173934519290924, 'learning_rate': 0.00014048504160028323, 'epoch': 0.3}\n",
      "{'loss': 0.7932, 'grad_norm': 0.25275924801826477, 'learning_rate': 0.00014044963710391223, 'epoch': 0.3}\n",
      "{'loss': 0.7099, 'grad_norm': 0.1822400540113449, 'learning_rate': 0.00014041423260754117, 'epoch': 0.3}\n",
      "{'loss': 0.7903, 'grad_norm': 0.26352831721305847, 'learning_rate': 0.00014037882811117014, 'epoch': 0.3}\n",
      "{'loss': 0.9829, 'grad_norm': 0.26566970348358154, 'learning_rate': 0.00014034342361479908, 'epoch': 0.3}\n",
      "{'loss': 0.6155, 'grad_norm': 0.2015901505947113, 'learning_rate': 0.00014030801911842804, 'epoch': 0.3}\n",
      "{'loss': 0.7877, 'grad_norm': 0.21442705392837524, 'learning_rate': 0.000140272614622057, 'epoch': 0.3}\n",
      "{'loss': 0.7235, 'grad_norm': 0.23424440622329712, 'learning_rate': 0.00014023721012568595, 'epoch': 0.3}\n",
      "{'loss': 0.7977, 'grad_norm': 0.2081579864025116, 'learning_rate': 0.00014020180562931492, 'epoch': 0.3}\n",
      "{'loss': 0.6247, 'grad_norm': 0.2087867558002472, 'learning_rate': 0.0001401664011329439, 'epoch': 0.3}\n",
      "{'loss': 0.6398, 'grad_norm': 0.20436032116413116, 'learning_rate': 0.00014013099663657286, 'epoch': 0.3}\n",
      "{'loss': 0.8303, 'grad_norm': 0.22684313356876373, 'learning_rate': 0.0001400955921402018, 'epoch': 0.3}\n",
      "{'loss': 0.9957, 'grad_norm': 0.2664574384689331, 'learning_rate': 0.00014006018764383077, 'epoch': 0.3}\n",
      "{'loss': 0.5966, 'grad_norm': 0.20070576667785645, 'learning_rate': 0.00014002478314745974, 'epoch': 0.3}\n",
      "{'loss': 0.6211, 'grad_norm': 0.17967885732650757, 'learning_rate': 0.0001399893786510887, 'epoch': 0.3}\n",
      "{'loss': 0.565, 'grad_norm': 0.15551796555519104, 'learning_rate': 0.00013995397415471765, 'epoch': 0.3}\n",
      "{'loss': 0.7591, 'grad_norm': 0.26693543791770935, 'learning_rate': 0.00013991856965834661, 'epoch': 0.3}\n",
      "{'loss': 0.428, 'grad_norm': 0.17672860622406006, 'learning_rate': 0.00013988316516197558, 'epoch': 0.3}\n",
      "{'loss': 0.6206, 'grad_norm': 0.20722174644470215, 'learning_rate': 0.00013984776066560455, 'epoch': 0.3}\n",
      "{'loss': 0.6855, 'grad_norm': 0.20424631237983704, 'learning_rate': 0.0001398123561692335, 'epoch': 0.3}\n",
      "{'loss': 0.695, 'grad_norm': 0.2284744530916214, 'learning_rate': 0.00013977695167286246, 'epoch': 0.3}\n",
      "{'loss': 0.8802, 'grad_norm': 0.256133496761322, 'learning_rate': 0.00013974154717649143, 'epoch': 0.3}\n",
      "{'loss': 0.7767, 'grad_norm': 0.21912863850593567, 'learning_rate': 0.0001397061426801204, 'epoch': 0.3}\n",
      "{'loss': 0.9348, 'grad_norm': 0.26088017225265503, 'learning_rate': 0.00013967073818374934, 'epoch': 0.3}\n",
      "{'loss': 1.1734, 'grad_norm': 0.2686757743358612, 'learning_rate': 0.0001396353336873783, 'epoch': 0.3}\n",
      "{'loss': 0.7752, 'grad_norm': 0.2821534276008606, 'learning_rate': 0.00013959992919100725, 'epoch': 0.3}\n",
      "{'loss': 0.7354, 'grad_norm': 0.22499312460422516, 'learning_rate': 0.00013956452469463624, 'epoch': 0.3}\n",
      "{'loss': 0.8813, 'grad_norm': 0.20566494762897491, 'learning_rate': 0.00013952912019826519, 'epoch': 0.3}\n",
      "{'loss': 0.8453, 'grad_norm': 0.24307356774806976, 'learning_rate': 0.00013949371570189415, 'epoch': 0.3}\n",
      "{'loss': 0.9395, 'grad_norm': 0.2870986759662628, 'learning_rate': 0.0001394583112055231, 'epoch': 0.3}\n",
      "{'loss': 0.7301, 'grad_norm': 0.2396281659603119, 'learning_rate': 0.0001394229067091521, 'epoch': 0.3}\n",
      "{'loss': 0.4679, 'grad_norm': 0.20591598749160767, 'learning_rate': 0.00013938750221278103, 'epoch': 0.3}\n",
      "{'loss': 0.7615, 'grad_norm': 0.287280410528183, 'learning_rate': 0.00013935209771641, 'epoch': 0.3}\n",
      "{'loss': 1.3805, 'grad_norm': 0.16123399138450623, 'learning_rate': 0.00013931669322003894, 'epoch': 0.3}\n",
      "{'loss': 0.7935, 'grad_norm': 0.20534615218639374, 'learning_rate': 0.0001392812887236679, 'epoch': 0.3}\n",
      "{'loss': 0.5148, 'grad_norm': 0.226642444729805, 'learning_rate': 0.00013924588422729688, 'epoch': 0.3}\n",
      "{'loss': 0.7631, 'grad_norm': 0.1803560107946396, 'learning_rate': 0.00013921047973092582, 'epoch': 0.3}\n",
      "{'loss': 0.6131, 'grad_norm': 0.18950039148330688, 'learning_rate': 0.0001391750752345548, 'epoch': 0.3}\n",
      "{'loss': 0.6276, 'grad_norm': 0.2370612770318985, 'learning_rate': 0.00013913967073818376, 'epoch': 0.3}\n",
      "{'loss': 1.1201, 'grad_norm': 0.30169281363487244, 'learning_rate': 0.00013910426624181272, 'epoch': 0.31}\n",
      "{'loss': 0.6669, 'grad_norm': 0.1894635558128357, 'learning_rate': 0.00013906886174544166, 'epoch': 0.31}\n",
      "{'loss': 0.5334, 'grad_norm': 0.19165921211242676, 'learning_rate': 0.00013903345724907063, 'epoch': 0.31}\n",
      "{'loss': 0.8052, 'grad_norm': 0.38376864790916443, 'learning_rate': 0.0001389980527526996, 'epoch': 0.31}\n",
      "{'loss': 0.8815, 'grad_norm': 0.23252202570438385, 'learning_rate': 0.00013896264825632857, 'epoch': 0.31}\n",
      "{'loss': 0.6271, 'grad_norm': 0.22233471274375916, 'learning_rate': 0.0001389272437599575, 'epoch': 0.31}\n",
      "{'loss': 1.1631, 'grad_norm': 0.29924893379211426, 'learning_rate': 0.00013889183926358648, 'epoch': 0.31}\n",
      "{'loss': 0.791, 'grad_norm': 0.2988237738609314, 'learning_rate': 0.00013885643476721545, 'epoch': 0.31}\n",
      "{'loss': 0.5304, 'grad_norm': 0.20505574345588684, 'learning_rate': 0.00013882103027084442, 'epoch': 0.31}\n",
      "{'loss': 0.5434, 'grad_norm': 0.1902063935995102, 'learning_rate': 0.00013878562577447336, 'epoch': 0.31}\n",
      "{'loss': 0.6056, 'grad_norm': 0.22514554858207703, 'learning_rate': 0.00013875022127810233, 'epoch': 0.31}\n",
      "{'loss': 0.8106, 'grad_norm': 0.2046644538640976, 'learning_rate': 0.0001387148167817313, 'epoch': 0.31}\n",
      "{'loss': 0.7182, 'grad_norm': 0.2315582036972046, 'learning_rate': 0.00013867941228536026, 'epoch': 0.31}\n",
      "{'loss': 0.6856, 'grad_norm': 0.19388580322265625, 'learning_rate': 0.0001386440077889892, 'epoch': 0.31}\n",
      "{'loss': 0.7687, 'grad_norm': 0.2361380010843277, 'learning_rate': 0.00013860860329261817, 'epoch': 0.31}\n",
      "{'loss': 0.5919, 'grad_norm': 0.21632635593414307, 'learning_rate': 0.00013857319879624714, 'epoch': 0.31}\n",
      "{'loss': 0.9632, 'grad_norm': 0.24167628586292267, 'learning_rate': 0.0001385377942998761, 'epoch': 0.31}\n",
      "{'loss': 0.5216, 'grad_norm': 0.21758495271205902, 'learning_rate': 0.00013850238980350505, 'epoch': 0.31}\n",
      "{'loss': 0.8265, 'grad_norm': 0.2864873707294464, 'learning_rate': 0.00013846698530713402, 'epoch': 0.31}\n",
      "{'loss': 0.7063, 'grad_norm': 0.2593241035938263, 'learning_rate': 0.00013843158081076299, 'epoch': 0.31}\n",
      "{'loss': 0.6698, 'grad_norm': 0.2022516429424286, 'learning_rate': 0.00013839617631439195, 'epoch': 0.31}\n",
      "{'loss': 0.7702, 'grad_norm': 0.2506347596645355, 'learning_rate': 0.0001383607718180209, 'epoch': 0.31}\n",
      "{'loss': 0.7177, 'grad_norm': 0.23258517682552338, 'learning_rate': 0.00013832536732164984, 'epoch': 0.31}\n",
      "{'loss': 0.6114, 'grad_norm': 0.21339547634124756, 'learning_rate': 0.0001382899628252788, 'epoch': 0.31}\n",
      "{'loss': 0.6824, 'grad_norm': 0.20082320272922516, 'learning_rate': 0.00013825455832890777, 'epoch': 0.31}\n",
      "{'loss': 0.4985, 'grad_norm': 0.19468584656715393, 'learning_rate': 0.00013821915383253674, 'epoch': 0.31}\n",
      "{'loss': 0.6938, 'grad_norm': 0.239329993724823, 'learning_rate': 0.00013818374933616568, 'epoch': 0.31}\n",
      "{'loss': 0.5504, 'grad_norm': 0.19805392622947693, 'learning_rate': 0.00013814834483979465, 'epoch': 0.31}\n",
      "{'loss': 0.8124, 'grad_norm': 0.2820546627044678, 'learning_rate': 0.00013811294034342362, 'epoch': 0.31}\n",
      "{'loss': 0.6565, 'grad_norm': 0.23164920508861542, 'learning_rate': 0.0001380775358470526, 'epoch': 0.31}\n",
      "{'loss': 0.9636, 'grad_norm': 0.29607677459716797, 'learning_rate': 0.00013804213135068153, 'epoch': 0.31}\n",
      "{'loss': 0.7364, 'grad_norm': 0.21450473368167877, 'learning_rate': 0.0001380067268543105, 'epoch': 0.31}\n",
      "{'loss': 0.8236, 'grad_norm': 0.2565290927886963, 'learning_rate': 0.00013797132235793947, 'epoch': 0.31}\n",
      "{'loss': 0.7931, 'grad_norm': 0.21614326536655426, 'learning_rate': 0.00013793591786156843, 'epoch': 0.31}\n",
      "{'loss': 0.5093, 'grad_norm': 0.21574895083904266, 'learning_rate': 0.00013790051336519738, 'epoch': 0.31}\n",
      "{'loss': 0.8033, 'grad_norm': 0.24913202226161957, 'learning_rate': 0.00013786510886882634, 'epoch': 0.31}\n",
      "{'loss': 0.6965, 'grad_norm': 0.1983279585838318, 'learning_rate': 0.0001378297043724553, 'epoch': 0.31}\n",
      "{'loss': 0.8247, 'grad_norm': 0.23562422394752502, 'learning_rate': 0.00013779429987608428, 'epoch': 0.31}\n",
      "{'loss': 0.6421, 'grad_norm': 0.24408599734306335, 'learning_rate': 0.00013775889537971322, 'epoch': 0.31}\n",
      "{'loss': 0.8156, 'grad_norm': 0.3566584885120392, 'learning_rate': 0.0001377234908833422, 'epoch': 0.31}\n",
      "{'loss': 0.5547, 'grad_norm': 0.20219098031520844, 'learning_rate': 0.00013768808638697116, 'epoch': 0.31}\n",
      "{'loss': 0.5902, 'grad_norm': 0.22442609071731567, 'learning_rate': 0.00013765268189060013, 'epoch': 0.31}\n",
      "{'loss': 1.0089, 'grad_norm': 0.2600824236869812, 'learning_rate': 0.00013761727739422907, 'epoch': 0.31}\n",
      "{'loss': 0.7065, 'grad_norm': 0.22522090375423431, 'learning_rate': 0.00013758187289785804, 'epoch': 0.31}\n",
      "{'loss': 0.8034, 'grad_norm': 0.20682726800441742, 'learning_rate': 0.000137546468401487, 'epoch': 0.31}\n",
      "{'loss': 0.6587, 'grad_norm': 0.17772312462329865, 'learning_rate': 0.00013751106390511597, 'epoch': 0.31}\n",
      "{'loss': 0.6755, 'grad_norm': 0.1981426477432251, 'learning_rate': 0.00013747565940874491, 'epoch': 0.31}\n",
      "{'loss': 0.6319, 'grad_norm': 0.19983628392219543, 'learning_rate': 0.00013744025491237386, 'epoch': 0.31}\n",
      "{'loss': 0.6947, 'grad_norm': 0.19099052250385284, 'learning_rate': 0.00013740485041600285, 'epoch': 0.31}\n",
      "{'loss': 0.6388, 'grad_norm': 0.1996285766363144, 'learning_rate': 0.0001373694459196318, 'epoch': 0.31}\n",
      "{'loss': 0.9459, 'grad_norm': 0.2866572141647339, 'learning_rate': 0.00013733404142326076, 'epoch': 0.31}\n",
      "{'loss': 0.8648, 'grad_norm': 0.28521960973739624, 'learning_rate': 0.0001372986369268897, 'epoch': 0.31}\n",
      "{'loss': 0.7435, 'grad_norm': 0.24881859123706818, 'learning_rate': 0.0001372632324305187, 'epoch': 0.31}\n",
      "{'loss': 0.6757, 'grad_norm': 0.2215680629014969, 'learning_rate': 0.00013722782793414764, 'epoch': 0.31}\n",
      "{'loss': 0.5494, 'grad_norm': 0.2374148815870285, 'learning_rate': 0.0001371924234377766, 'epoch': 0.31}\n",
      "{'loss': 0.7402, 'grad_norm': 0.2095666080713272, 'learning_rate': 0.00013715701894140555, 'epoch': 0.31}\n",
      "{'loss': 0.9849, 'grad_norm': 0.22616970539093018, 'learning_rate': 0.00013712161444503454, 'epoch': 0.31}\n",
      "{'loss': 0.5726, 'grad_norm': 0.2532421052455902, 'learning_rate': 0.00013708620994866348, 'epoch': 0.32}\n",
      "{'loss': 0.5762, 'grad_norm': 0.22177429497241974, 'learning_rate': 0.00013705080545229245, 'epoch': 0.32}\n",
      "{'loss': 0.924, 'grad_norm': 0.2731972634792328, 'learning_rate': 0.0001370154009559214, 'epoch': 0.32}\n",
      "{'loss': 0.7015, 'grad_norm': 0.21965250372886658, 'learning_rate': 0.00013697999645955036, 'epoch': 0.32}\n",
      "{'loss': 0.7401, 'grad_norm': 0.2306145578622818, 'learning_rate': 0.00013694459196317933, 'epoch': 0.32}\n",
      "{'loss': 0.6245, 'grad_norm': 0.23432819545269012, 'learning_rate': 0.0001369091874668083, 'epoch': 0.32}\n",
      "{'loss': 0.4841, 'grad_norm': 0.18894751369953156, 'learning_rate': 0.00013687378297043724, 'epoch': 0.32}\n",
      "{'loss': 0.819, 'grad_norm': 0.2603655755519867, 'learning_rate': 0.0001368383784740662, 'epoch': 0.32}\n",
      "{'loss': 0.5221, 'grad_norm': 0.2172754555940628, 'learning_rate': 0.00013680297397769518, 'epoch': 0.32}\n",
      "{'loss': 0.8033, 'grad_norm': 0.23047249019145966, 'learning_rate': 0.00013676756948132414, 'epoch': 0.32}\n",
      "{'loss': 0.699, 'grad_norm': 0.20360098779201508, 'learning_rate': 0.00013673216498495309, 'epoch': 0.32}\n",
      "{'loss': 0.841, 'grad_norm': 0.19810661673545837, 'learning_rate': 0.00013669676048858205, 'epoch': 0.32}\n",
      "{'loss': 0.7654, 'grad_norm': 0.25963032245635986, 'learning_rate': 0.00013666135599221102, 'epoch': 0.32}\n",
      "{'loss': 0.7963, 'grad_norm': 0.23734921216964722, 'learning_rate': 0.00013662595149584, 'epoch': 0.32}\n",
      "{'loss': 0.8087, 'grad_norm': 0.256635844707489, 'learning_rate': 0.00013659054699946893, 'epoch': 0.32}\n",
      "{'loss': 0.818, 'grad_norm': 0.26007241010665894, 'learning_rate': 0.0001365551425030979, 'epoch': 0.32}\n",
      "{'loss': 0.6461, 'grad_norm': 0.18057921528816223, 'learning_rate': 0.00013651973800672687, 'epoch': 0.32}\n",
      "{'loss': 1.2516, 'grad_norm': 0.2360783815383911, 'learning_rate': 0.0001364843335103558, 'epoch': 0.32}\n",
      "{'loss': 0.5276, 'grad_norm': 0.23507030308246613, 'learning_rate': 0.00013644892901398478, 'epoch': 0.32}\n",
      "{'loss': 0.7158, 'grad_norm': 0.26496952772140503, 'learning_rate': 0.00013641352451761375, 'epoch': 0.32}\n",
      "{'loss': 0.766, 'grad_norm': 0.20387190580368042, 'learning_rate': 0.00013637812002124271, 'epoch': 0.32}\n",
      "{'loss': 0.7728, 'grad_norm': 0.22416061162948608, 'learning_rate': 0.00013634271552487166, 'epoch': 0.32}\n",
      "{'loss': 0.6253, 'grad_norm': 0.26162517070770264, 'learning_rate': 0.00013630731102850062, 'epoch': 0.32}\n",
      "{'loss': 0.603, 'grad_norm': 0.21330693364143372, 'learning_rate': 0.00013627190653212957, 'epoch': 0.32}\n",
      "{'loss': 0.5734, 'grad_norm': 0.18694272637367249, 'learning_rate': 0.00013623650203575856, 'epoch': 0.32}\n",
      "{'loss': 0.7023, 'grad_norm': 0.21931332349777222, 'learning_rate': 0.0001362010975393875, 'epoch': 0.32}\n",
      "{'loss': 0.5537, 'grad_norm': 0.18470938503742218, 'learning_rate': 0.00013616569304301647, 'epoch': 0.32}\n",
      "{'loss': 1.1294, 'grad_norm': 0.2908738851547241, 'learning_rate': 0.0001361302885466454, 'epoch': 0.32}\n",
      "{'loss': 0.9167, 'grad_norm': 0.24726958572864532, 'learning_rate': 0.0001360948840502744, 'epoch': 0.32}\n",
      "{'loss': 0.6382, 'grad_norm': 0.17880648374557495, 'learning_rate': 0.00013605947955390335, 'epoch': 0.32}\n",
      "{'loss': 0.4706, 'grad_norm': 0.1575303077697754, 'learning_rate': 0.00013602407505753232, 'epoch': 0.32}\n",
      "{'loss': 0.6024, 'grad_norm': 0.1936977058649063, 'learning_rate': 0.00013598867056116126, 'epoch': 0.32}\n",
      "{'loss': 0.6881, 'grad_norm': 0.20801420509815216, 'learning_rate': 0.00013595326606479025, 'epoch': 0.32}\n",
      "{'loss': 0.7506, 'grad_norm': 0.22241953015327454, 'learning_rate': 0.0001359178615684192, 'epoch': 0.32}\n",
      "{'loss': 0.6598, 'grad_norm': 0.22063198685646057, 'learning_rate': 0.00013588245707204816, 'epoch': 0.32}\n",
      "{'loss': 0.5256, 'grad_norm': 0.14724718034267426, 'learning_rate': 0.0001358470525756771, 'epoch': 0.32}\n",
      "{'loss': 1.029, 'grad_norm': 0.2697444260120392, 'learning_rate': 0.0001358116480793061, 'epoch': 0.32}\n",
      "{'loss': 0.7989, 'grad_norm': 0.2172950953245163, 'learning_rate': 0.00013577624358293504, 'epoch': 0.32}\n",
      "{'loss': 0.755, 'grad_norm': 0.2037327140569687, 'learning_rate': 0.000135740839086564, 'epoch': 0.32}\n",
      "{'loss': 0.5525, 'grad_norm': 0.18305152654647827, 'learning_rate': 0.00013570543459019295, 'epoch': 0.32}\n",
      "{'loss': 0.8873, 'grad_norm': 0.26038897037506104, 'learning_rate': 0.00013567003009382192, 'epoch': 0.32}\n",
      "{'loss': 0.7833, 'grad_norm': 0.2513357400894165, 'learning_rate': 0.0001356346255974509, 'epoch': 0.32}\n",
      "{'loss': 0.8803, 'grad_norm': 0.22976598143577576, 'learning_rate': 0.00013559922110107986, 'epoch': 0.32}\n",
      "{'loss': 0.534, 'grad_norm': 0.20717063546180725, 'learning_rate': 0.0001355638166047088, 'epoch': 0.32}\n",
      "{'loss': 0.6566, 'grad_norm': 0.21935422718524933, 'learning_rate': 0.00013552841210833776, 'epoch': 0.32}\n",
      "{'loss': 0.6733, 'grad_norm': 0.24634961783885956, 'learning_rate': 0.00013549300761196673, 'epoch': 0.32}\n",
      "{'loss': 0.8136, 'grad_norm': 0.21017012000083923, 'learning_rate': 0.00013545760311559567, 'epoch': 0.32}\n",
      "{'loss': 0.6023, 'grad_norm': 0.20350942015647888, 'learning_rate': 0.00013542219861922464, 'epoch': 0.32}\n",
      "{'loss': 0.6319, 'grad_norm': 0.18330268561840057, 'learning_rate': 0.0001353867941228536, 'epoch': 0.32}\n",
      "{'loss': 0.724, 'grad_norm': 0.17264586687088013, 'learning_rate': 0.00013535138962648258, 'epoch': 0.32}\n",
      "{'loss': 0.5924, 'grad_norm': 0.25294482707977295, 'learning_rate': 0.00013531598513011152, 'epoch': 0.32}\n",
      "{'loss': 0.8381, 'grad_norm': 0.2779543399810791, 'learning_rate': 0.0001352805806337405, 'epoch': 0.32}\n",
      "{'loss': 0.8288, 'grad_norm': 0.2602049708366394, 'learning_rate': 0.00013524517613736946, 'epoch': 0.32}\n",
      "{'loss': 0.567, 'grad_norm': 0.19732841849327087, 'learning_rate': 0.00013520977164099843, 'epoch': 0.32}\n",
      "{'loss': 0.65, 'grad_norm': 0.23109093308448792, 'learning_rate': 0.00013517436714462737, 'epoch': 0.32}\n",
      "{'loss': 0.7241, 'grad_norm': 0.24962985515594482, 'learning_rate': 0.00013513896264825633, 'epoch': 0.32}\n",
      "{'loss': 0.6428, 'grad_norm': 0.25749561190605164, 'learning_rate': 0.0001351035581518853, 'epoch': 0.33}\n",
      "{'loss': 0.9416, 'grad_norm': 0.2849558889865875, 'learning_rate': 0.00013506815365551427, 'epoch': 0.33}\n",
      "{'loss': 0.5998, 'grad_norm': 0.22325797379016876, 'learning_rate': 0.0001350327491591432, 'epoch': 0.33}\n",
      "{'loss': 0.6508, 'grad_norm': 0.24535760283470154, 'learning_rate': 0.00013499734466277218, 'epoch': 0.33}\n",
      "{'loss': 0.6501, 'grad_norm': 0.2135349065065384, 'learning_rate': 0.00013496194016640112, 'epoch': 0.33}\n",
      "{'loss': 0.8584, 'grad_norm': 0.22098670899868011, 'learning_rate': 0.00013492653567003012, 'epoch': 0.33}\n",
      "{'loss': 0.9587, 'grad_norm': 0.3215964436531067, 'learning_rate': 0.00013489113117365906, 'epoch': 0.33}\n",
      "{'loss': 0.7554, 'grad_norm': 0.19301900267601013, 'learning_rate': 0.00013485572667728803, 'epoch': 0.33}\n",
      "{'loss': 0.6381, 'grad_norm': 0.21236003935337067, 'learning_rate': 0.00013482032218091697, 'epoch': 0.33}\n",
      "{'loss': 0.7874, 'grad_norm': 0.23847219347953796, 'learning_rate': 0.00013478491768454596, 'epoch': 0.33}\n",
      "{'loss': 0.4922, 'grad_norm': 0.19863472878932953, 'learning_rate': 0.0001347495131881749, 'epoch': 0.33}\n",
      "{'loss': 0.6409, 'grad_norm': 0.2875441014766693, 'learning_rate': 0.00013471410869180387, 'epoch': 0.33}\n",
      "{'loss': 0.6224, 'grad_norm': 0.17371337115764618, 'learning_rate': 0.00013467870419543281, 'epoch': 0.33}\n",
      "{'loss': 0.7966, 'grad_norm': 0.21387895941734314, 'learning_rate': 0.0001346432996990618, 'epoch': 0.33}\n",
      "{'loss': 0.8716, 'grad_norm': 0.21136949956417084, 'learning_rate': 0.00013460789520269075, 'epoch': 0.33}\n",
      "{'loss': 0.5419, 'grad_norm': 0.1472330093383789, 'learning_rate': 0.0001345724907063197, 'epoch': 0.33}\n",
      "{'loss': 0.5735, 'grad_norm': 0.22846046090126038, 'learning_rate': 0.00013453708620994866, 'epoch': 0.33}\n",
      "{'loss': 0.6213, 'grad_norm': 0.18922770023345947, 'learning_rate': 0.00013450168171357763, 'epoch': 0.33}\n",
      "{'loss': 0.7817, 'grad_norm': 0.22264984250068665, 'learning_rate': 0.0001344662772172066, 'epoch': 0.33}\n",
      "{'loss': 1.0226, 'grad_norm': 0.2692510783672333, 'learning_rate': 0.00013443087272083554, 'epoch': 0.33}\n",
      "{'loss': 0.7363, 'grad_norm': 0.2099922001361847, 'learning_rate': 0.0001343954682244645, 'epoch': 0.33}\n",
      "{'loss': 0.7112, 'grad_norm': 0.17760711908340454, 'learning_rate': 0.00013436006372809348, 'epoch': 0.33}\n",
      "{'loss': 0.6634, 'grad_norm': 0.28125739097595215, 'learning_rate': 0.00013432465923172244, 'epoch': 0.33}\n",
      "{'loss': 0.5968, 'grad_norm': 0.24253088235855103, 'learning_rate': 0.00013428925473535138, 'epoch': 0.33}\n",
      "{'loss': 0.6714, 'grad_norm': 0.2235880196094513, 'learning_rate': 0.00013425385023898035, 'epoch': 0.33}\n",
      "{'loss': 0.7723, 'grad_norm': 0.3095243275165558, 'learning_rate': 0.00013421844574260932, 'epoch': 0.33}\n",
      "{'loss': 0.7947, 'grad_norm': 0.2684463560581207, 'learning_rate': 0.0001341830412462383, 'epoch': 0.33}\n",
      "{'loss': 0.9186, 'grad_norm': 0.2492021769285202, 'learning_rate': 0.00013414763674986723, 'epoch': 0.33}\n",
      "{'loss': 0.8367, 'grad_norm': 0.291420042514801, 'learning_rate': 0.0001341122322534962, 'epoch': 0.33}\n",
      "{'loss': 0.6148, 'grad_norm': 0.4298844635486603, 'learning_rate': 0.00013407682775712517, 'epoch': 0.33}\n",
      "{'loss': 0.8655, 'grad_norm': 0.21425099670886993, 'learning_rate': 0.00013404142326075414, 'epoch': 0.33}\n",
      "{'loss': 0.7259, 'grad_norm': 0.1742306351661682, 'learning_rate': 0.00013400601876438308, 'epoch': 0.33}\n",
      "{'loss': 0.6063, 'grad_norm': 0.17761537432670593, 'learning_rate': 0.00013397061426801205, 'epoch': 0.33}\n",
      "{'loss': 0.7987, 'grad_norm': 0.24608933925628662, 'learning_rate': 0.00013393520977164101, 'epoch': 0.33}\n",
      "{'loss': 0.8916, 'grad_norm': 0.2642766535282135, 'learning_rate': 0.00013389980527526998, 'epoch': 0.33}\n",
      "{'loss': 0.6883, 'grad_norm': 0.2262456864118576, 'learning_rate': 0.00013386440077889892, 'epoch': 0.33}\n",
      "{'loss': 0.7454, 'grad_norm': 0.25064846873283386, 'learning_rate': 0.0001338289962825279, 'epoch': 0.33}\n",
      "{'loss': 0.8145, 'grad_norm': 0.19915306568145752, 'learning_rate': 0.00013379359178615686, 'epoch': 0.33}\n",
      "{'loss': 0.8221, 'grad_norm': 0.21832330524921417, 'learning_rate': 0.00013375818728978583, 'epoch': 0.33}\n",
      "{'loss': 1.0484, 'grad_norm': 0.2160869538784027, 'learning_rate': 0.00013372278279341477, 'epoch': 0.33}\n",
      "{'loss': 0.7061, 'grad_norm': 0.2140614092350006, 'learning_rate': 0.00013368737829704374, 'epoch': 0.33}\n",
      "{'loss': 0.8136, 'grad_norm': 0.22573743760585785, 'learning_rate': 0.00013365197380067268, 'epoch': 0.33}\n",
      "{'loss': 0.6257, 'grad_norm': 0.205434188246727, 'learning_rate': 0.00013361656930430165, 'epoch': 0.33}\n",
      "{'loss': 0.8194, 'grad_norm': 0.24095195531845093, 'learning_rate': 0.00013358116480793062, 'epoch': 0.33}\n",
      "{'loss': 0.5051, 'grad_norm': 0.22507865726947784, 'learning_rate': 0.00013354576031155956, 'epoch': 0.33}\n",
      "{'loss': 0.8023, 'grad_norm': 0.212471604347229, 'learning_rate': 0.00013351035581518853, 'epoch': 0.33}\n",
      "{'loss': 0.6235, 'grad_norm': 0.23164544999599457, 'learning_rate': 0.0001334749513188175, 'epoch': 0.33}\n",
      "{'loss': 0.7265, 'grad_norm': 0.2516891658306122, 'learning_rate': 0.00013343954682244646, 'epoch': 0.33}\n",
      "{'loss': 0.6986, 'grad_norm': 0.2314504086971283, 'learning_rate': 0.0001334041423260754, 'epoch': 0.33}\n",
      "{'loss': 0.795, 'grad_norm': 0.29023051261901855, 'learning_rate': 0.00013336873782970437, 'epoch': 0.33}\n",
      "{'loss': 0.8881, 'grad_norm': 0.2580544352531433, 'learning_rate': 0.00013333333333333334, 'epoch': 0.33}\n",
      "{'loss': 1.031, 'grad_norm': 0.3228417634963989, 'learning_rate': 0.0001332979288369623, 'epoch': 0.33}\n",
      "{'loss': 0.8296, 'grad_norm': 0.2495160698890686, 'learning_rate': 0.00013326252434059125, 'epoch': 0.33}\n",
      "{'loss': 0.6599, 'grad_norm': 0.17405880987644196, 'learning_rate': 0.00013322711984422022, 'epoch': 0.33}\n",
      "{'loss': 0.9902, 'grad_norm': 0.1771167814731598, 'learning_rate': 0.00013319171534784919, 'epoch': 0.33}\n",
      "{'loss': 0.7016, 'grad_norm': 0.19703570008277893, 'learning_rate': 0.00013315631085147815, 'epoch': 0.33}\n",
      "{'loss': 0.7026, 'grad_norm': 0.20945800840854645, 'learning_rate': 0.0001331209063551071, 'epoch': 0.33}\n",
      "{'loss': 0.7614, 'grad_norm': 0.25380057096481323, 'learning_rate': 0.00013308550185873606, 'epoch': 0.34}\n",
      "{'loss': 0.6667, 'grad_norm': 0.27340662479400635, 'learning_rate': 0.00013305009736236503, 'epoch': 0.34}\n",
      "{'loss': 0.6006, 'grad_norm': 0.2503828704357147, 'learning_rate': 0.000133014692865994, 'epoch': 0.34}\n",
      "{'loss': 0.7289, 'grad_norm': 0.1553647667169571, 'learning_rate': 0.00013297928836962294, 'epoch': 0.34}\n",
      "{'loss': 0.7688, 'grad_norm': 0.20006346702575684, 'learning_rate': 0.0001329438838732519, 'epoch': 0.34}\n",
      "{'loss': 0.7339, 'grad_norm': 0.24123865365982056, 'learning_rate': 0.00013290847937688088, 'epoch': 0.34}\n",
      "{'loss': 0.9802, 'grad_norm': 0.2838050425052643, 'learning_rate': 0.00013287307488050985, 'epoch': 0.34}\n",
      "{'loss': 0.6343, 'grad_norm': 0.20447498559951782, 'learning_rate': 0.0001328376703841388, 'epoch': 0.34}\n",
      "{'loss': 0.9419, 'grad_norm': 0.30026745796203613, 'learning_rate': 0.00013280226588776776, 'epoch': 0.34}\n",
      "{'loss': 0.6778, 'grad_norm': 0.2520335018634796, 'learning_rate': 0.00013276686139139672, 'epoch': 0.34}\n",
      "{'loss': 0.8428, 'grad_norm': 0.20439694821834564, 'learning_rate': 0.0001327314568950257, 'epoch': 0.34}\n",
      "{'loss': 0.5619, 'grad_norm': 0.2258739173412323, 'learning_rate': 0.00013269605239865463, 'epoch': 0.34}\n",
      "{'loss': 0.7316, 'grad_norm': 0.2181122750043869, 'learning_rate': 0.00013266064790228358, 'epoch': 0.34}\n",
      "{'loss': 1.0657, 'grad_norm': 0.29674211144447327, 'learning_rate': 0.00013262524340591257, 'epoch': 0.34}\n",
      "{'loss': 0.6978, 'grad_norm': 0.26298728585243225, 'learning_rate': 0.0001325898389095415, 'epoch': 0.34}\n",
      "{'loss': 0.9607, 'grad_norm': 0.25420430302619934, 'learning_rate': 0.00013255443441317048, 'epoch': 0.34}\n",
      "{'loss': 0.7116, 'grad_norm': 0.16010607779026031, 'learning_rate': 0.00013251902991679942, 'epoch': 0.34}\n",
      "{'loss': 0.7199, 'grad_norm': 0.2640618085861206, 'learning_rate': 0.00013248362542042842, 'epoch': 0.34}\n",
      "{'loss': 0.5483, 'grad_norm': 0.17157965898513794, 'learning_rate': 0.00013244822092405736, 'epoch': 0.34}\n",
      "{'loss': 0.6933, 'grad_norm': 0.23280540108680725, 'learning_rate': 0.00013241281642768633, 'epoch': 0.34}\n",
      "{'loss': 0.6956, 'grad_norm': 0.21400640904903412, 'learning_rate': 0.00013237741193131527, 'epoch': 0.34}\n",
      "{'loss': 0.7583, 'grad_norm': 0.29901817440986633, 'learning_rate': 0.00013234200743494424, 'epoch': 0.34}\n",
      "{'loss': 0.7909, 'grad_norm': 0.250081866979599, 'learning_rate': 0.0001323066029385732, 'epoch': 0.34}\n",
      "{'loss': 0.8763, 'grad_norm': 0.2856171727180481, 'learning_rate': 0.00013227119844220217, 'epoch': 0.34}\n",
      "{'loss': 0.7973, 'grad_norm': 0.30662763118743896, 'learning_rate': 0.00013223579394583111, 'epoch': 0.34}\n",
      "{'loss': 1.0115, 'grad_norm': 0.24324864149093628, 'learning_rate': 0.00013220038944946008, 'epoch': 0.34}\n",
      "{'loss': 0.6776, 'grad_norm': 0.19471991062164307, 'learning_rate': 0.00013216498495308905, 'epoch': 0.34}\n",
      "{'loss': 1.2879, 'grad_norm': 0.2816477119922638, 'learning_rate': 0.00013212958045671802, 'epoch': 0.34}\n",
      "{'loss': 0.4913, 'grad_norm': 0.18212047219276428, 'learning_rate': 0.00013209417596034696, 'epoch': 0.34}\n",
      "{'loss': 0.7211, 'grad_norm': 0.2067427933216095, 'learning_rate': 0.00013205877146397593, 'epoch': 0.34}\n",
      "{'loss': 0.7024, 'grad_norm': 0.21715644001960754, 'learning_rate': 0.0001320233669676049, 'epoch': 0.34}\n",
      "{'loss': 0.8021, 'grad_norm': 0.2241811752319336, 'learning_rate': 0.00013198796247123386, 'epoch': 0.34}\n",
      "{'loss': 0.5893, 'grad_norm': 0.20275652408599854, 'learning_rate': 0.0001319525579748628, 'epoch': 0.34}\n",
      "{'loss': 0.8541, 'grad_norm': 0.2497377097606659, 'learning_rate': 0.00013191715347849177, 'epoch': 0.34}\n",
      "{'loss': 1.2053, 'grad_norm': 0.2373291254043579, 'learning_rate': 0.00013188174898212074, 'epoch': 0.34}\n",
      "{'loss': 1.2922, 'grad_norm': 0.26537078619003296, 'learning_rate': 0.0001318463444857497, 'epoch': 0.34}\n",
      "{'loss': 0.9504, 'grad_norm': 0.18668760359287262, 'learning_rate': 0.00013181093998937865, 'epoch': 0.34}\n",
      "{'loss': 0.7161, 'grad_norm': 0.19569435715675354, 'learning_rate': 0.00013177553549300762, 'epoch': 0.34}\n",
      "{'loss': 0.914, 'grad_norm': 0.25079506635665894, 'learning_rate': 0.0001317401309966366, 'epoch': 0.34}\n",
      "{'loss': 0.8867, 'grad_norm': 0.2014872431755066, 'learning_rate': 0.00013170472650026553, 'epoch': 0.34}\n",
      "{'loss': 0.6145, 'grad_norm': 0.2817157208919525, 'learning_rate': 0.0001316693220038945, 'epoch': 0.34}\n",
      "{'loss': 0.7579, 'grad_norm': 0.2610868811607361, 'learning_rate': 0.00013163391750752344, 'epoch': 0.34}\n",
      "{'loss': 0.7581, 'grad_norm': 0.2122737318277359, 'learning_rate': 0.00013159851301115243, 'epoch': 0.34}\n",
      "{'loss': 0.8957, 'grad_norm': 0.20107810199260712, 'learning_rate': 0.00013156310851478138, 'epoch': 0.34}\n",
      "{'loss': 0.7864, 'grad_norm': 0.2431797981262207, 'learning_rate': 0.00013152770401841034, 'epoch': 0.34}\n",
      "{'loss': 0.8258, 'grad_norm': 0.20445719361305237, 'learning_rate': 0.00013149229952203929, 'epoch': 0.34}\n",
      "{'loss': 0.8232, 'grad_norm': 0.18999092280864716, 'learning_rate': 0.00013145689502566828, 'epoch': 0.34}\n",
      "{'loss': 0.6662, 'grad_norm': 0.17181438207626343, 'learning_rate': 0.00013142149052929722, 'epoch': 0.34}\n",
      "{'loss': 0.8781, 'grad_norm': 0.25662052631378174, 'learning_rate': 0.0001313860860329262, 'epoch': 0.34}\n",
      "{'loss': 0.7276, 'grad_norm': 0.16380223631858826, 'learning_rate': 0.00013135068153655513, 'epoch': 0.34}\n",
      "{'loss': 0.9338, 'grad_norm': 0.25396135449409485, 'learning_rate': 0.00013131527704018413, 'epoch': 0.34}\n",
      "{'loss': 0.6116, 'grad_norm': 0.16782261431217194, 'learning_rate': 0.00013127987254381307, 'epoch': 0.34}\n",
      "{'loss': 0.6158, 'grad_norm': 0.20376050472259521, 'learning_rate': 0.00013124446804744204, 'epoch': 0.34}\n",
      "{'loss': 0.5725, 'grad_norm': 0.1765226274728775, 'learning_rate': 0.00013120906355107098, 'epoch': 0.34}\n",
      "{'loss': 0.6314, 'grad_norm': 0.18724502623081207, 'learning_rate': 0.00013117365905469997, 'epoch': 0.34}\n",
      "{'loss': 0.8888, 'grad_norm': 0.24902337789535522, 'learning_rate': 0.00013113825455832891, 'epoch': 0.34}\n",
      "{'loss': 1.0574, 'grad_norm': 0.26536211371421814, 'learning_rate': 0.00013110285006195788, 'epoch': 0.35}\n",
      "{'loss': 0.8802, 'grad_norm': 0.23552186787128448, 'learning_rate': 0.00013106744556558682, 'epoch': 0.35}\n",
      "{'loss': 0.7752, 'grad_norm': 0.184294655919075, 'learning_rate': 0.0001310320410692158, 'epoch': 0.35}\n",
      "{'loss': 0.5838, 'grad_norm': 0.18456244468688965, 'learning_rate': 0.00013099663657284476, 'epoch': 0.35}\n",
      "{'loss': 0.7721, 'grad_norm': 0.22242100536823273, 'learning_rate': 0.00013096123207647373, 'epoch': 0.35}\n",
      "{'loss': 0.499, 'grad_norm': 0.2234315723180771, 'learning_rate': 0.00013092582758010267, 'epoch': 0.35}\n",
      "{'loss': 0.7359, 'grad_norm': 0.1648312509059906, 'learning_rate': 0.00013089042308373164, 'epoch': 0.35}\n",
      "{'loss': 0.6178, 'grad_norm': 0.2615204453468323, 'learning_rate': 0.0001308550185873606, 'epoch': 0.35}\n",
      "{'loss': 0.8332, 'grad_norm': 0.2628994882106781, 'learning_rate': 0.00013081961409098958, 'epoch': 0.35}\n",
      "{'loss': 0.5245, 'grad_norm': 0.20353522896766663, 'learning_rate': 0.00013078420959461852, 'epoch': 0.35}\n",
      "{'loss': 0.8599, 'grad_norm': 0.230632945895195, 'learning_rate': 0.00013074880509824748, 'epoch': 0.35}\n",
      "{'loss': 1.0027, 'grad_norm': 0.2660003900527954, 'learning_rate': 0.00013071340060187645, 'epoch': 0.35}\n",
      "{'loss': 0.7724, 'grad_norm': 0.2313142716884613, 'learning_rate': 0.0001306779961055054, 'epoch': 0.35}\n",
      "{'loss': 0.6653, 'grad_norm': 0.22828690707683563, 'learning_rate': 0.00013064259160913436, 'epoch': 0.35}\n",
      "{'loss': 0.5121, 'grad_norm': 0.19470110535621643, 'learning_rate': 0.00013060718711276333, 'epoch': 0.35}\n",
      "{'loss': 0.7005, 'grad_norm': 0.2359534502029419, 'learning_rate': 0.0001305717826163923, 'epoch': 0.35}\n",
      "{'loss': 0.8643, 'grad_norm': 0.23278659582138062, 'learning_rate': 0.00013053637812002124, 'epoch': 0.35}\n",
      "{'loss': 0.669, 'grad_norm': 0.24531741440296173, 'learning_rate': 0.0001305009736236502, 'epoch': 0.35}\n",
      "{'loss': 0.6637, 'grad_norm': 0.20312562584877014, 'learning_rate': 0.00013046556912727915, 'epoch': 0.35}\n",
      "{'loss': 1.1263, 'grad_norm': 0.2951923608779907, 'learning_rate': 0.00013043016463090815, 'epoch': 0.35}\n",
      "{'loss': 0.4458, 'grad_norm': 0.17333854734897614, 'learning_rate': 0.0001303947601345371, 'epoch': 0.35}\n",
      "{'loss': 0.6402, 'grad_norm': 0.20861709117889404, 'learning_rate': 0.00013035935563816606, 'epoch': 0.35}\n",
      "{'loss': 0.7111, 'grad_norm': 0.2560066878795624, 'learning_rate': 0.000130323951141795, 'epoch': 0.35}\n",
      "{'loss': 0.6393, 'grad_norm': 0.23939307034015656, 'learning_rate': 0.000130288546645424, 'epoch': 0.35}\n",
      "{'loss': 1.0601, 'grad_norm': 0.2696075737476349, 'learning_rate': 0.00013025314214905293, 'epoch': 0.35}\n",
      "{'loss': 0.6527, 'grad_norm': 0.21382156014442444, 'learning_rate': 0.0001302177376526819, 'epoch': 0.35}\n",
      "{'loss': 0.6015, 'grad_norm': 0.23533910512924194, 'learning_rate': 0.00013018233315631084, 'epoch': 0.35}\n",
      "{'loss': 0.6878, 'grad_norm': 0.2553133964538574, 'learning_rate': 0.00013014692865993984, 'epoch': 0.35}\n",
      "{'loss': 1.1013, 'grad_norm': 0.23375838994979858, 'learning_rate': 0.00013011152416356878, 'epoch': 0.35}\n",
      "{'loss': 0.856, 'grad_norm': 0.2923072874546051, 'learning_rate': 0.00013007611966719775, 'epoch': 0.35}\n",
      "{'loss': 0.7535, 'grad_norm': 0.25531384348869324, 'learning_rate': 0.0001300407151708267, 'epoch': 0.35}\n",
      "{'loss': 0.5557, 'grad_norm': 0.15416625142097473, 'learning_rate': 0.00013000531067445568, 'epoch': 0.35}\n",
      "{'loss': 0.678, 'grad_norm': 0.2625696063041687, 'learning_rate': 0.00012996990617808463, 'epoch': 0.35}\n",
      "{'loss': 0.9728, 'grad_norm': 0.2793366611003876, 'learning_rate': 0.0001299345016817136, 'epoch': 0.35}\n",
      "{'loss': 0.7014, 'grad_norm': 0.1857050657272339, 'learning_rate': 0.00012989909718534253, 'epoch': 0.35}\n",
      "{'loss': 0.8156, 'grad_norm': 0.3178828954696655, 'learning_rate': 0.0001298636926889715, 'epoch': 0.35}\n",
      "{'loss': 0.6949, 'grad_norm': 0.23180103302001953, 'learning_rate': 0.00012982828819260047, 'epoch': 0.35}\n",
      "{'loss': 0.5678, 'grad_norm': 0.22431866824626923, 'learning_rate': 0.0001297928836962294, 'epoch': 0.35}\n",
      "{'loss': 0.8566, 'grad_norm': 0.2796608805656433, 'learning_rate': 0.00012975747919985838, 'epoch': 0.35}\n",
      "{'loss': 0.6815, 'grad_norm': 0.2784372568130493, 'learning_rate': 0.00012972207470348735, 'epoch': 0.35}\n",
      "{'loss': 0.6094, 'grad_norm': 0.22849799692630768, 'learning_rate': 0.00012968667020711632, 'epoch': 0.35}\n",
      "{'loss': 0.7055, 'grad_norm': 0.19131192564964294, 'learning_rate': 0.00012965126571074526, 'epoch': 0.35}\n",
      "{'loss': 0.6368, 'grad_norm': 0.2353428602218628, 'learning_rate': 0.00012961586121437423, 'epoch': 0.35}\n",
      "{'loss': 0.8056, 'grad_norm': 0.2766427993774414, 'learning_rate': 0.0001295804567180032, 'epoch': 0.35}\n",
      "{'loss': 0.9247, 'grad_norm': 0.27520695328712463, 'learning_rate': 0.00012954505222163216, 'epoch': 0.35}\n",
      "{'loss': 0.6442, 'grad_norm': 0.21107177436351776, 'learning_rate': 0.0001295096477252611, 'epoch': 0.35}\n",
      "{'loss': 0.7468, 'grad_norm': 0.2581915259361267, 'learning_rate': 0.00012947424322889007, 'epoch': 0.35}\n",
      "{'loss': 0.6985, 'grad_norm': 0.2609824538230896, 'learning_rate': 0.00012943883873251904, 'epoch': 0.35}\n",
      "{'loss': 0.7878, 'grad_norm': 0.23998606204986572, 'learning_rate': 0.000129403434236148, 'epoch': 0.35}\n",
      "{'loss': 0.8461, 'grad_norm': 0.24596218764781952, 'learning_rate': 0.00012936802973977695, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee37635ab0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 220ce13f-522a-4e04-978e-bef44ade85b5)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7365, 'grad_norm': 0.23606492578983307, 'learning_rate': 0.00012933262524340592, 'epoch': 0.35}\n",
      "{'loss': 0.7442, 'grad_norm': 0.29858458042144775, 'learning_rate': 0.0001292972207470349, 'epoch': 0.35}\n",
      "{'loss': 1.0686, 'grad_norm': 0.3309692144393921, 'learning_rate': 0.00012926181625066386, 'epoch': 0.35}\n",
      "{'loss': 0.8067, 'grad_norm': 0.2661139667034149, 'learning_rate': 0.0001292264117542928, 'epoch': 0.35}\n",
      "{'loss': 0.6633, 'grad_norm': 0.20102441310882568, 'learning_rate': 0.00012919100725792177, 'epoch': 0.35}\n",
      "{'loss': 0.6424, 'grad_norm': 0.3060685396194458, 'learning_rate': 0.0001291556027615507, 'epoch': 0.35}\n",
      "{'loss': 0.5998, 'grad_norm': 0.18895766139030457, 'learning_rate': 0.0001291201982651797, 'epoch': 0.35}\n",
      "{'loss': 0.6661, 'grad_norm': 0.23000478744506836, 'learning_rate': 0.00012908479376880864, 'epoch': 0.36}\n",
      "{'loss': 0.8632, 'grad_norm': 0.22174608707427979, 'learning_rate': 0.0001290493892724376, 'epoch': 0.36}\n",
      "{'loss': 0.7792, 'grad_norm': 0.23891766369342804, 'learning_rate': 0.00012901398477606655, 'epoch': 0.36}\n",
      "{'loss': 0.5041, 'grad_norm': 0.20461703836917877, 'learning_rate': 0.00012897858027969555, 'epoch': 0.36}\n",
      "{'loss': 0.9809, 'grad_norm': 0.27665120363235474, 'learning_rate': 0.0001289431757833245, 'epoch': 0.36}\n",
      "{'loss': 0.6404, 'grad_norm': 0.1854954957962036, 'learning_rate': 0.00012890777128695343, 'epoch': 0.36}\n",
      "{'loss': 0.572, 'grad_norm': 0.2021096646785736, 'learning_rate': 0.0001288723667905824, 'epoch': 0.36}\n",
      "{'loss': 0.6419, 'grad_norm': 0.2346602976322174, 'learning_rate': 0.00012883696229421137, 'epoch': 0.36}\n",
      "{'loss': 0.6411, 'grad_norm': 0.2103341817855835, 'learning_rate': 0.00012880155779784034, 'epoch': 0.36}\n",
      "{'loss': 0.9576, 'grad_norm': 0.31831955909729004, 'learning_rate': 0.00012876615330146928, 'epoch': 0.36}\n",
      "{'loss': 0.7184, 'grad_norm': 0.19417238235473633, 'learning_rate': 0.00012873074880509825, 'epoch': 0.36}\n",
      "{'loss': 0.6595, 'grad_norm': 0.18874962627887726, 'learning_rate': 0.00012869534430872721, 'epoch': 0.36}\n",
      "{'loss': 0.6423, 'grad_norm': 0.19715595245361328, 'learning_rate': 0.00012865993981235618, 'epoch': 0.36}\n",
      "{'loss': 0.7769, 'grad_norm': 0.19519609212875366, 'learning_rate': 0.00012862453531598512, 'epoch': 0.36}\n",
      "{'loss': 0.532, 'grad_norm': 0.18543849885463715, 'learning_rate': 0.0001285891308196141, 'epoch': 0.36}\n",
      "{'loss': 0.7341, 'grad_norm': 0.22944676876068115, 'learning_rate': 0.00012855372632324306, 'epoch': 0.36}\n",
      "{'loss': 0.8537, 'grad_norm': 0.26485738158226013, 'learning_rate': 0.00012851832182687203, 'epoch': 0.36}\n",
      "{'loss': 0.9146, 'grad_norm': 0.28683438897132874, 'learning_rate': 0.00012848291733050097, 'epoch': 0.36}\n",
      "{'loss': 0.643, 'grad_norm': 0.1975380778312683, 'learning_rate': 0.00012844751283412994, 'epoch': 0.36}\n",
      "{'loss': 0.7221, 'grad_norm': 0.2222980260848999, 'learning_rate': 0.0001284121083377589, 'epoch': 0.36}\n",
      "{'loss': 0.592, 'grad_norm': 0.22982710599899292, 'learning_rate': 0.00012837670384138787, 'epoch': 0.36}\n",
      "{'loss': 0.841, 'grad_norm': 0.21300143003463745, 'learning_rate': 0.00012834129934501682, 'epoch': 0.36}\n",
      "{'loss': 0.7005, 'grad_norm': 0.24602149426937103, 'learning_rate': 0.00012830589484864578, 'epoch': 0.36}\n",
      "{'loss': 0.7749, 'grad_norm': 0.2706718146800995, 'learning_rate': 0.00012827049035227475, 'epoch': 0.36}\n",
      "{'loss': 0.8283, 'grad_norm': 0.24637681245803833, 'learning_rate': 0.00012823508585590372, 'epoch': 0.36}\n",
      "{'loss': 0.6407, 'grad_norm': 0.1869649738073349, 'learning_rate': 0.00012819968135953266, 'epoch': 0.36}\n",
      "{'loss': 0.7466, 'grad_norm': 0.16925844550132751, 'learning_rate': 0.00012816427686316163, 'epoch': 0.36}\n",
      "{'loss': 0.6748, 'grad_norm': 0.2236078381538391, 'learning_rate': 0.0001281288723667906, 'epoch': 0.36}\n",
      "{'loss': 0.6886, 'grad_norm': 0.2235143780708313, 'learning_rate': 0.00012809346787041957, 'epoch': 0.36}\n",
      "{'loss': 0.8456, 'grad_norm': 0.16365861892700195, 'learning_rate': 0.0001280580633740485, 'epoch': 0.36}\n",
      "{'loss': 0.8478, 'grad_norm': 0.3048495948314667, 'learning_rate': 0.00012802265887767748, 'epoch': 0.36}\n",
      "{'loss': 0.6188, 'grad_norm': 0.2134658694267273, 'learning_rate': 0.00012798725438130644, 'epoch': 0.36}\n",
      "{'loss': 0.7001, 'grad_norm': 0.22823414206504822, 'learning_rate': 0.00012795184988493539, 'epoch': 0.36}\n",
      "{'loss': 0.7708, 'grad_norm': 0.27547645568847656, 'learning_rate': 0.00012791644538856435, 'epoch': 0.36}\n",
      "{'loss': 0.6892, 'grad_norm': 0.23029083013534546, 'learning_rate': 0.0001278810408921933, 'epoch': 0.36}\n",
      "{'loss': 0.748, 'grad_norm': 0.27419722080230713, 'learning_rate': 0.00012784563639582226, 'epoch': 0.36}\n",
      "{'loss': 0.9585, 'grad_norm': 0.3081391155719757, 'learning_rate': 0.00012781023189945123, 'epoch': 0.36}\n",
      "{'loss': 0.8151, 'grad_norm': 0.18692009150981903, 'learning_rate': 0.0001277748274030802, 'epoch': 0.36}\n",
      "{'loss': 0.6405, 'grad_norm': 0.17174145579338074, 'learning_rate': 0.00012773942290670914, 'epoch': 0.36}\n",
      "{'loss': 0.5353, 'grad_norm': 0.2520834505558014, 'learning_rate': 0.0001277040184103381, 'epoch': 0.36}\n",
      "{'loss': 0.8222, 'grad_norm': 0.2584437131881714, 'learning_rate': 0.00012766861391396708, 'epoch': 0.36}\n",
      "{'loss': 0.8731, 'grad_norm': 0.3265135586261749, 'learning_rate': 0.00012763320941759605, 'epoch': 0.36}\n",
      "{'loss': 0.9756, 'grad_norm': 0.2622045874595642, 'learning_rate': 0.000127597804921225, 'epoch': 0.36}\n",
      "{'loss': 0.5157, 'grad_norm': 0.2184644490480423, 'learning_rate': 0.00012756240042485396, 'epoch': 0.36}\n",
      "{'loss': 0.4701, 'grad_norm': 0.18366055190563202, 'learning_rate': 0.00012752699592848292, 'epoch': 0.36}\n",
      "{'loss': 0.5546, 'grad_norm': 0.176883727312088, 'learning_rate': 0.0001274915914321119, 'epoch': 0.36}\n",
      "{'loss': 0.789, 'grad_norm': 0.18269871175289154, 'learning_rate': 0.00012745618693574083, 'epoch': 0.36}\n",
      "{'loss': 0.5606, 'grad_norm': 0.22426390647888184, 'learning_rate': 0.0001274207824393698, 'epoch': 0.36}\n",
      "{'loss': 0.5307, 'grad_norm': 0.16340167820453644, 'learning_rate': 0.00012738537794299877, 'epoch': 0.36}\n",
      "{'loss': 1.3917, 'grad_norm': 0.2623806297779083, 'learning_rate': 0.00012734997344662774, 'epoch': 0.36}\n",
      "{'loss': 0.808, 'grad_norm': 0.26567983627319336, 'learning_rate': 0.00012731456895025668, 'epoch': 0.36}\n",
      "{'loss': 0.9073, 'grad_norm': 0.21618586778640747, 'learning_rate': 0.00012727916445388565, 'epoch': 0.36}\n",
      "{'loss': 0.7317, 'grad_norm': 0.22246085107326508, 'learning_rate': 0.00012724375995751462, 'epoch': 0.36}\n",
      "{'loss': 0.5776, 'grad_norm': 0.17908504605293274, 'learning_rate': 0.00012720835546114358, 'epoch': 0.36}\n",
      "{'loss': 0.7575, 'grad_norm': 0.23187416791915894, 'learning_rate': 0.00012717295096477253, 'epoch': 0.36}\n",
      "{'loss': 0.6321, 'grad_norm': 0.22328023612499237, 'learning_rate': 0.0001271375464684015, 'epoch': 0.36}\n",
      "{'loss': 0.4789, 'grad_norm': 0.23601460456848145, 'learning_rate': 0.00012710214197203046, 'epoch': 0.37}\n",
      "{'loss': 0.7655, 'grad_norm': 0.23125220835208893, 'learning_rate': 0.00012706673747565943, 'epoch': 0.37}\n",
      "{'loss': 0.5373, 'grad_norm': 0.16786591708660126, 'learning_rate': 0.00012703133297928837, 'epoch': 0.37}\n",
      "{'loss': 0.7508, 'grad_norm': 0.19349586963653564, 'learning_rate': 0.0001269959284829173, 'epoch': 0.37}\n",
      "{'loss': 0.6515, 'grad_norm': 0.18861621618270874, 'learning_rate': 0.0001269605239865463, 'epoch': 0.37}\n",
      "{'loss': 0.7484, 'grad_norm': 0.1909700185060501, 'learning_rate': 0.00012692511949017525, 'epoch': 0.37}\n",
      "{'loss': 0.8342, 'grad_norm': 0.277723103761673, 'learning_rate': 0.00012688971499380422, 'epoch': 0.37}\n",
      "{'loss': 0.8143, 'grad_norm': 0.21842435002326965, 'learning_rate': 0.00012685431049743316, 'epoch': 0.37}\n",
      "{'loss': 0.6937, 'grad_norm': 0.2162848711013794, 'learning_rate': 0.00012681890600106216, 'epoch': 0.37}\n",
      "{'loss': 0.9662, 'grad_norm': 0.24978402256965637, 'learning_rate': 0.0001267835015046911, 'epoch': 0.37}\n",
      "{'loss': 0.8425, 'grad_norm': 0.26516032218933105, 'learning_rate': 0.00012674809700832006, 'epoch': 0.37}\n",
      "{'loss': 0.9845, 'grad_norm': 0.28204262256622314, 'learning_rate': 0.000126712692511949, 'epoch': 0.37}\n",
      "{'loss': 0.8242, 'grad_norm': 0.2782309353351593, 'learning_rate': 0.000126677288015578, 'epoch': 0.37}\n",
      "{'loss': 0.781, 'grad_norm': 0.27882492542266846, 'learning_rate': 0.00012664188351920694, 'epoch': 0.37}\n",
      "{'loss': 0.7962, 'grad_norm': 0.21018725633621216, 'learning_rate': 0.0001266064790228359, 'epoch': 0.37}\n",
      "{'loss': 0.6702, 'grad_norm': 0.2383050173521042, 'learning_rate': 0.00012657107452646485, 'epoch': 0.37}\n",
      "{'loss': 0.7417, 'grad_norm': 0.24760235846042633, 'learning_rate': 0.00012653567003009382, 'epoch': 0.37}\n",
      "{'loss': 0.6988, 'grad_norm': 0.23143993318080902, 'learning_rate': 0.0001265002655337228, 'epoch': 0.37}\n",
      "{'loss': 0.9915, 'grad_norm': 0.274544358253479, 'learning_rate': 0.00012646486103735176, 'epoch': 0.37}\n",
      "{'loss': 0.9171, 'grad_norm': 0.23407307267189026, 'learning_rate': 0.0001264294565409807, 'epoch': 0.37}\n",
      "{'loss': 0.7106, 'grad_norm': 0.24658645689487457, 'learning_rate': 0.00012639405204460967, 'epoch': 0.37}\n",
      "{'loss': 0.5464, 'grad_norm': 0.18084095418453217, 'learning_rate': 0.00012635864754823863, 'epoch': 0.37}\n",
      "{'loss': 0.6817, 'grad_norm': 0.19737327098846436, 'learning_rate': 0.0001263232430518676, 'epoch': 0.37}\n",
      "{'loss': 0.7686, 'grad_norm': 0.21178744733333588, 'learning_rate': 0.00012628783855549654, 'epoch': 0.37}\n",
      "{'loss': 0.5187, 'grad_norm': 0.17566891014575958, 'learning_rate': 0.0001262524340591255, 'epoch': 0.37}\n",
      "{'loss': 0.4994, 'grad_norm': 0.1970471441745758, 'learning_rate': 0.00012621702956275448, 'epoch': 0.37}\n",
      "{'loss': 0.5194, 'grad_norm': 0.1645677089691162, 'learning_rate': 0.00012618162506638345, 'epoch': 0.37}\n",
      "{'loss': 1.5901, 'grad_norm': 0.17583529651165009, 'learning_rate': 0.0001261462205700124, 'epoch': 0.37}\n",
      "{'loss': 0.6413, 'grad_norm': 0.21344855427742004, 'learning_rate': 0.00012611081607364136, 'epoch': 0.37}\n",
      "{'loss': 0.6349, 'grad_norm': 0.20801793038845062, 'learning_rate': 0.00012607541157727033, 'epoch': 0.37}\n",
      "{'loss': 0.8647, 'grad_norm': 0.25589555501937866, 'learning_rate': 0.00012604000708089927, 'epoch': 0.37}\n",
      "{'loss': 0.8155, 'grad_norm': 0.25956466794013977, 'learning_rate': 0.00012600460258452824, 'epoch': 0.37}\n",
      "{'loss': 0.7041, 'grad_norm': 0.2736298441886902, 'learning_rate': 0.0001259691980881572, 'epoch': 0.37}\n",
      "{'loss': 1.0369, 'grad_norm': 0.23962773382663727, 'learning_rate': 0.00012593379359178617, 'epoch': 0.37}\n",
      "{'loss': 0.639, 'grad_norm': 0.1859341710805893, 'learning_rate': 0.00012589838909541511, 'epoch': 0.37}\n",
      "{'loss': 0.8049, 'grad_norm': 0.24769681692123413, 'learning_rate': 0.00012586298459904408, 'epoch': 0.37}\n",
      "{'loss': 0.6941, 'grad_norm': 0.19345831871032715, 'learning_rate': 0.00012582758010267302, 'epoch': 0.37}\n",
      "{'loss': 0.7815, 'grad_norm': 0.2189275175333023, 'learning_rate': 0.00012579217560630202, 'epoch': 0.37}\n",
      "{'loss': 0.7444, 'grad_norm': 0.21962511539459229, 'learning_rate': 0.00012575677110993096, 'epoch': 0.37}\n",
      "{'loss': 0.763, 'grad_norm': 0.23712565004825592, 'learning_rate': 0.00012572136661355993, 'epoch': 0.37}\n",
      "{'loss': 0.8471, 'grad_norm': 0.21290184557437897, 'learning_rate': 0.00012568596211718887, 'epoch': 0.37}\n",
      "{'loss': 1.015, 'grad_norm': 0.32924342155456543, 'learning_rate': 0.00012565055762081787, 'epoch': 0.37}\n",
      "{'loss': 0.7476, 'grad_norm': 0.2373090386390686, 'learning_rate': 0.0001256151531244468, 'epoch': 0.37}\n",
      "{'loss': 1.1196, 'grad_norm': 0.28839728236198425, 'learning_rate': 0.00012557974862807578, 'epoch': 0.37}\n",
      "{'loss': 0.6874, 'grad_norm': 0.24684946238994598, 'learning_rate': 0.00012554434413170472, 'epoch': 0.37}\n",
      "{'loss': 0.7598, 'grad_norm': 0.2125188559293747, 'learning_rate': 0.0001255089396353337, 'epoch': 0.37}\n",
      "{'loss': 0.7866, 'grad_norm': 0.2939067780971527, 'learning_rate': 0.00012547353513896265, 'epoch': 0.37}\n",
      "{'loss': 0.7993, 'grad_norm': 0.19833707809448242, 'learning_rate': 0.00012543813064259162, 'epoch': 0.37}\n",
      "{'loss': 0.9162, 'grad_norm': 0.20886720716953278, 'learning_rate': 0.00012540272614622056, 'epoch': 0.37}\n",
      "{'loss': 0.5728, 'grad_norm': 0.19632644951343536, 'learning_rate': 0.00012536732164984956, 'epoch': 0.37}\n",
      "{'loss': 0.7075, 'grad_norm': 0.2111773043870926, 'learning_rate': 0.0001253319171534785, 'epoch': 0.37}\n",
      "{'loss': 0.6206, 'grad_norm': 0.24055425822734833, 'learning_rate': 0.00012529651265710747, 'epoch': 0.37}\n",
      "{'loss': 0.8764, 'grad_norm': 0.19680632650852203, 'learning_rate': 0.0001252611081607364, 'epoch': 0.37}\n",
      "{'loss': 0.7353, 'grad_norm': 0.23316846787929535, 'learning_rate': 0.00012522570366436538, 'epoch': 0.37}\n",
      "{'loss': 0.791, 'grad_norm': 0.27386683225631714, 'learning_rate': 0.00012519029916799435, 'epoch': 0.37}\n",
      "{'loss': 0.795, 'grad_norm': 0.23838329315185547, 'learning_rate': 0.00012515489467162331, 'epoch': 0.37}\n",
      "{'loss': 0.842, 'grad_norm': 0.258459210395813, 'learning_rate': 0.00012511949017525225, 'epoch': 0.37}\n",
      "{'loss': 0.6791, 'grad_norm': 0.1784159243106842, 'learning_rate': 0.00012508408567888122, 'epoch': 0.38}\n",
      "{'loss': 1.4178, 'grad_norm': 0.4394546449184418, 'learning_rate': 0.0001250486811825102, 'epoch': 0.38}\n",
      "{'loss': 1.1994, 'grad_norm': 0.30874016880989075, 'learning_rate': 0.00012501327668613913, 'epoch': 0.38}\n",
      "{'loss': 0.69, 'grad_norm': 0.33316275477409363, 'learning_rate': 0.0001249778721897681, 'epoch': 0.38}\n",
      "{'loss': 0.879, 'grad_norm': 0.27651479840278625, 'learning_rate': 0.00012494246769339707, 'epoch': 0.38}\n",
      "{'loss': 0.919, 'grad_norm': 0.23236985504627228, 'learning_rate': 0.00012490706319702604, 'epoch': 0.38}\n",
      "{'loss': 0.5411, 'grad_norm': 0.18649788200855255, 'learning_rate': 0.00012487165870065498, 'epoch': 0.38}\n",
      "{'loss': 0.588, 'grad_norm': 0.2606830596923828, 'learning_rate': 0.00012483625420428395, 'epoch': 0.38}\n",
      "{'loss': 0.8113, 'grad_norm': 0.22881801426410675, 'learning_rate': 0.00012480084970791292, 'epoch': 0.38}\n",
      "{'loss': 0.6357, 'grad_norm': 0.2529013752937317, 'learning_rate': 0.00012476544521154188, 'epoch': 0.38}\n",
      "{'loss': 0.8166, 'grad_norm': 0.2437940537929535, 'learning_rate': 0.00012473004071517083, 'epoch': 0.38}\n",
      "{'loss': 0.722, 'grad_norm': 0.2455959916114807, 'learning_rate': 0.0001246946362187998, 'epoch': 0.38}\n",
      "{'loss': 0.9237, 'grad_norm': 0.25820404291152954, 'learning_rate': 0.00012465923172242876, 'epoch': 0.38}\n",
      "{'loss': 0.8231, 'grad_norm': 0.30794456601142883, 'learning_rate': 0.00012462382722605773, 'epoch': 0.38}\n",
      "{'loss': 0.8265, 'grad_norm': 0.3506036698818207, 'learning_rate': 0.00012458842272968667, 'epoch': 0.38}\n",
      "{'loss': 1.1067, 'grad_norm': 0.2716263234615326, 'learning_rate': 0.00012455301823331564, 'epoch': 0.38}\n",
      "{'loss': 0.9939, 'grad_norm': 0.27374422550201416, 'learning_rate': 0.00012451761373694458, 'epoch': 0.38}\n",
      "{'loss': 0.7946, 'grad_norm': 0.22410427033901215, 'learning_rate': 0.00012448220924057358, 'epoch': 0.38}\n",
      "{'loss': 0.744, 'grad_norm': 0.22236283123493195, 'learning_rate': 0.00012444680474420252, 'epoch': 0.38}\n",
      "{'loss': 1.0402, 'grad_norm': 0.2625115215778351, 'learning_rate': 0.00012441140024783149, 'epoch': 0.38}\n",
      "{'loss': 0.8558, 'grad_norm': 0.23525561392307281, 'learning_rate': 0.00012437599575146043, 'epoch': 0.38}\n",
      "{'loss': 0.6525, 'grad_norm': 0.28889429569244385, 'learning_rate': 0.00012434059125508942, 'epoch': 0.38}\n",
      "{'loss': 0.8893, 'grad_norm': 0.2032858431339264, 'learning_rate': 0.00012430518675871836, 'epoch': 0.38}\n",
      "{'loss': 0.6774, 'grad_norm': 0.2300385683774948, 'learning_rate': 0.00012426978226234733, 'epoch': 0.38}\n",
      "{'loss': 0.5697, 'grad_norm': 0.209683358669281, 'learning_rate': 0.00012423437776597627, 'epoch': 0.38}\n",
      "{'loss': 0.7381, 'grad_norm': 0.22581996023654938, 'learning_rate': 0.00012419897326960527, 'epoch': 0.38}\n",
      "{'loss': 0.6953, 'grad_norm': 0.2507040798664093, 'learning_rate': 0.0001241635687732342, 'epoch': 0.38}\n",
      "{'loss': 0.6599, 'grad_norm': 0.20749451220035553, 'learning_rate': 0.00012412816427686315, 'epoch': 0.38}\n",
      "{'loss': 0.6655, 'grad_norm': 0.23066222667694092, 'learning_rate': 0.00012409275978049212, 'epoch': 0.38}\n",
      "{'loss': 0.6849, 'grad_norm': 0.20257680118083954, 'learning_rate': 0.0001240573552841211, 'epoch': 0.38}\n",
      "{'loss': 0.7134, 'grad_norm': 0.2029353231191635, 'learning_rate': 0.00012402195078775006, 'epoch': 0.38}\n",
      "{'loss': 0.707, 'grad_norm': 0.23547188937664032, 'learning_rate': 0.000123986546291379, 'epoch': 0.38}\n",
      "{'loss': 0.5704, 'grad_norm': 0.216630220413208, 'learning_rate': 0.00012395114179500797, 'epoch': 0.38}\n",
      "{'loss': 0.5836, 'grad_norm': 0.21558640897274017, 'learning_rate': 0.00012391573729863693, 'epoch': 0.38}\n",
      "{'loss': 0.7778, 'grad_norm': 0.20503202080726624, 'learning_rate': 0.0001238803328022659, 'epoch': 0.38}\n",
      "{'loss': 0.7195, 'grad_norm': 0.2159598171710968, 'learning_rate': 0.00012384492830589484, 'epoch': 0.38}\n",
      "{'loss': 0.968, 'grad_norm': 0.28183355927467346, 'learning_rate': 0.0001238095238095238, 'epoch': 0.38}\n",
      "{'loss': 0.8305, 'grad_norm': 0.221762552857399, 'learning_rate': 0.00012377411931315278, 'epoch': 0.38}\n",
      "{'loss': 0.8155, 'grad_norm': 0.24881315231323242, 'learning_rate': 0.00012373871481678175, 'epoch': 0.38}\n",
      "{'loss': 0.8727, 'grad_norm': 0.24256333708763123, 'learning_rate': 0.0001237033103204107, 'epoch': 0.38}\n",
      "{'loss': 0.5988, 'grad_norm': 0.21685607731342316, 'learning_rate': 0.00012366790582403966, 'epoch': 0.38}\n",
      "{'loss': 0.6804, 'grad_norm': 0.27695396542549133, 'learning_rate': 0.00012363250132766863, 'epoch': 0.38}\n",
      "{'loss': 0.7247, 'grad_norm': 0.20149458944797516, 'learning_rate': 0.0001235970968312976, 'epoch': 0.38}\n",
      "{'loss': 1.1445, 'grad_norm': 0.28860408067703247, 'learning_rate': 0.00012356169233492654, 'epoch': 0.38}\n",
      "{'loss': 0.9651, 'grad_norm': 0.26849812269210815, 'learning_rate': 0.0001235262878385555, 'epoch': 0.38}\n",
      "{'loss': 0.7092, 'grad_norm': 0.19466014206409454, 'learning_rate': 0.00012349088334218447, 'epoch': 0.38}\n",
      "{'loss': 0.8118, 'grad_norm': 0.24200838804244995, 'learning_rate': 0.00012345547884581344, 'epoch': 0.38}\n",
      "{'loss': 0.6688, 'grad_norm': 0.2402571737766266, 'learning_rate': 0.00012342007434944238, 'epoch': 0.38}\n",
      "{'loss': 0.607, 'grad_norm': 0.2105657458305359, 'learning_rate': 0.00012338466985307135, 'epoch': 0.38}\n",
      "{'loss': 0.6226, 'grad_norm': 0.19510352611541748, 'learning_rate': 0.00012334926535670032, 'epoch': 0.38}\n",
      "{'loss': 1.0125, 'grad_norm': 0.22928528487682343, 'learning_rate': 0.0001233138608603293, 'epoch': 0.38}\n",
      "{'loss': 1.0283, 'grad_norm': 0.22901083528995514, 'learning_rate': 0.00012327845636395823, 'epoch': 0.38}\n",
      "{'loss': 0.6202, 'grad_norm': 0.2605890929698944, 'learning_rate': 0.0001232430518675872, 'epoch': 0.38}\n",
      "{'loss': 1.0069, 'grad_norm': 0.32392358779907227, 'learning_rate': 0.00012320764737121614, 'epoch': 0.38}\n",
      "{'loss': 0.8513, 'grad_norm': 0.2217944711446762, 'learning_rate': 0.0001231722428748451, 'epoch': 0.38}\n",
      "{'loss': 0.7979, 'grad_norm': 0.2410537600517273, 'learning_rate': 0.00012313683837847407, 'epoch': 0.38}\n",
      "{'loss': 0.64, 'grad_norm': 0.20066560804843903, 'learning_rate': 0.00012310143388210302, 'epoch': 0.39}\n",
      "{'loss': 0.8148, 'grad_norm': 0.25342968106269836, 'learning_rate': 0.00012306602938573198, 'epoch': 0.39}\n",
      "{'loss': 0.6321, 'grad_norm': 0.21565541625022888, 'learning_rate': 0.00012303062488936095, 'epoch': 0.39}\n",
      "{'loss': 0.6439, 'grad_norm': 0.19320376217365265, 'learning_rate': 0.00012299522039298992, 'epoch': 0.39}\n",
      "{'loss': 0.9355, 'grad_norm': 0.2522253394126892, 'learning_rate': 0.00012295981589661886, 'epoch': 0.39}\n",
      "{'loss': 0.5892, 'grad_norm': 0.19472332298755646, 'learning_rate': 0.00012292441140024783, 'epoch': 0.39}\n",
      "{'loss': 0.6791, 'grad_norm': 0.2591189742088318, 'learning_rate': 0.0001228890069038768, 'epoch': 0.39}\n",
      "{'loss': 0.9616, 'grad_norm': 0.27073389291763306, 'learning_rate': 0.00012285360240750577, 'epoch': 0.39}\n",
      "{'loss': 0.9654, 'grad_norm': 0.286888062953949, 'learning_rate': 0.0001228181979111347, 'epoch': 0.39}\n",
      "{'loss': 0.8223, 'grad_norm': 0.17337250709533691, 'learning_rate': 0.00012278279341476368, 'epoch': 0.39}\n",
      "{'loss': 0.7445, 'grad_norm': 0.19503624737262726, 'learning_rate': 0.00012274738891839264, 'epoch': 0.39}\n",
      "{'loss': 0.8712, 'grad_norm': 0.2639225125312805, 'learning_rate': 0.0001227119844220216, 'epoch': 0.39}\n",
      "{'loss': 0.6671, 'grad_norm': 0.2844751179218292, 'learning_rate': 0.00012267657992565055, 'epoch': 0.39}\n",
      "{'loss': 0.7974, 'grad_norm': 0.21532249450683594, 'learning_rate': 0.00012264117542927952, 'epoch': 0.39}\n",
      "{'loss': 0.8006, 'grad_norm': 0.24244165420532227, 'learning_rate': 0.0001226057709329085, 'epoch': 0.39}\n",
      "{'loss': 0.6216, 'grad_norm': 0.16606514155864716, 'learning_rate': 0.00012257036643653746, 'epoch': 0.39}\n",
      "{'loss': 0.8906, 'grad_norm': 0.21298189461231232, 'learning_rate': 0.0001225349619401664, 'epoch': 0.39}\n",
      "{'loss': 0.6398, 'grad_norm': 0.24135881662368774, 'learning_rate': 0.00012249955744379537, 'epoch': 0.39}\n",
      "{'loss': 0.9658, 'grad_norm': 0.25612980127334595, 'learning_rate': 0.00012246415294742434, 'epoch': 0.39}\n",
      "{'loss': 0.7005, 'grad_norm': 0.2412205934524536, 'learning_rate': 0.0001224287484510533, 'epoch': 0.39}\n",
      "{'loss': 0.6163, 'grad_norm': 0.21950994431972504, 'learning_rate': 0.00012239334395468225, 'epoch': 0.39}\n",
      "{'loss': 0.9452, 'grad_norm': 0.23154550790786743, 'learning_rate': 0.00012235793945831121, 'epoch': 0.39}\n",
      "{'loss': 0.818, 'grad_norm': 0.2188534140586853, 'learning_rate': 0.00012232253496194018, 'epoch': 0.39}\n",
      "{'loss': 0.7636, 'grad_norm': 0.22774071991443634, 'learning_rate': 0.00012228713046556915, 'epoch': 0.39}\n",
      "{'loss': 0.6645, 'grad_norm': 0.20006558299064636, 'learning_rate': 0.0001222517259691981, 'epoch': 0.39}\n",
      "{'loss': 0.6836, 'grad_norm': 0.23367665708065033, 'learning_rate': 0.00012221632147282703, 'epoch': 0.39}\n",
      "{'loss': 0.7941, 'grad_norm': 0.2863070070743561, 'learning_rate': 0.00012218091697645603, 'epoch': 0.39}\n",
      "{'loss': 0.9436, 'grad_norm': 0.26832783222198486, 'learning_rate': 0.00012214551248008497, 'epoch': 0.39}\n",
      "{'loss': 0.9041, 'grad_norm': 0.2424475997686386, 'learning_rate': 0.00012211010798371394, 'epoch': 0.39}\n",
      "{'loss': 0.5848, 'grad_norm': 0.2203204333782196, 'learning_rate': 0.00012207470348734288, 'epoch': 0.39}\n",
      "{'loss': 0.8504, 'grad_norm': 0.23013590276241302, 'learning_rate': 0.00012203929899097186, 'epoch': 0.39}\n",
      "{'loss': 0.6482, 'grad_norm': 0.20288608968257904, 'learning_rate': 0.00012200389449460082, 'epoch': 0.39}\n",
      "{'loss': 0.917, 'grad_norm': 0.2306664139032364, 'learning_rate': 0.00012196848999822978, 'epoch': 0.39}\n",
      "{'loss': 0.6742, 'grad_norm': 0.22532425820827484, 'learning_rate': 0.00012193308550185874, 'epoch': 0.39}\n",
      "{'loss': 1.3778, 'grad_norm': 0.2724865674972534, 'learning_rate': 0.00012189768100548771, 'epoch': 0.39}\n",
      "{'loss': 1.0751, 'grad_norm': 0.24775350093841553, 'learning_rate': 0.00012186227650911666, 'epoch': 0.39}\n",
      "{'loss': 1.0054, 'grad_norm': 0.28983354568481445, 'learning_rate': 0.00012182687201274563, 'epoch': 0.39}\n",
      "{'loss': 0.5053, 'grad_norm': 0.1906995326280594, 'learning_rate': 0.00012179146751637459, 'epoch': 0.39}\n",
      "{'loss': 0.8209, 'grad_norm': 0.2925916314125061, 'learning_rate': 0.00012175606302000355, 'epoch': 0.39}\n",
      "{'loss': 0.5504, 'grad_norm': 0.19953252375125885, 'learning_rate': 0.00012172065852363251, 'epoch': 0.39}\n",
      "{'loss': 0.6802, 'grad_norm': 0.36090171337127686, 'learning_rate': 0.00012168525402726148, 'epoch': 0.39}\n",
      "{'loss': 0.6225, 'grad_norm': 0.22494469583034515, 'learning_rate': 0.00012164984953089043, 'epoch': 0.39}\n",
      "{'loss': 0.9948, 'grad_norm': 0.22933624684810638, 'learning_rate': 0.0001216144450345194, 'epoch': 0.39}\n",
      "{'loss': 0.8968, 'grad_norm': 0.30959534645080566, 'learning_rate': 0.00012157904053814834, 'epoch': 0.39}\n",
      "{'loss': 1.0524, 'grad_norm': 0.20749086141586304, 'learning_rate': 0.00012154363604177732, 'epoch': 0.39}\n",
      "{'loss': 0.6143, 'grad_norm': 0.22882701456546783, 'learning_rate': 0.00012150823154540626, 'epoch': 0.39}\n",
      "{'loss': 0.8077, 'grad_norm': 0.27720534801483154, 'learning_rate': 0.00012147282704903525, 'epoch': 0.39}\n",
      "{'loss': 0.545, 'grad_norm': 0.19170847535133362, 'learning_rate': 0.00012143742255266419, 'epoch': 0.39}\n",
      "{'loss': 0.5509, 'grad_norm': 0.20963898301124573, 'learning_rate': 0.00012140201805629317, 'epoch': 0.39}\n",
      "{'loss': 0.6886, 'grad_norm': 0.1755203902721405, 'learning_rate': 0.00012136661355992211, 'epoch': 0.39}\n",
      "{'loss': 0.8801, 'grad_norm': 0.30947980284690857, 'learning_rate': 0.00012133120906355109, 'epoch': 0.39}\n",
      "{'loss': 0.5862, 'grad_norm': 0.19904066622257233, 'learning_rate': 0.00012129580456718003, 'epoch': 0.39}\n",
      "{'loss': 0.7068, 'grad_norm': 0.24207039177417755, 'learning_rate': 0.00012126040007080899, 'epoch': 0.39}\n",
      "{'loss': 0.5454, 'grad_norm': 0.20041877031326294, 'learning_rate': 0.00012122499557443796, 'epoch': 0.39}\n",
      "{'loss': 0.8326, 'grad_norm': 0.254379540681839, 'learning_rate': 0.00012118959107806691, 'epoch': 0.39}\n",
      "{'loss': 0.4203, 'grad_norm': 0.14937393367290497, 'learning_rate': 0.00012115418658169588, 'epoch': 0.39}\n",
      "{'loss': 0.8266, 'grad_norm': 0.26241815090179443, 'learning_rate': 0.00012111878208532483, 'epoch': 0.39}\n",
      "{'loss': 0.6758, 'grad_norm': 0.2390831708908081, 'learning_rate': 0.0001210833775889538, 'epoch': 0.4}\n",
      "{'loss': 1.1178, 'grad_norm': 0.26538363099098206, 'learning_rate': 0.00012104797309258276, 'epoch': 0.4}\n",
      "{'loss': 0.7034, 'grad_norm': 0.23710013926029205, 'learning_rate': 0.00012101256859621173, 'epoch': 0.4}\n",
      "{'loss': 1.0434, 'grad_norm': 0.2675431966781616, 'learning_rate': 0.00012097716409984068, 'epoch': 0.4}\n",
      "{'loss': 0.8581, 'grad_norm': 0.24432528018951416, 'learning_rate': 0.00012094175960346965, 'epoch': 0.4}\n",
      "{'loss': 0.5741, 'grad_norm': 0.17055021226406097, 'learning_rate': 0.0001209063551070986, 'epoch': 0.4}\n",
      "{'loss': 0.9763, 'grad_norm': 0.21297556161880493, 'learning_rate': 0.00012087095061072757, 'epoch': 0.4}\n",
      "{'loss': 0.7617, 'grad_norm': 0.25809887051582336, 'learning_rate': 0.00012083554611435653, 'epoch': 0.4}\n",
      "{'loss': 0.7604, 'grad_norm': 0.2463485151529312, 'learning_rate': 0.0001208001416179855, 'epoch': 0.4}\n",
      "{'loss': 0.5685, 'grad_norm': 0.18353909254074097, 'learning_rate': 0.00012076473712161445, 'epoch': 0.4}\n",
      "{'loss': 0.6998, 'grad_norm': 0.22297321259975433, 'learning_rate': 0.00012072933262524342, 'epoch': 0.4}\n",
      "{'loss': 0.563, 'grad_norm': 0.21009816229343414, 'learning_rate': 0.00012069392812887237, 'epoch': 0.4}\n",
      "{'loss': 0.7062, 'grad_norm': 0.17196285724639893, 'learning_rate': 0.00012065852363250134, 'epoch': 0.4}\n",
      "{'loss': 0.6988, 'grad_norm': 0.2442370355129242, 'learning_rate': 0.0001206231191361303, 'epoch': 0.4}\n",
      "{'loss': 0.8672, 'grad_norm': 0.23348495364189148, 'learning_rate': 0.00012058771463975926, 'epoch': 0.4}\n",
      "{'loss': 0.6932, 'grad_norm': 0.30274471640586853, 'learning_rate': 0.00012055231014338822, 'epoch': 0.4}\n",
      "{'loss': 0.5643, 'grad_norm': 0.1735321581363678, 'learning_rate': 0.00012051690564701719, 'epoch': 0.4}\n",
      "{'loss': 0.482, 'grad_norm': 0.17172127962112427, 'learning_rate': 0.00012048150115064614, 'epoch': 0.4}\n",
      "{'loss': 0.6342, 'grad_norm': 0.22725600004196167, 'learning_rate': 0.00012044609665427511, 'epoch': 0.4}\n",
      "{'loss': 0.7257, 'grad_norm': 0.25460758805274963, 'learning_rate': 0.00012041069215790407, 'epoch': 0.4}\n",
      "{'loss': 0.5615, 'grad_norm': 0.20895351469516754, 'learning_rate': 0.000120375287661533, 'epoch': 0.4}\n",
      "{'loss': 1.0183, 'grad_norm': 0.25263142585754395, 'learning_rate': 0.00012033988316516199, 'epoch': 0.4}\n",
      "{'loss': 0.7574, 'grad_norm': 0.22925546765327454, 'learning_rate': 0.00012030447866879093, 'epoch': 0.4}\n",
      "{'loss': 0.6514, 'grad_norm': 0.20306308567523956, 'learning_rate': 0.0001202690741724199, 'epoch': 0.4}\n",
      "{'loss': 0.8618, 'grad_norm': 0.25503209233283997, 'learning_rate': 0.00012023366967604885, 'epoch': 0.4}\n",
      "{'loss': 0.6853, 'grad_norm': 0.29374757409095764, 'learning_rate': 0.00012019826517967782, 'epoch': 0.4}\n",
      "{'loss': 0.5605, 'grad_norm': 0.18868012726306915, 'learning_rate': 0.00012016286068330678, 'epoch': 0.4}\n",
      "{'loss': 0.8447, 'grad_norm': 0.255622535943985, 'learning_rate': 0.00012012745618693574, 'epoch': 0.4}\n",
      "{'loss': 0.9439, 'grad_norm': 0.2693394720554352, 'learning_rate': 0.0001200920516905647, 'epoch': 0.4}\n",
      "{'loss': 0.5168, 'grad_norm': 0.20424912869930267, 'learning_rate': 0.00012005664719419367, 'epoch': 0.4}\n",
      "{'loss': 0.7516, 'grad_norm': 0.23109300434589386, 'learning_rate': 0.00012002124269782262, 'epoch': 0.4}\n",
      "{'loss': 0.6731, 'grad_norm': 0.23925051093101501, 'learning_rate': 0.00011998583820145159, 'epoch': 0.4}\n",
      "{'loss': 0.7229, 'grad_norm': 0.21504566073417664, 'learning_rate': 0.00011995043370508055, 'epoch': 0.4}\n",
      "{'loss': 0.9587, 'grad_norm': 0.2912485599517822, 'learning_rate': 0.00011991502920870951, 'epoch': 0.4}\n",
      "{'loss': 0.664, 'grad_norm': 0.24263954162597656, 'learning_rate': 0.00011987962471233847, 'epoch': 0.4}\n",
      "{'loss': 1.0048, 'grad_norm': 0.24129977822303772, 'learning_rate': 0.00011984422021596744, 'epoch': 0.4}\n",
      "{'loss': 0.7822, 'grad_norm': 0.2971113622188568, 'learning_rate': 0.00011980881571959639, 'epoch': 0.4}\n",
      "{'loss': 0.5641, 'grad_norm': 0.21080705523490906, 'learning_rate': 0.00011977341122322536, 'epoch': 0.4}\n",
      "{'loss': 0.7672, 'grad_norm': 0.255670428276062, 'learning_rate': 0.00011973800672685431, 'epoch': 0.4}\n",
      "{'loss': 0.601, 'grad_norm': 0.23806089162826538, 'learning_rate': 0.00011970260223048328, 'epoch': 0.4}\n",
      "{'loss': 0.8863, 'grad_norm': 0.2218153327703476, 'learning_rate': 0.00011966719773411224, 'epoch': 0.4}\n",
      "{'loss': 0.5127, 'grad_norm': 0.16932463645935059, 'learning_rate': 0.0001196317932377412, 'epoch': 0.4}\n",
      "{'loss': 0.635, 'grad_norm': 0.19385743141174316, 'learning_rate': 0.00011959638874137016, 'epoch': 0.4}\n",
      "{'loss': 0.9983, 'grad_norm': 0.26197028160095215, 'learning_rate': 0.00011956098424499913, 'epoch': 0.4}\n",
      "{'loss': 0.8432, 'grad_norm': 0.2925778031349182, 'learning_rate': 0.00011952557974862808, 'epoch': 0.4}\n",
      "{'loss': 0.578, 'grad_norm': 0.19293296337127686, 'learning_rate': 0.00011949017525225705, 'epoch': 0.4}\n",
      "{'loss': 1.0207, 'grad_norm': 0.27729305624961853, 'learning_rate': 0.000119454770755886, 'epoch': 0.4}\n",
      "{'loss': 0.5979, 'grad_norm': 0.21796488761901855, 'learning_rate': 0.00011941936625951495, 'epoch': 0.4}\n",
      "{'loss': 0.6559, 'grad_norm': 0.20833908021450043, 'learning_rate': 0.00011938396176314393, 'epoch': 0.4}\n",
      "{'loss': 0.5305, 'grad_norm': 0.1836126744747162, 'learning_rate': 0.00011934855726677287, 'epoch': 0.4}\n",
      "{'loss': 0.6322, 'grad_norm': 0.18091171979904175, 'learning_rate': 0.00011931315277040185, 'epoch': 0.4}\n",
      "{'loss': 0.6436, 'grad_norm': 0.19874325394630432, 'learning_rate': 0.0001192777482740308, 'epoch': 0.4}\n",
      "{'loss': 0.6415, 'grad_norm': 0.2722797691822052, 'learning_rate': 0.00011924234377765978, 'epoch': 0.4}\n",
      "{'loss': 0.8488, 'grad_norm': 0.3361353576183319, 'learning_rate': 0.00011920693928128872, 'epoch': 0.4}\n",
      "{'loss': 0.9484, 'grad_norm': 0.2551185190677643, 'learning_rate': 0.0001191715347849177, 'epoch': 0.4}\n",
      "{'loss': 0.8139, 'grad_norm': 0.23192797601222992, 'learning_rate': 0.00011913613028854664, 'epoch': 0.4}\n",
      "{'loss': 0.7905, 'grad_norm': 0.24754740297794342, 'learning_rate': 0.00011910072579217562, 'epoch': 0.41}\n",
      "{'loss': 1.1017, 'grad_norm': 0.2536485195159912, 'learning_rate': 0.00011906532129580456, 'epoch': 0.41}\n",
      "{'loss': 0.8041, 'grad_norm': 0.2164924293756485, 'learning_rate': 0.00011902991679943355, 'epoch': 0.41}\n",
      "{'loss': 0.6881, 'grad_norm': 0.18232516944408417, 'learning_rate': 0.00011899451230306249, 'epoch': 0.41}\n",
      "{'loss': 0.7184, 'grad_norm': 0.3413443863391876, 'learning_rate': 0.00011895910780669145, 'epoch': 0.41}\n",
      "{'loss': 0.8345, 'grad_norm': 0.18022221326828003, 'learning_rate': 0.00011892370331032041, 'epoch': 0.41}\n",
      "{'loss': 0.902, 'grad_norm': 0.24750781059265137, 'learning_rate': 0.00011888829881394938, 'epoch': 0.41}\n",
      "{'loss': 0.5957, 'grad_norm': 0.24250580370426178, 'learning_rate': 0.00011885289431757833, 'epoch': 0.41}\n",
      "{'loss': 0.7131, 'grad_norm': 0.21976213157176971, 'learning_rate': 0.0001188174898212073, 'epoch': 0.41}\n",
      "{'loss': 0.9029, 'grad_norm': 0.3613477051258087, 'learning_rate': 0.00011878208532483626, 'epoch': 0.41}\n",
      "{'loss': 0.8328, 'grad_norm': 0.2357664257287979, 'learning_rate': 0.00011874668082846522, 'epoch': 0.41}\n",
      "{'loss': 0.792, 'grad_norm': 0.20860125124454498, 'learning_rate': 0.00011871127633209418, 'epoch': 0.41}\n",
      "{'loss': 0.9186, 'grad_norm': 0.3307364881038666, 'learning_rate': 0.00011867587183572315, 'epoch': 0.41}\n",
      "{'loss': 0.6356, 'grad_norm': 0.19458739459514618, 'learning_rate': 0.0001186404673393521, 'epoch': 0.41}\n",
      "{'loss': 0.6348, 'grad_norm': 0.2562415897846222, 'learning_rate': 0.00011860506284298107, 'epoch': 0.41}\n",
      "{'loss': 0.6884, 'grad_norm': 0.204255148768425, 'learning_rate': 0.00011856965834661002, 'epoch': 0.41}\n",
      "{'loss': 0.7773, 'grad_norm': 0.21595704555511475, 'learning_rate': 0.000118534253850239, 'epoch': 0.41}\n",
      "{'loss': 0.7979, 'grad_norm': 0.2514026463031769, 'learning_rate': 0.00011849884935386795, 'epoch': 0.41}\n",
      "{'loss': 0.8314, 'grad_norm': 0.24655917286872864, 'learning_rate': 0.0001184634448574969, 'epoch': 0.41}\n",
      "{'loss': 0.6014, 'grad_norm': 0.19909027218818665, 'learning_rate': 0.00011842804036112587, 'epoch': 0.41}\n",
      "{'loss': 0.8696, 'grad_norm': 0.22773465514183044, 'learning_rate': 0.00011839263586475483, 'epoch': 0.41}\n",
      "{'loss': 0.8352, 'grad_norm': 0.29386401176452637, 'learning_rate': 0.0001183572313683838, 'epoch': 0.41}\n",
      "{'loss': 0.8182, 'grad_norm': 0.24390928447246552, 'learning_rate': 0.00011832182687201274, 'epoch': 0.41}\n",
      "{'loss': 0.9033, 'grad_norm': 0.2543615996837616, 'learning_rate': 0.00011828642237564172, 'epoch': 0.41}\n",
      "{'loss': 0.5744, 'grad_norm': 0.19469968974590302, 'learning_rate': 0.00011825101787927066, 'epoch': 0.41}\n",
      "{'loss': 0.8088, 'grad_norm': 0.21890956163406372, 'learning_rate': 0.00011821561338289964, 'epoch': 0.41}\n",
      "{'loss': 0.8211, 'grad_norm': 0.23855991661548615, 'learning_rate': 0.00011818020888652858, 'epoch': 0.41}\n",
      "{'loss': 0.6022, 'grad_norm': 0.17067694664001465, 'learning_rate': 0.00011814480439015756, 'epoch': 0.41}\n",
      "{'loss': 0.9737, 'grad_norm': 0.2698069214820862, 'learning_rate': 0.0001181093998937865, 'epoch': 0.41}\n",
      "{'loss': 1.0209, 'grad_norm': 0.30988141894340515, 'learning_rate': 0.00011807399539741549, 'epoch': 0.41}\n",
      "{'loss': 0.6078, 'grad_norm': 0.20329289138317108, 'learning_rate': 0.00011803859090104443, 'epoch': 0.41}\n",
      "{'loss': 0.5847, 'grad_norm': 0.20973847806453705, 'learning_rate': 0.00011800318640467341, 'epoch': 0.41}\n",
      "{'loss': 0.677, 'grad_norm': 0.23367759585380554, 'learning_rate': 0.00011796778190830235, 'epoch': 0.41}\n",
      "{'loss': 0.8046, 'grad_norm': 0.2771141529083252, 'learning_rate': 0.00011793237741193133, 'epoch': 0.41}\n",
      "{'loss': 0.6106, 'grad_norm': 0.2768732011318207, 'learning_rate': 0.00011789697291556027, 'epoch': 0.41}\n",
      "{'loss': 0.6447, 'grad_norm': 0.19642093777656555, 'learning_rate': 0.00011786156841918926, 'epoch': 0.41}\n",
      "{'loss': 0.8891, 'grad_norm': 0.30840039253234863, 'learning_rate': 0.0001178261639228182, 'epoch': 0.41}\n",
      "{'loss': 0.7922, 'grad_norm': 0.2727774977684021, 'learning_rate': 0.00011779075942644718, 'epoch': 0.41}\n",
      "{'loss': 0.741, 'grad_norm': 0.20473602414131165, 'learning_rate': 0.00011775535493007612, 'epoch': 0.41}\n",
      "{'loss': 0.4477, 'grad_norm': 0.20340774953365326, 'learning_rate': 0.00011771995043370509, 'epoch': 0.41}\n",
      "{'loss': 0.6841, 'grad_norm': 0.22379851341247559, 'learning_rate': 0.00011768454593733404, 'epoch': 0.41}\n",
      "{'loss': 0.7954, 'grad_norm': 0.22182630002498627, 'learning_rate': 0.00011764914144096301, 'epoch': 0.41}\n",
      "{'loss': 0.8785, 'grad_norm': 0.25114309787750244, 'learning_rate': 0.00011761373694459197, 'epoch': 0.41}\n",
      "{'loss': 0.5482, 'grad_norm': 0.17240574955940247, 'learning_rate': 0.00011757833244822093, 'epoch': 0.41}\n",
      "{'loss': 0.9346, 'grad_norm': 0.32086893916130066, 'learning_rate': 0.00011754292795184989, 'epoch': 0.41}\n",
      "{'loss': 0.5568, 'grad_norm': 0.21778011322021484, 'learning_rate': 0.00011750752345547884, 'epoch': 0.41}\n",
      "{'loss': 0.7632, 'grad_norm': 0.3085354268550873, 'learning_rate': 0.00011747211895910781, 'epoch': 0.41}\n",
      "{'loss': 0.8395, 'grad_norm': 0.21328775584697723, 'learning_rate': 0.00011743671446273677, 'epoch': 0.41}\n",
      "{'loss': 1.1671, 'grad_norm': 0.2137208878993988, 'learning_rate': 0.00011740130996636574, 'epoch': 0.41}\n",
      "{'loss': 0.5229, 'grad_norm': 0.21302957832813263, 'learning_rate': 0.00011736590546999469, 'epoch': 0.41}\n",
      "{'loss': 0.6507, 'grad_norm': 0.213919997215271, 'learning_rate': 0.00011733050097362366, 'epoch': 0.41}\n",
      "{'loss': 0.8772, 'grad_norm': 0.23246905207633972, 'learning_rate': 0.00011729509647725261, 'epoch': 0.41}\n",
      "{'loss': 0.5679, 'grad_norm': 0.21359869837760925, 'learning_rate': 0.00011725969198088158, 'epoch': 0.41}\n",
      "{'loss': 0.9068, 'grad_norm': 0.26202327013015747, 'learning_rate': 0.00011722428748451054, 'epoch': 0.41}\n",
      "{'loss': 0.7473, 'grad_norm': 0.24597667157649994, 'learning_rate': 0.0001171888829881395, 'epoch': 0.41}\n",
      "{'loss': 0.6687, 'grad_norm': 0.22696839272975922, 'learning_rate': 0.00011715347849176846, 'epoch': 0.41}\n",
      "{'loss': 0.4794, 'grad_norm': 0.2061561793088913, 'learning_rate': 0.00011711807399539743, 'epoch': 0.41}\n",
      "{'loss': 0.8237, 'grad_norm': 0.2332247942686081, 'learning_rate': 0.00011708266949902638, 'epoch': 0.42}\n",
      "{'loss': 0.6352, 'grad_norm': 0.2208830863237381, 'learning_rate': 0.00011704726500265535, 'epoch': 0.42}\n",
      "{'loss': 0.6255, 'grad_norm': 0.3144070506095886, 'learning_rate': 0.00011701186050628429, 'epoch': 0.42}\n",
      "{'loss': 0.6193, 'grad_norm': 0.2792925238609314, 'learning_rate': 0.00011697645600991327, 'epoch': 0.42}\n",
      "{'loss': 0.593, 'grad_norm': 0.22521817684173584, 'learning_rate': 0.00011694105151354222, 'epoch': 0.42}\n",
      "{'loss': 0.7381, 'grad_norm': 0.20623384416103363, 'learning_rate': 0.0001169056470171712, 'epoch': 0.42}\n",
      "{'loss': 1.1181, 'grad_norm': 0.3229049742221832, 'learning_rate': 0.00011687024252080014, 'epoch': 0.42}\n",
      "{'loss': 0.6748, 'grad_norm': 0.2087118923664093, 'learning_rate': 0.00011683483802442912, 'epoch': 0.42}\n",
      "{'loss': 0.6423, 'grad_norm': 0.20959413051605225, 'learning_rate': 0.00011679943352805806, 'epoch': 0.42}\n",
      "{'loss': 0.4682, 'grad_norm': 0.1982329785823822, 'learning_rate': 0.00011676402903168704, 'epoch': 0.42}\n",
      "{'loss': 0.7885, 'grad_norm': 0.2435932755470276, 'learning_rate': 0.00011672862453531598, 'epoch': 0.42}\n",
      "{'loss': 0.9168, 'grad_norm': 0.2727450430393219, 'learning_rate': 0.00011669322003894497, 'epoch': 0.42}\n",
      "{'loss': 0.7393, 'grad_norm': 0.23215630650520325, 'learning_rate': 0.00011665781554257391, 'epoch': 0.42}\n",
      "{'loss': 0.5369, 'grad_norm': 0.18628083169460297, 'learning_rate': 0.00011662241104620289, 'epoch': 0.42}\n",
      "{'loss': 0.7711, 'grad_norm': 0.2563557028770447, 'learning_rate': 0.00011658700654983183, 'epoch': 0.42}\n",
      "{'loss': 0.603, 'grad_norm': 0.17120321094989777, 'learning_rate': 0.00011655160205346079, 'epoch': 0.42}\n",
      "{'loss': 0.5866, 'grad_norm': 0.23026932775974274, 'learning_rate': 0.00011651619755708975, 'epoch': 0.42}\n",
      "{'loss': 0.7731, 'grad_norm': 0.23709343373775482, 'learning_rate': 0.00011648079306071871, 'epoch': 0.42}\n",
      "{'loss': 0.8039, 'grad_norm': 0.22093120217323303, 'learning_rate': 0.00011644538856434768, 'epoch': 0.42}\n",
      "{'loss': 1.0454, 'grad_norm': 0.33978843688964844, 'learning_rate': 0.00011640998406797663, 'epoch': 0.42}\n",
      "{'loss': 0.5925, 'grad_norm': 0.24731102585792542, 'learning_rate': 0.0001163745795716056, 'epoch': 0.42}\n",
      "{'loss': 0.7192, 'grad_norm': 0.25725772976875305, 'learning_rate': 0.00011633917507523455, 'epoch': 0.42}\n",
      "{'loss': 1.114, 'grad_norm': 0.3466876447200775, 'learning_rate': 0.00011630377057886352, 'epoch': 0.42}\n",
      "{'loss': 0.8758, 'grad_norm': 0.26314449310302734, 'learning_rate': 0.00011626836608249248, 'epoch': 0.42}\n",
      "{'loss': 0.72, 'grad_norm': 0.2032061219215393, 'learning_rate': 0.00011623296158612145, 'epoch': 0.42}\n",
      "{'loss': 0.6808, 'grad_norm': 0.20586442947387695, 'learning_rate': 0.0001161975570897504, 'epoch': 0.42}\n",
      "{'loss': 0.8522, 'grad_norm': 0.26975277066230774, 'learning_rate': 0.00011616215259337937, 'epoch': 0.42}\n",
      "{'loss': 0.5465, 'grad_norm': 0.17199990153312683, 'learning_rate': 0.00011612674809700832, 'epoch': 0.42}\n",
      "{'loss': 0.5917, 'grad_norm': 0.2422630488872528, 'learning_rate': 0.00011609134360063729, 'epoch': 0.42}\n",
      "{'loss': 0.7411, 'grad_norm': 0.2530765235424042, 'learning_rate': 0.00011605593910426625, 'epoch': 0.42}\n",
      "{'loss': 0.7786, 'grad_norm': 0.29538601636886597, 'learning_rate': 0.00011602053460789522, 'epoch': 0.42}\n",
      "{'loss': 0.7697, 'grad_norm': 0.20376838743686676, 'learning_rate': 0.00011598513011152417, 'epoch': 0.42}\n",
      "{'loss': 0.8839, 'grad_norm': 0.25869396328926086, 'learning_rate': 0.00011594972561515314, 'epoch': 0.42}\n",
      "{'loss': 0.8669, 'grad_norm': 0.23819822072982788, 'learning_rate': 0.00011591432111878209, 'epoch': 0.42}\n",
      "{'loss': 0.7254, 'grad_norm': 0.3279576897621155, 'learning_rate': 0.00011587891662241106, 'epoch': 0.42}\n",
      "{'loss': 0.9977, 'grad_norm': 0.31466299295425415, 'learning_rate': 0.00011584351212604002, 'epoch': 0.42}\n",
      "{'loss': 0.7464, 'grad_norm': 0.24026139080524445, 'learning_rate': 0.00011580810762966898, 'epoch': 0.42}\n",
      "{'loss': 0.9691, 'grad_norm': 0.3186871409416199, 'learning_rate': 0.00011577270313329794, 'epoch': 0.42}\n",
      "{'loss': 0.8795, 'grad_norm': 0.1996990442276001, 'learning_rate': 0.00011573729863692691, 'epoch': 0.42}\n",
      "{'loss': 0.7898, 'grad_norm': 0.21918180584907532, 'learning_rate': 0.00011570189414055585, 'epoch': 0.42}\n",
      "{'loss': 0.8807, 'grad_norm': 0.2121478021144867, 'learning_rate': 0.00011566648964418483, 'epoch': 0.42}\n",
      "{'loss': 1.0406, 'grad_norm': 0.2339811474084854, 'learning_rate': 0.00011563108514781377, 'epoch': 0.42}\n",
      "{'loss': 0.4681, 'grad_norm': 0.2341102957725525, 'learning_rate': 0.00011559568065144273, 'epoch': 0.42}\n",
      "{'loss': 0.6906, 'grad_norm': 0.1991090476512909, 'learning_rate': 0.0001155602761550717, 'epoch': 0.42}\n",
      "{'loss': 0.5809, 'grad_norm': 0.21958962082862854, 'learning_rate': 0.00011552487165870065, 'epoch': 0.42}\n",
      "{'loss': 0.8171, 'grad_norm': 0.2226216048002243, 'learning_rate': 0.00011548946716232962, 'epoch': 0.42}\n",
      "{'loss': 0.8218, 'grad_norm': 0.2257787436246872, 'learning_rate': 0.00011545406266595857, 'epoch': 0.42}\n",
      "{'loss': 0.8835, 'grad_norm': 0.2756980359554291, 'learning_rate': 0.00011541865816958754, 'epoch': 0.42}\n",
      "{'loss': 0.6339, 'grad_norm': 0.20087073743343353, 'learning_rate': 0.0001153832536732165, 'epoch': 0.42}\n",
      "{'loss': 0.6458, 'grad_norm': 0.24985474348068237, 'learning_rate': 0.00011534784917684546, 'epoch': 0.42}\n",
      "{'loss': 0.6437, 'grad_norm': 0.18109536170959473, 'learning_rate': 0.00011531244468047442, 'epoch': 0.42}\n",
      "{'loss': 0.5484, 'grad_norm': 0.22314780950546265, 'learning_rate': 0.00011527704018410339, 'epoch': 0.42}\n",
      "{'loss': 0.6879, 'grad_norm': 0.29068461060523987, 'learning_rate': 0.00011524163568773234, 'epoch': 0.42}\n",
      "{'loss': 0.7914, 'grad_norm': 0.2939501404762268, 'learning_rate': 0.00011520623119136131, 'epoch': 0.42}\n",
      "{'loss': 0.6149, 'grad_norm': 0.26191794872283936, 'learning_rate': 0.00011517082669499027, 'epoch': 0.42}\n",
      "{'loss': 0.522, 'grad_norm': 0.16813607513904572, 'learning_rate': 0.00011513542219861923, 'epoch': 0.42}\n",
      "{'loss': 0.6355, 'grad_norm': 0.23498143255710602, 'learning_rate': 0.00011510001770224819, 'epoch': 0.43}\n",
      "{'loss': 0.7533, 'grad_norm': 0.2522260546684265, 'learning_rate': 0.00011506461320587716, 'epoch': 0.43}\n",
      "{'loss': 0.6923, 'grad_norm': 0.20673339068889618, 'learning_rate': 0.00011502920870950611, 'epoch': 0.43}\n",
      "{'loss': 0.9674, 'grad_norm': 0.26613301038742065, 'learning_rate': 0.00011499380421313508, 'epoch': 0.43}\n",
      "{'loss': 0.8442, 'grad_norm': 0.21281364560127258, 'learning_rate': 0.00011495839971676403, 'epoch': 0.43}\n",
      "{'loss': 0.7794, 'grad_norm': 0.26011744141578674, 'learning_rate': 0.000114922995220393, 'epoch': 0.43}\n",
      "{'loss': 0.6091, 'grad_norm': 0.2184644639492035, 'learning_rate': 0.00011488759072402196, 'epoch': 0.43}\n",
      "{'loss': 0.7833, 'grad_norm': 0.29533880949020386, 'learning_rate': 0.00011485218622765093, 'epoch': 0.43}\n",
      "{'loss': 0.7108, 'grad_norm': 0.20653128623962402, 'learning_rate': 0.00011481678173127988, 'epoch': 0.43}\n",
      "{'loss': 0.7967, 'grad_norm': 0.19820529222488403, 'learning_rate': 0.00011478137723490885, 'epoch': 0.43}\n",
      "{'loss': 0.7315, 'grad_norm': 0.24236564338207245, 'learning_rate': 0.0001147459727385378, 'epoch': 0.43}\n",
      "{'loss': 1.3492, 'grad_norm': 0.2974976599216461, 'learning_rate': 0.00011471056824216677, 'epoch': 0.43}\n",
      "{'loss': 0.6594, 'grad_norm': 0.24079236388206482, 'learning_rate': 0.00011467516374579573, 'epoch': 0.43}\n",
      "{'loss': 0.7074, 'grad_norm': 0.19986815750598907, 'learning_rate': 0.00011463975924942467, 'epoch': 0.43}\n",
      "{'loss': 0.6924, 'grad_norm': 0.22715029120445251, 'learning_rate': 0.00011460435475305365, 'epoch': 0.43}\n",
      "{'loss': 0.5131, 'grad_norm': 0.2252848744392395, 'learning_rate': 0.00011456895025668259, 'epoch': 0.43}\n",
      "{'loss': 0.7312, 'grad_norm': 0.20701256394386292, 'learning_rate': 0.00011453354576031157, 'epoch': 0.43}\n",
      "{'loss': 0.6756, 'grad_norm': 0.2299276739358902, 'learning_rate': 0.00011449814126394051, 'epoch': 0.43}\n",
      "{'loss': 0.7968, 'grad_norm': 0.20827557146549225, 'learning_rate': 0.0001144627367675695, 'epoch': 0.43}\n",
      "{'loss': 1.1466, 'grad_norm': 0.2819405496120453, 'learning_rate': 0.00011442733227119844, 'epoch': 0.43}\n",
      "{'loss': 0.7145, 'grad_norm': 0.23361079394817352, 'learning_rate': 0.0001143919277748274, 'epoch': 0.43}\n",
      "{'loss': 0.7699, 'grad_norm': 0.23412372171878815, 'learning_rate': 0.00011435652327845636, 'epoch': 0.43}\n",
      "{'loss': 0.5464, 'grad_norm': 0.22611436247825623, 'learning_rate': 0.00011432111878208533, 'epoch': 0.43}\n",
      "{'loss': 0.8823, 'grad_norm': 0.24787673354148865, 'learning_rate': 0.00011428571428571428, 'epoch': 0.43}\n",
      "{'loss': 0.7382, 'grad_norm': 0.22790645062923431, 'learning_rate': 0.00011425030978934325, 'epoch': 0.43}\n",
      "{'loss': 0.7916, 'grad_norm': 0.19716788828372955, 'learning_rate': 0.0001142149052929722, 'epoch': 0.43}\n",
      "{'loss': 0.6858, 'grad_norm': 0.21259132027626038, 'learning_rate': 0.00011417950079660117, 'epoch': 0.43}\n",
      "{'loss': 0.6336, 'grad_norm': 0.2611643970012665, 'learning_rate': 0.00011414409630023013, 'epoch': 0.43}\n",
      "{'loss': 0.5845, 'grad_norm': 0.19599415361881256, 'learning_rate': 0.0001141086918038591, 'epoch': 0.43}\n",
      "{'loss': 0.9978, 'grad_norm': 0.24981902539730072, 'learning_rate': 0.00011407328730748805, 'epoch': 0.43}\n",
      "{'loss': 0.5653, 'grad_norm': 0.22384320199489594, 'learning_rate': 0.00011403788281111702, 'epoch': 0.43}\n",
      "{'loss': 0.6503, 'grad_norm': 0.18990737199783325, 'learning_rate': 0.00011400247831474598, 'epoch': 0.43}\n",
      "{'loss': 0.6372, 'grad_norm': 0.20486268401145935, 'learning_rate': 0.00011396707381837494, 'epoch': 0.43}\n",
      "{'loss': 0.6916, 'grad_norm': 0.25783365964889526, 'learning_rate': 0.0001139316693220039, 'epoch': 0.43}\n",
      "{'loss': 0.5986, 'grad_norm': 0.18699544668197632, 'learning_rate': 0.00011389626482563287, 'epoch': 0.43}\n",
      "{'loss': 0.9086, 'grad_norm': 0.22305436432361603, 'learning_rate': 0.00011386086032926182, 'epoch': 0.43}\n",
      "{'loss': 0.8287, 'grad_norm': 0.26792213320732117, 'learning_rate': 0.00011382545583289079, 'epoch': 0.43}\n",
      "{'loss': 0.8778, 'grad_norm': 0.2641857862472534, 'learning_rate': 0.00011379005133651975, 'epoch': 0.43}\n",
      "{'loss': 0.6976, 'grad_norm': 0.19331927597522736, 'learning_rate': 0.00011375464684014871, 'epoch': 0.43}\n",
      "{'loss': 0.8978, 'grad_norm': 0.2707369029521942, 'learning_rate': 0.00011371924234377767, 'epoch': 0.43}\n",
      "{'loss': 0.8777, 'grad_norm': 0.2658982276916504, 'learning_rate': 0.00011368383784740661, 'epoch': 0.43}\n",
      "{'loss': 0.8883, 'grad_norm': 0.2043152153491974, 'learning_rate': 0.00011364843335103559, 'epoch': 0.43}\n",
      "{'loss': 0.9606, 'grad_norm': 0.2641889452934265, 'learning_rate': 0.00011361302885466453, 'epoch': 0.43}\n",
      "{'loss': 0.6289, 'grad_norm': 0.2248932421207428, 'learning_rate': 0.00011357762435829351, 'epoch': 0.43}\n",
      "{'loss': 1.0341, 'grad_norm': 0.2465185970067978, 'learning_rate': 0.00011354221986192246, 'epoch': 0.43}\n",
      "{'loss': 0.855, 'grad_norm': 0.2795741856098175, 'learning_rate': 0.00011350681536555144, 'epoch': 0.43}\n",
      "{'loss': 0.5406, 'grad_norm': 0.23074057698249817, 'learning_rate': 0.00011347141086918038, 'epoch': 0.43}\n",
      "{'loss': 0.6223, 'grad_norm': 0.2052978277206421, 'learning_rate': 0.00011343600637280936, 'epoch': 0.43}\n",
      "{'loss': 0.6157, 'grad_norm': 0.21430604159832, 'learning_rate': 0.0001134006018764383, 'epoch': 0.43}\n",
      "{'loss': 0.5593, 'grad_norm': 0.21694540977478027, 'learning_rate': 0.00011336519738006728, 'epoch': 0.43}\n",
      "{'loss': 0.5196, 'grad_norm': 0.19983479380607605, 'learning_rate': 0.00011332979288369622, 'epoch': 0.43}\n",
      "{'loss': 0.8833, 'grad_norm': 0.2679749131202698, 'learning_rate': 0.0001132943883873252, 'epoch': 0.43}\n",
      "{'loss': 0.8665, 'grad_norm': 0.25974783301353455, 'learning_rate': 0.00011325898389095415, 'epoch': 0.43}\n",
      "{'loss': 0.541, 'grad_norm': 0.21520617604255676, 'learning_rate': 0.00011322357939458313, 'epoch': 0.43}\n",
      "{'loss': 1.0283, 'grad_norm': 0.21025300025939941, 'learning_rate': 0.00011318817489821207, 'epoch': 0.43}\n",
      "{'loss': 0.6651, 'grad_norm': 0.1802154779434204, 'learning_rate': 0.00011315277040184105, 'epoch': 0.43}\n",
      "{'loss': 0.9104, 'grad_norm': 0.2969909608364105, 'learning_rate': 0.00011311736590547, 'epoch': 0.43}\n",
      "{'loss': 0.8955, 'grad_norm': 0.3261174261569977, 'learning_rate': 0.00011308196140909896, 'epoch': 0.44}\n",
      "{'loss': 0.9063, 'grad_norm': 0.20240168273448944, 'learning_rate': 0.00011304655691272792, 'epoch': 0.44}\n",
      "{'loss': 0.8202, 'grad_norm': 0.2110130488872528, 'learning_rate': 0.00011301115241635689, 'epoch': 0.44}\n",
      "{'loss': 0.7212, 'grad_norm': 0.23100882768630981, 'learning_rate': 0.00011297574791998584, 'epoch': 0.44}\n",
      "{'loss': 0.861, 'grad_norm': 0.27033287286758423, 'learning_rate': 0.00011294034342361481, 'epoch': 0.44}\n",
      "{'loss': 0.5908, 'grad_norm': 0.2061244696378708, 'learning_rate': 0.00011290493892724376, 'epoch': 0.44}\n",
      "{'loss': 0.4987, 'grad_norm': 0.18465182185173035, 'learning_rate': 0.00011286953443087273, 'epoch': 0.44}\n",
      "{'loss': 0.5789, 'grad_norm': 0.19337448477745056, 'learning_rate': 0.00011283412993450169, 'epoch': 0.44}\n",
      "{'loss': 0.8721, 'grad_norm': 0.25253215432167053, 'learning_rate': 0.00011279872543813065, 'epoch': 0.44}\n",
      "{'loss': 0.7589, 'grad_norm': 0.24507443606853485, 'learning_rate': 0.00011276332094175961, 'epoch': 0.44}\n",
      "{'loss': 0.8639, 'grad_norm': 0.2511126399040222, 'learning_rate': 0.00011272791644538856, 'epoch': 0.44}\n",
      "{'loss': 0.692, 'grad_norm': 0.23581187427043915, 'learning_rate': 0.00011269251194901753, 'epoch': 0.44}\n",
      "{'loss': 0.6046, 'grad_norm': 0.18299928307533264, 'learning_rate': 0.00011265710745264649, 'epoch': 0.44}\n",
      "{'loss': 0.9122, 'grad_norm': 0.25257033109664917, 'learning_rate': 0.00011262170295627546, 'epoch': 0.44}\n",
      "{'loss': 0.9564, 'grad_norm': 0.25910982489585876, 'learning_rate': 0.00011258629845990441, 'epoch': 0.44}\n",
      "{'loss': 0.616, 'grad_norm': 0.24935229122638702, 'learning_rate': 0.00011255089396353338, 'epoch': 0.44}\n",
      "{'loss': 0.9043, 'grad_norm': 0.25987282395362854, 'learning_rate': 0.00011251548946716233, 'epoch': 0.44}\n",
      "{'loss': 0.6686, 'grad_norm': 0.2534603476524353, 'learning_rate': 0.0001124800849707913, 'epoch': 0.44}\n",
      "{'loss': 0.578, 'grad_norm': 0.19508136808872223, 'learning_rate': 0.00011244468047442026, 'epoch': 0.44}\n",
      "{'loss': 0.6949, 'grad_norm': 0.19537357985973358, 'learning_rate': 0.00011240927597804922, 'epoch': 0.44}\n",
      "{'loss': 0.5842, 'grad_norm': 0.29961642622947693, 'learning_rate': 0.00011237387148167817, 'epoch': 0.44}\n",
      "{'loss': 1.1428, 'grad_norm': 0.22705741226673126, 'learning_rate': 0.00011233846698530715, 'epoch': 0.44}\n",
      "{'loss': 0.8653, 'grad_norm': 0.28938934206962585, 'learning_rate': 0.00011230306248893609, 'epoch': 0.44}\n",
      "{'loss': 0.5287, 'grad_norm': 0.23869900405406952, 'learning_rate': 0.00011226765799256507, 'epoch': 0.44}\n",
      "{'loss': 0.6326, 'grad_norm': 0.2153562754392624, 'learning_rate': 0.00011223225349619401, 'epoch': 0.44}\n",
      "{'loss': 0.5759, 'grad_norm': 0.19831202924251556, 'learning_rate': 0.000112196848999823, 'epoch': 0.44}\n",
      "{'loss': 0.7798, 'grad_norm': 0.24582673609256744, 'learning_rate': 0.00011216144450345194, 'epoch': 0.44}\n",
      "{'loss': 0.9198, 'grad_norm': 0.2953689694404602, 'learning_rate': 0.00011212604000708092, 'epoch': 0.44}\n",
      "{'loss': 0.8717, 'grad_norm': 0.23932480812072754, 'learning_rate': 0.00011209063551070986, 'epoch': 0.44}\n",
      "{'loss': 0.5657, 'grad_norm': 0.24971652030944824, 'learning_rate': 0.00011205523101433884, 'epoch': 0.44}\n",
      "{'loss': 0.7588, 'grad_norm': 0.24112007021903992, 'learning_rate': 0.00011201982651796778, 'epoch': 0.44}\n",
      "{'loss': 0.733, 'grad_norm': 0.21039628982543945, 'learning_rate': 0.00011198442202159676, 'epoch': 0.44}\n",
      "{'loss': 0.6624, 'grad_norm': 0.23609741032123566, 'learning_rate': 0.0001119490175252257, 'epoch': 0.44}\n",
      "{'loss': 0.4138, 'grad_norm': 0.1812484711408615, 'learning_rate': 0.00011191361302885469, 'epoch': 0.44}\n",
      "{'loss': 0.8271, 'grad_norm': 0.49473291635513306, 'learning_rate': 0.00011187820853248363, 'epoch': 0.44}\n",
      "{'loss': 0.6713, 'grad_norm': 0.23218297958374023, 'learning_rate': 0.00011184280403611258, 'epoch': 0.44}\n",
      "{'loss': 0.5647, 'grad_norm': 0.22383971512317657, 'learning_rate': 0.00011180739953974155, 'epoch': 0.44}\n",
      "{'loss': 1.1006, 'grad_norm': 0.2335977703332901, 'learning_rate': 0.0001117719950433705, 'epoch': 0.44}\n",
      "{'loss': 0.9546, 'grad_norm': 0.2415323555469513, 'learning_rate': 0.00011173659054699947, 'epoch': 0.44}\n",
      "{'loss': 0.5704, 'grad_norm': 0.2393629550933838, 'learning_rate': 0.00011170118605062843, 'epoch': 0.44}\n",
      "{'loss': 0.6515, 'grad_norm': 0.24686723947525024, 'learning_rate': 0.0001116657815542574, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee376355d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 526bddb2-078c-471e-b22e-fbc2d5628297)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6804, 'grad_norm': 0.2083280384540558, 'learning_rate': 0.00011163037705788635, 'epoch': 0.44}\n",
      "{'loss': 0.562, 'grad_norm': 0.22783422470092773, 'learning_rate': 0.00011159497256151532, 'epoch': 0.44}\n",
      "{'loss': 0.6954, 'grad_norm': 0.24710874259471893, 'learning_rate': 0.00011155956806514427, 'epoch': 0.44}\n",
      "{'loss': 0.6193, 'grad_norm': 0.18318966031074524, 'learning_rate': 0.00011152416356877324, 'epoch': 0.44}\n",
      "{'loss': 0.8396, 'grad_norm': 0.35866793990135193, 'learning_rate': 0.0001114887590724022, 'epoch': 0.44}\n",
      "{'loss': 0.725, 'grad_norm': 0.21781031787395477, 'learning_rate': 0.00011145335457603117, 'epoch': 0.44}\n",
      "{'loss': 0.681, 'grad_norm': 0.20789621770381927, 'learning_rate': 0.00011141795007966012, 'epoch': 0.44}\n",
      "{'loss': 0.7947, 'grad_norm': 0.23209047317504883, 'learning_rate': 0.00011138254558328909, 'epoch': 0.44}\n",
      "{'loss': 0.7888, 'grad_norm': 0.24139094352722168, 'learning_rate': 0.00011134714108691804, 'epoch': 0.44}\n",
      "{'loss': 0.7503, 'grad_norm': 0.2531319260597229, 'learning_rate': 0.00011131173659054701, 'epoch': 0.44}\n",
      "{'loss': 0.6253, 'grad_norm': 0.2459634691476822, 'learning_rate': 0.00011127633209417597, 'epoch': 0.44}\n",
      "{'loss': 0.7719, 'grad_norm': 0.29286736249923706, 'learning_rate': 0.00011124092759780494, 'epoch': 0.44}\n",
      "{'loss': 0.6571, 'grad_norm': 0.25944066047668457, 'learning_rate': 0.00011120552310143389, 'epoch': 0.44}\n",
      "{'loss': 0.8361, 'grad_norm': 0.19777700304985046, 'learning_rate': 0.00011117011860506286, 'epoch': 0.44}\n",
      "{'loss': 0.6408, 'grad_norm': 0.22957704961299896, 'learning_rate': 0.0001111347141086918, 'epoch': 0.44}\n",
      "{'loss': 0.7264, 'grad_norm': 0.21505817770957947, 'learning_rate': 0.00011109930961232078, 'epoch': 0.44}\n",
      "{'loss': 1.042, 'grad_norm': 0.2196384221315384, 'learning_rate': 0.00011106390511594972, 'epoch': 0.45}\n",
      "{'loss': 0.7265, 'grad_norm': 0.2095492035150528, 'learning_rate': 0.0001110285006195787, 'epoch': 0.45}\n",
      "{'loss': 0.801, 'grad_norm': 0.21126598119735718, 'learning_rate': 0.00011099309612320765, 'epoch': 0.45}\n",
      "{'loss': 0.9997, 'grad_norm': 0.2612585127353668, 'learning_rate': 0.00011095769162683663, 'epoch': 0.45}\n",
      "{'loss': 0.6606, 'grad_norm': 0.22786860167980194, 'learning_rate': 0.00011092228713046557, 'epoch': 0.45}\n",
      "{'loss': 0.8081, 'grad_norm': 0.22269019484519958, 'learning_rate': 0.00011088688263409452, 'epoch': 0.45}\n",
      "{'loss': 0.8243, 'grad_norm': 0.2298058271408081, 'learning_rate': 0.00011085147813772349, 'epoch': 0.45}\n",
      "{'loss': 0.7376, 'grad_norm': 0.19579175114631653, 'learning_rate': 0.00011081607364135245, 'epoch': 0.45}\n",
      "{'loss': 0.5675, 'grad_norm': 0.21788859367370605, 'learning_rate': 0.00011078066914498142, 'epoch': 0.45}\n",
      "{'loss': 0.7578, 'grad_norm': 0.2484145164489746, 'learning_rate': 0.00011074526464861037, 'epoch': 0.45}\n",
      "{'loss': 0.8902, 'grad_norm': 0.2653680443763733, 'learning_rate': 0.00011070986015223934, 'epoch': 0.45}\n",
      "{'loss': 0.6007, 'grad_norm': 0.25072750449180603, 'learning_rate': 0.00011067445565586829, 'epoch': 0.45}\n",
      "{'loss': 0.6743, 'grad_norm': 0.29211175441741943, 'learning_rate': 0.00011063905115949726, 'epoch': 0.45}\n",
      "{'loss': 0.7573, 'grad_norm': 0.2427402138710022, 'learning_rate': 0.00011060364666312622, 'epoch': 0.45}\n",
      "{'loss': 0.7078, 'grad_norm': 0.2055252343416214, 'learning_rate': 0.00011056824216675518, 'epoch': 0.45}\n",
      "{'loss': 0.6972, 'grad_norm': 0.20146459341049194, 'learning_rate': 0.00011053283767038414, 'epoch': 0.45}\n",
      "{'loss': 1.0078, 'grad_norm': 0.2982630729675293, 'learning_rate': 0.00011049743317401311, 'epoch': 0.45}\n",
      "{'loss': 0.7683, 'grad_norm': 0.22319072484970093, 'learning_rate': 0.00011046202867764206, 'epoch': 0.45}\n",
      "{'loss': 0.6688, 'grad_norm': 0.21524320542812347, 'learning_rate': 0.00011042662418127103, 'epoch': 0.45}\n",
      "{'loss': 0.7779, 'grad_norm': 0.27276235818862915, 'learning_rate': 0.00011039121968489999, 'epoch': 0.45}\n",
      "{'loss': 0.5852, 'grad_norm': 0.17281505465507507, 'learning_rate': 0.00011035581518852895, 'epoch': 0.45}\n",
      "{'loss': 0.9813, 'grad_norm': 0.18707996606826782, 'learning_rate': 0.00011032041069215791, 'epoch': 0.45}\n",
      "{'loss': 0.6945, 'grad_norm': 0.19509316980838776, 'learning_rate': 0.00011028500619578688, 'epoch': 0.45}\n",
      "{'loss': 0.7809, 'grad_norm': 0.18457308411598206, 'learning_rate': 0.00011024960169941583, 'epoch': 0.45}\n",
      "{'loss': 0.6993, 'grad_norm': 0.24532824754714966, 'learning_rate': 0.0001102141972030448, 'epoch': 0.45}\n",
      "{'loss': 0.6529, 'grad_norm': 0.18114887177944183, 'learning_rate': 0.00011017879270667375, 'epoch': 0.45}\n",
      "{'loss': 0.7518, 'grad_norm': 0.22328220307826996, 'learning_rate': 0.00011014338821030272, 'epoch': 0.45}\n",
      "{'loss': 0.7413, 'grad_norm': 0.2565694749355316, 'learning_rate': 0.00011010798371393168, 'epoch': 0.45}\n",
      "{'loss': 0.8863, 'grad_norm': 0.22166821360588074, 'learning_rate': 0.00011007257921756065, 'epoch': 0.45}\n",
      "{'loss': 0.7201, 'grad_norm': 0.21091660857200623, 'learning_rate': 0.0001100371747211896, 'epoch': 0.45}\n",
      "{'loss': 0.5833, 'grad_norm': 0.20511195063591003, 'learning_rate': 0.00011000177022481857, 'epoch': 0.45}\n",
      "{'loss': 0.6864, 'grad_norm': 0.21791404485702515, 'learning_rate': 0.00010996636572844752, 'epoch': 0.45}\n",
      "{'loss': 0.6083, 'grad_norm': 0.1939045488834381, 'learning_rate': 0.00010993096123207647, 'epoch': 0.45}\n",
      "{'loss': 0.8393, 'grad_norm': 0.3122435510158539, 'learning_rate': 0.00010989555673570545, 'epoch': 0.45}\n",
      "{'loss': 0.6695, 'grad_norm': 0.20832981169223785, 'learning_rate': 0.00010986015223933439, 'epoch': 0.45}\n",
      "{'loss': 0.5435, 'grad_norm': 0.2367010861635208, 'learning_rate': 0.00010982474774296336, 'epoch': 0.45}\n",
      "{'loss': 0.81, 'grad_norm': 0.21532250940799713, 'learning_rate': 0.00010978934324659231, 'epoch': 0.45}\n",
      "{'loss': 0.5612, 'grad_norm': 0.20712153613567352, 'learning_rate': 0.00010975393875022128, 'epoch': 0.45}\n",
      "{'loss': 0.8227, 'grad_norm': 0.18037350475788116, 'learning_rate': 0.00010971853425385023, 'epoch': 0.45}\n",
      "{'loss': 0.5631, 'grad_norm': 0.22321075201034546, 'learning_rate': 0.0001096831297574792, 'epoch': 0.45}\n",
      "{'loss': 0.5658, 'grad_norm': 0.21677681803703308, 'learning_rate': 0.00010964772526110816, 'epoch': 0.45}\n",
      "{'loss': 0.5661, 'grad_norm': 0.19573527574539185, 'learning_rate': 0.00010961232076473713, 'epoch': 0.45}\n",
      "{'loss': 0.9633, 'grad_norm': 0.22225628793239594, 'learning_rate': 0.00010957691626836608, 'epoch': 0.45}\n",
      "{'loss': 0.5394, 'grad_norm': 0.21444737911224365, 'learning_rate': 0.00010954151177199505, 'epoch': 0.45}\n",
      "{'loss': 0.8463, 'grad_norm': 0.254634827375412, 'learning_rate': 0.000109506107275624, 'epoch': 0.45}\n",
      "{'loss': 0.8693, 'grad_norm': 0.2517090141773224, 'learning_rate': 0.00010947070277925297, 'epoch': 0.45}\n",
      "{'loss': 0.4893, 'grad_norm': 0.21757806837558746, 'learning_rate': 0.00010943529828288193, 'epoch': 0.45}\n",
      "{'loss': 0.7857, 'grad_norm': 0.19574373960494995, 'learning_rate': 0.0001093998937865109, 'epoch': 0.45}\n",
      "{'loss': 0.9809, 'grad_norm': 0.25380975008010864, 'learning_rate': 0.00010936448929013985, 'epoch': 0.45}\n",
      "{'loss': 0.7316, 'grad_norm': 0.22178025543689728, 'learning_rate': 0.00010932908479376882, 'epoch': 0.45}\n",
      "{'loss': 0.8131, 'grad_norm': 0.25103047490119934, 'learning_rate': 0.00010929368029739777, 'epoch': 0.45}\n",
      "{'loss': 0.5769, 'grad_norm': 0.18740606307983398, 'learning_rate': 0.00010925827580102674, 'epoch': 0.45}\n",
      "{'loss': 0.6368, 'grad_norm': 0.1946379840373993, 'learning_rate': 0.0001092228713046557, 'epoch': 0.45}\n",
      "{'loss': 0.8959, 'grad_norm': 0.24334469437599182, 'learning_rate': 0.00010918746680828466, 'epoch': 0.45}\n",
      "{'loss': 0.652, 'grad_norm': 0.23588994145393372, 'learning_rate': 0.00010915206231191362, 'epoch': 0.45}\n",
      "{'loss': 0.8609, 'grad_norm': 0.22855304181575775, 'learning_rate': 0.00010911665781554259, 'epoch': 0.45}\n",
      "{'loss': 0.7252, 'grad_norm': 0.41402700543403625, 'learning_rate': 0.00010908125331917154, 'epoch': 0.46}\n",
      "{'loss': 0.8222, 'grad_norm': 0.1797686219215393, 'learning_rate': 0.00010904584882280051, 'epoch': 0.46}\n",
      "{'loss': 0.4912, 'grad_norm': 0.20025786757469177, 'learning_rate': 0.00010901044432642947, 'epoch': 0.46}\n",
      "{'loss': 0.7608, 'grad_norm': 0.20992760360240936, 'learning_rate': 0.0001089750398300584, 'epoch': 0.46}\n",
      "{'loss': 0.624, 'grad_norm': 0.28170767426490784, 'learning_rate': 0.00010893963533368739, 'epoch': 0.46}\n",
      "{'loss': 0.8741, 'grad_norm': 0.2673763632774353, 'learning_rate': 0.00010890423083731633, 'epoch': 0.46}\n",
      "{'loss': 0.9865, 'grad_norm': 0.3191360533237457, 'learning_rate': 0.00010886882634094531, 'epoch': 0.46}\n",
      "{'loss': 0.5528, 'grad_norm': 0.2714541554450989, 'learning_rate': 0.00010883342184457425, 'epoch': 0.46}\n",
      "{'loss': 0.7068, 'grad_norm': 0.2243098020553589, 'learning_rate': 0.00010879801734820323, 'epoch': 0.46}\n",
      "{'loss': 0.6871, 'grad_norm': 0.20345072448253632, 'learning_rate': 0.00010876261285183218, 'epoch': 0.46}\n",
      "{'loss': 0.6148, 'grad_norm': 0.19200052320957184, 'learning_rate': 0.00010872720835546116, 'epoch': 0.46}\n",
      "{'loss': 0.6766, 'grad_norm': 0.1700364351272583, 'learning_rate': 0.0001086918038590901, 'epoch': 0.46}\n",
      "{'loss': 0.7185, 'grad_norm': 0.24563735723495483, 'learning_rate': 0.00010865639936271908, 'epoch': 0.46}\n",
      "{'loss': 0.7817, 'grad_norm': 0.24703176319599152, 'learning_rate': 0.00010862099486634802, 'epoch': 0.46}\n",
      "{'loss': 0.7028, 'grad_norm': 0.2017187774181366, 'learning_rate': 0.000108585590369977, 'epoch': 0.46}\n",
      "{'loss': 0.4414, 'grad_norm': 0.21636246144771576, 'learning_rate': 0.00010855018587360594, 'epoch': 0.46}\n",
      "{'loss': 0.779, 'grad_norm': 0.36271193623542786, 'learning_rate': 0.00010851478137723491, 'epoch': 0.46}\n",
      "{'loss': 0.6736, 'grad_norm': 0.21245118975639343, 'learning_rate': 0.00010847937688086387, 'epoch': 0.46}\n",
      "{'loss': 0.8198, 'grad_norm': 0.262621134519577, 'learning_rate': 0.00010844397238449284, 'epoch': 0.46}\n",
      "{'loss': 0.4383, 'grad_norm': 0.19935491681098938, 'learning_rate': 0.00010840856788812179, 'epoch': 0.46}\n",
      "{'loss': 0.8667, 'grad_norm': 0.21059705317020416, 'learning_rate': 0.00010837316339175076, 'epoch': 0.46}\n",
      "{'loss': 0.877, 'grad_norm': 0.22180989384651184, 'learning_rate': 0.00010833775889537971, 'epoch': 0.46}\n",
      "{'loss': 1.0238, 'grad_norm': 0.24394957721233368, 'learning_rate': 0.00010830235439900868, 'epoch': 0.46}\n",
      "{'loss': 0.5211, 'grad_norm': 0.2536158263683319, 'learning_rate': 0.00010826694990263764, 'epoch': 0.46}\n",
      "{'loss': 0.8945, 'grad_norm': 0.32922816276550293, 'learning_rate': 0.0001082315454062666, 'epoch': 0.46}\n",
      "{'loss': 0.733, 'grad_norm': 0.21288646757602692, 'learning_rate': 0.00010819614090989556, 'epoch': 0.46}\n",
      "{'loss': 0.7281, 'grad_norm': 0.22849129140377045, 'learning_rate': 0.00010816073641352453, 'epoch': 0.46}\n",
      "{'loss': 0.6775, 'grad_norm': 0.24517202377319336, 'learning_rate': 0.00010812533191715348, 'epoch': 0.46}\n",
      "{'loss': 0.6067, 'grad_norm': 0.2354881465435028, 'learning_rate': 0.00010808992742078245, 'epoch': 0.46}\n",
      "{'loss': 0.5745, 'grad_norm': 0.15356820821762085, 'learning_rate': 0.0001080545229244114, 'epoch': 0.46}\n",
      "{'loss': 0.524, 'grad_norm': 0.1919865757226944, 'learning_rate': 0.00010801911842804036, 'epoch': 0.46}\n",
      "{'loss': 0.702, 'grad_norm': 0.2413950264453888, 'learning_rate': 0.00010798371393166933, 'epoch': 0.46}\n",
      "{'loss': 0.6241, 'grad_norm': 0.18003471195697784, 'learning_rate': 0.00010794830943529828, 'epoch': 0.46}\n",
      "{'loss': 0.6822, 'grad_norm': 0.19115789234638214, 'learning_rate': 0.00010791290493892725, 'epoch': 0.46}\n",
      "{'loss': 0.5514, 'grad_norm': 0.19320395588874817, 'learning_rate': 0.00010787750044255621, 'epoch': 0.46}\n",
      "{'loss': 0.6896, 'grad_norm': 0.20253266394138336, 'learning_rate': 0.00010784209594618518, 'epoch': 0.46}\n",
      "{'loss': 0.5896, 'grad_norm': 0.2398148775100708, 'learning_rate': 0.00010780669144981412, 'epoch': 0.46}\n",
      "{'loss': 0.8395, 'grad_norm': 0.24196071922779083, 'learning_rate': 0.0001077712869534431, 'epoch': 0.46}\n",
      "{'loss': 0.5516, 'grad_norm': 0.25583869218826294, 'learning_rate': 0.00010773588245707204, 'epoch': 0.46}\n",
      "{'loss': 0.4797, 'grad_norm': 0.17170646786689758, 'learning_rate': 0.00010770047796070102, 'epoch': 0.46}\n",
      "{'loss': 0.4589, 'grad_norm': 0.20305348932743073, 'learning_rate': 0.00010766507346432996, 'epoch': 0.46}\n",
      "{'loss': 0.6626, 'grad_norm': 0.24845735728740692, 'learning_rate': 0.00010762966896795894, 'epoch': 0.46}\n",
      "{'loss': 0.7978, 'grad_norm': 0.25659412145614624, 'learning_rate': 0.00010759426447158789, 'epoch': 0.46}\n",
      "{'loss': 0.7821, 'grad_norm': 0.23621244728565216, 'learning_rate': 0.00010755885997521687, 'epoch': 0.46}\n",
      "{'loss': 0.7675, 'grad_norm': 0.19849920272827148, 'learning_rate': 0.00010752345547884581, 'epoch': 0.46}\n",
      "{'loss': 0.8034, 'grad_norm': 0.26603448390960693, 'learning_rate': 0.00010748805098247479, 'epoch': 0.46}\n",
      "{'loss': 0.9026, 'grad_norm': 0.21573619544506073, 'learning_rate': 0.00010745264648610373, 'epoch': 0.46}\n",
      "{'loss': 0.6707, 'grad_norm': 0.29377177357673645, 'learning_rate': 0.00010741724198973271, 'epoch': 0.46}\n",
      "{'loss': 0.9172, 'grad_norm': 0.26290401816368103, 'learning_rate': 0.00010738183749336166, 'epoch': 0.46}\n",
      "{'loss': 0.818, 'grad_norm': 0.27637314796447754, 'learning_rate': 0.00010734643299699064, 'epoch': 0.46}\n",
      "{'loss': 0.5361, 'grad_norm': 0.19218620657920837, 'learning_rate': 0.00010731102850061958, 'epoch': 0.46}\n",
      "{'loss': 0.8713, 'grad_norm': 0.25139665603637695, 'learning_rate': 0.00010727562400424856, 'epoch': 0.46}\n",
      "{'loss': 0.6732, 'grad_norm': 0.18292434513568878, 'learning_rate': 0.0001072402195078775, 'epoch': 0.46}\n",
      "{'loss': 0.9425, 'grad_norm': 0.24886971712112427, 'learning_rate': 0.00010720481501150647, 'epoch': 0.46}\n",
      "{'loss': 0.7509, 'grad_norm': 0.20498576760292053, 'learning_rate': 0.00010716941051513542, 'epoch': 0.46}\n",
      "{'loss': 0.8894, 'grad_norm': 0.27097654342651367, 'learning_rate': 0.00010713400601876439, 'epoch': 0.46}\n",
      "{'loss': 0.6078, 'grad_norm': 0.20841854810714722, 'learning_rate': 0.00010709860152239335, 'epoch': 0.46}\n",
      "{'loss': 1.0067, 'grad_norm': 0.31031322479248047, 'learning_rate': 0.0001070631970260223, 'epoch': 0.47}\n",
      "{'loss': 0.4091, 'grad_norm': 0.24445925652980804, 'learning_rate': 0.00010702779252965127, 'epoch': 0.47}\n",
      "{'loss': 0.6149, 'grad_norm': 0.19264136254787445, 'learning_rate': 0.00010699238803328023, 'epoch': 0.47}\n",
      "{'loss': 1.0085, 'grad_norm': 0.27428609132766724, 'learning_rate': 0.0001069569835369092, 'epoch': 0.47}\n",
      "{'loss': 0.7299, 'grad_norm': 0.22980822622776031, 'learning_rate': 0.00010692157904053815, 'epoch': 0.47}\n",
      "{'loss': 0.4965, 'grad_norm': 0.22672252357006073, 'learning_rate': 0.00010688617454416712, 'epoch': 0.47}\n",
      "{'loss': 0.7827, 'grad_norm': 0.2921290993690491, 'learning_rate': 0.00010685077004779607, 'epoch': 0.47}\n",
      "{'loss': 1.0007, 'grad_norm': 0.25431886315345764, 'learning_rate': 0.00010681536555142504, 'epoch': 0.47}\n",
      "{'loss': 0.8464, 'grad_norm': 0.2801722586154938, 'learning_rate': 0.000106779961055054, 'epoch': 0.47}\n",
      "{'loss': 0.6908, 'grad_norm': 0.2068089097738266, 'learning_rate': 0.00010674455655868296, 'epoch': 0.47}\n",
      "{'loss': 0.8335, 'grad_norm': 0.313225656747818, 'learning_rate': 0.00010670915206231192, 'epoch': 0.47}\n",
      "{'loss': 0.7558, 'grad_norm': 0.26517772674560547, 'learning_rate': 0.00010667374756594089, 'epoch': 0.47}\n",
      "{'loss': 0.9123, 'grad_norm': 0.2954372465610504, 'learning_rate': 0.00010663834306956984, 'epoch': 0.47}\n",
      "{'loss': 0.5534, 'grad_norm': 0.20372514426708221, 'learning_rate': 0.00010660293857319881, 'epoch': 0.47}\n",
      "{'loss': 0.8161, 'grad_norm': 0.24583998322486877, 'learning_rate': 0.00010656753407682776, 'epoch': 0.47}\n",
      "{'loss': 0.7921, 'grad_norm': 0.26653605699539185, 'learning_rate': 0.00010653212958045673, 'epoch': 0.47}\n",
      "{'loss': 0.6279, 'grad_norm': 0.1947384625673294, 'learning_rate': 0.00010649672508408567, 'epoch': 0.47}\n",
      "{'loss': 0.6465, 'grad_norm': 0.18230681121349335, 'learning_rate': 0.00010646132058771466, 'epoch': 0.47}\n",
      "{'loss': 0.859, 'grad_norm': 0.22407114505767822, 'learning_rate': 0.0001064259160913436, 'epoch': 0.47}\n",
      "{'loss': 0.776, 'grad_norm': 0.26916736364364624, 'learning_rate': 0.00010639051159497258, 'epoch': 0.47}\n",
      "{'loss': 0.5712, 'grad_norm': 0.22183118760585785, 'learning_rate': 0.00010635510709860152, 'epoch': 0.47}\n",
      "{'loss': 0.6705, 'grad_norm': 0.25424209237098694, 'learning_rate': 0.0001063197026022305, 'epoch': 0.47}\n",
      "{'loss': 0.9203, 'grad_norm': 0.34055590629577637, 'learning_rate': 0.00010628429810585944, 'epoch': 0.47}\n",
      "{'loss': 0.8568, 'grad_norm': 0.2783324122428894, 'learning_rate': 0.00010624889360948842, 'epoch': 0.47}\n",
      "{'loss': 0.6402, 'grad_norm': 0.20635321736335754, 'learning_rate': 0.00010621348911311737, 'epoch': 0.47}\n",
      "{'loss': 0.6753, 'grad_norm': 0.24133442342281342, 'learning_rate': 0.00010617808461674635, 'epoch': 0.47}\n",
      "{'loss': 0.6355, 'grad_norm': 0.22917748987674713, 'learning_rate': 0.00010614268012037529, 'epoch': 0.47}\n",
      "{'loss': 0.7163, 'grad_norm': 0.17921122908592224, 'learning_rate': 0.00010610727562400424, 'epoch': 0.47}\n",
      "{'loss': 0.5502, 'grad_norm': 0.19788241386413574, 'learning_rate': 0.00010607187112763321, 'epoch': 0.47}\n",
      "{'loss': 0.6887, 'grad_norm': 0.20502158999443054, 'learning_rate': 0.00010603646663126217, 'epoch': 0.47}\n",
      "{'loss': 0.8351, 'grad_norm': 0.3160320222377777, 'learning_rate': 0.00010600106213489114, 'epoch': 0.47}\n",
      "{'loss': 0.7945, 'grad_norm': 0.24515043199062347, 'learning_rate': 0.00010596565763852009, 'epoch': 0.47}\n",
      "{'loss': 0.5678, 'grad_norm': 0.21199741959571838, 'learning_rate': 0.00010593025314214906, 'epoch': 0.47}\n",
      "{'loss': 0.6261, 'grad_norm': 0.21456341445446014, 'learning_rate': 0.00010589484864577801, 'epoch': 0.47}\n",
      "{'loss': 0.5909, 'grad_norm': 0.21841441094875336, 'learning_rate': 0.00010585944414940698, 'epoch': 0.47}\n",
      "{'loss': 0.7248, 'grad_norm': 0.2260972410440445, 'learning_rate': 0.00010582403965303594, 'epoch': 0.47}\n",
      "{'loss': 0.69, 'grad_norm': 0.2167934775352478, 'learning_rate': 0.0001057886351566649, 'epoch': 0.47}\n",
      "{'loss': 1.026, 'grad_norm': 0.24505242705345154, 'learning_rate': 0.00010575323066029386, 'epoch': 0.47}\n",
      "{'loss': 0.8052, 'grad_norm': 0.250282347202301, 'learning_rate': 0.00010571782616392283, 'epoch': 0.47}\n",
      "{'loss': 0.8935, 'grad_norm': 0.2680604159832001, 'learning_rate': 0.00010568242166755178, 'epoch': 0.47}\n",
      "{'loss': 0.9882, 'grad_norm': 0.26162341237068176, 'learning_rate': 0.00010564701717118075, 'epoch': 0.47}\n",
      "{'loss': 0.5443, 'grad_norm': 0.20021311938762665, 'learning_rate': 0.0001056116126748097, 'epoch': 0.47}\n",
      "{'loss': 0.5922, 'grad_norm': 0.1750379353761673, 'learning_rate': 0.00010557620817843867, 'epoch': 0.47}\n",
      "{'loss': 0.9211, 'grad_norm': 0.26427972316741943, 'learning_rate': 0.00010554080368206763, 'epoch': 0.47}\n",
      "{'loss': 0.9981, 'grad_norm': 0.2572423219680786, 'learning_rate': 0.0001055053991856966, 'epoch': 0.47}\n",
      "{'loss': 1.1556, 'grad_norm': 0.2770293056964874, 'learning_rate': 0.00010546999468932555, 'epoch': 0.47}\n",
      "{'loss': 0.5991, 'grad_norm': 0.2161409705877304, 'learning_rate': 0.00010543459019295452, 'epoch': 0.47}\n",
      "{'loss': 0.524, 'grad_norm': 0.21131722629070282, 'learning_rate': 0.00010539918569658347, 'epoch': 0.47}\n",
      "{'loss': 0.9421, 'grad_norm': 0.2184387445449829, 'learning_rate': 0.00010536378120021244, 'epoch': 0.47}\n",
      "{'loss': 0.6103, 'grad_norm': 0.21813222765922546, 'learning_rate': 0.0001053283767038414, 'epoch': 0.47}\n",
      "{'loss': 0.6266, 'grad_norm': 0.2087428718805313, 'learning_rate': 0.00010529297220747037, 'epoch': 0.47}\n",
      "{'loss': 0.6972, 'grad_norm': 0.24931685626506805, 'learning_rate': 0.00010525756771109932, 'epoch': 0.47}\n",
      "{'loss': 0.7471, 'grad_norm': 0.24164675176143646, 'learning_rate': 0.00010522216321472829, 'epoch': 0.47}\n",
      "{'loss': 0.8562, 'grad_norm': 0.2207854688167572, 'learning_rate': 0.00010518675871835723, 'epoch': 0.47}\n",
      "{'loss': 0.5477, 'grad_norm': 0.22296863794326782, 'learning_rate': 0.00010515135422198619, 'epoch': 0.47}\n",
      "{'loss': 0.7243, 'grad_norm': 0.20860788226127625, 'learning_rate': 0.00010511594972561515, 'epoch': 0.47}\n",
      "{'loss': 0.7386, 'grad_norm': 0.22347186505794525, 'learning_rate': 0.00010508054522924411, 'epoch': 0.48}\n",
      "{'loss': 0.7728, 'grad_norm': 0.20321956276893616, 'learning_rate': 0.00010504514073287308, 'epoch': 0.48}\n",
      "{'loss': 0.634, 'grad_norm': 0.2060418576002121, 'learning_rate': 0.00010500973623650203, 'epoch': 0.48}\n",
      "{'loss': 0.8683, 'grad_norm': 0.23624251782894135, 'learning_rate': 0.000104974331740131, 'epoch': 0.48}\n",
      "{'loss': 0.6794, 'grad_norm': 0.22625766694545746, 'learning_rate': 0.00010493892724375995, 'epoch': 0.48}\n",
      "{'loss': 0.6306, 'grad_norm': 0.20947863161563873, 'learning_rate': 0.00010490352274738892, 'epoch': 0.48}\n",
      "{'loss': 0.8235, 'grad_norm': 0.26519104838371277, 'learning_rate': 0.00010486811825101788, 'epoch': 0.48}\n",
      "{'loss': 0.6529, 'grad_norm': 0.2247052639722824, 'learning_rate': 0.00010483271375464685, 'epoch': 0.48}\n",
      "{'loss': 0.6025, 'grad_norm': 0.21143415570259094, 'learning_rate': 0.0001047973092582758, 'epoch': 0.48}\n",
      "{'loss': 0.4918, 'grad_norm': 0.24483978748321533, 'learning_rate': 0.00010476190476190477, 'epoch': 0.48}\n",
      "{'loss': 1.1494, 'grad_norm': 0.32515525817871094, 'learning_rate': 0.00010472650026553372, 'epoch': 0.48}\n",
      "{'loss': 0.5917, 'grad_norm': 0.20963916182518005, 'learning_rate': 0.00010469109576916269, 'epoch': 0.48}\n",
      "{'loss': 0.5697, 'grad_norm': 0.2123851627111435, 'learning_rate': 0.00010465569127279165, 'epoch': 0.48}\n",
      "{'loss': 0.659, 'grad_norm': 0.1733449548482895, 'learning_rate': 0.00010462028677642062, 'epoch': 0.48}\n",
      "{'loss': 0.8066, 'grad_norm': 0.2988409698009491, 'learning_rate': 0.00010458488228004957, 'epoch': 0.48}\n",
      "{'loss': 0.5705, 'grad_norm': 0.20401020348072052, 'learning_rate': 0.00010454947778367854, 'epoch': 0.48}\n",
      "{'loss': 0.5147, 'grad_norm': 0.23099985718727112, 'learning_rate': 0.00010451407328730749, 'epoch': 0.48}\n",
      "{'loss': 0.797, 'grad_norm': 0.2411177158355713, 'learning_rate': 0.00010447866879093646, 'epoch': 0.48}\n",
      "{'loss': 0.7424, 'grad_norm': 0.22143033146858215, 'learning_rate': 0.00010444326429456542, 'epoch': 0.48}\n",
      "{'loss': 1.0022, 'grad_norm': 0.2648310661315918, 'learning_rate': 0.00010440785979819438, 'epoch': 0.48}\n",
      "{'loss': 0.5984, 'grad_norm': 0.23411710560321808, 'learning_rate': 0.00010437245530182334, 'epoch': 0.48}\n",
      "{'loss': 0.9751, 'grad_norm': 0.30542171001434326, 'learning_rate': 0.00010433705080545231, 'epoch': 0.48}\n",
      "{'loss': 0.7125, 'grad_norm': 0.22514581680297852, 'learning_rate': 0.00010430164630908126, 'epoch': 0.48}\n",
      "{'loss': 0.6383, 'grad_norm': 0.24345268309116364, 'learning_rate': 0.00010426624181271023, 'epoch': 0.48}\n",
      "{'loss': 0.5294, 'grad_norm': 0.2408807873725891, 'learning_rate': 0.00010423083731633919, 'epoch': 0.48}\n",
      "{'loss': 0.7021, 'grad_norm': 0.23090393841266632, 'learning_rate': 0.00010419543281996813, 'epoch': 0.48}\n",
      "{'loss': 0.6543, 'grad_norm': 0.21770675480365753, 'learning_rate': 0.00010416002832359711, 'epoch': 0.48}\n",
      "{'loss': 1.062, 'grad_norm': 0.2832573354244232, 'learning_rate': 0.00010412462382722605, 'epoch': 0.48}\n",
      "{'loss': 0.7226, 'grad_norm': 0.2550728917121887, 'learning_rate': 0.00010408921933085503, 'epoch': 0.48}\n",
      "{'loss': 0.719, 'grad_norm': 0.2258361130952835, 'learning_rate': 0.00010405381483448397, 'epoch': 0.48}\n",
      "{'loss': 0.8417, 'grad_norm': 0.2706010341644287, 'learning_rate': 0.00010401841033811295, 'epoch': 0.48}\n",
      "{'loss': 0.7426, 'grad_norm': 0.1877041459083557, 'learning_rate': 0.0001039830058417419, 'epoch': 0.48}\n",
      "{'loss': 1.0465, 'grad_norm': 0.24473513662815094, 'learning_rate': 0.00010394760134537088, 'epoch': 0.48}\n",
      "{'loss': 0.6211, 'grad_norm': 0.19526632130146027, 'learning_rate': 0.00010391219684899982, 'epoch': 0.48}\n",
      "{'loss': 0.6471, 'grad_norm': 0.22149008512496948, 'learning_rate': 0.00010387679235262879, 'epoch': 0.48}\n",
      "{'loss': 0.6672, 'grad_norm': 0.2561721205711365, 'learning_rate': 0.00010384138785625774, 'epoch': 0.48}\n",
      "{'loss': 0.835, 'grad_norm': 0.2792675793170929, 'learning_rate': 0.00010380598335988671, 'epoch': 0.48}\n",
      "{'loss': 0.8605, 'grad_norm': 0.2682746350765228, 'learning_rate': 0.00010377057886351566, 'epoch': 0.48}\n",
      "{'loss': 0.5406, 'grad_norm': 0.18438003957271576, 'learning_rate': 0.00010373517436714463, 'epoch': 0.48}\n",
      "{'loss': 0.8516, 'grad_norm': 0.24045534431934357, 'learning_rate': 0.00010369976987077359, 'epoch': 0.48}\n",
      "{'loss': 0.4518, 'grad_norm': 0.14888791739940643, 'learning_rate': 0.00010366436537440256, 'epoch': 0.48}\n",
      "{'loss': 0.6855, 'grad_norm': 0.3186296224594116, 'learning_rate': 0.00010362896087803151, 'epoch': 0.48}\n",
      "{'loss': 0.7056, 'grad_norm': 0.2585533857345581, 'learning_rate': 0.00010359355638166048, 'epoch': 0.48}\n",
      "{'loss': 0.9659, 'grad_norm': 0.22363615036010742, 'learning_rate': 0.00010355815188528943, 'epoch': 0.48}\n",
      "{'loss': 0.5243, 'grad_norm': 0.19569480419158936, 'learning_rate': 0.0001035227473889184, 'epoch': 0.48}\n",
      "{'loss': 0.794, 'grad_norm': 0.22978119552135468, 'learning_rate': 0.00010348734289254736, 'epoch': 0.48}\n",
      "{'loss': 0.7342, 'grad_norm': 0.2864730656147003, 'learning_rate': 0.00010345193839617633, 'epoch': 0.48}\n",
      "{'loss': 0.6788, 'grad_norm': 0.22188855707645416, 'learning_rate': 0.00010341653389980528, 'epoch': 0.48}\n",
      "{'loss': 0.4779, 'grad_norm': 0.21723778545856476, 'learning_rate': 0.00010338112940343425, 'epoch': 0.48}\n",
      "{'loss': 0.5878, 'grad_norm': 0.22413110733032227, 'learning_rate': 0.0001033457249070632, 'epoch': 0.48}\n",
      "{'loss': 0.5663, 'grad_norm': 0.18740427494049072, 'learning_rate': 0.00010331032041069216, 'epoch': 0.48}\n",
      "{'loss': 0.7423, 'grad_norm': 0.2540895938873291, 'learning_rate': 0.00010327491591432113, 'epoch': 0.48}\n",
      "{'loss': 0.8954, 'grad_norm': 0.24069248139858246, 'learning_rate': 0.00010323951141795007, 'epoch': 0.48}\n",
      "{'loss': 0.8302, 'grad_norm': 0.2250695377588272, 'learning_rate': 0.00010320410692157905, 'epoch': 0.48}\n",
      "{'loss': 0.6169, 'grad_norm': 0.225788414478302, 'learning_rate': 0.00010316870242520799, 'epoch': 0.48}\n",
      "{'loss': 0.644, 'grad_norm': 0.19339482486248016, 'learning_rate': 0.00010313329792883697, 'epoch': 0.48}\n",
      "{'loss': 0.5562, 'grad_norm': 0.22283567488193512, 'learning_rate': 0.00010309789343246591, 'epoch': 0.48}\n",
      "{'loss': 0.6296, 'grad_norm': 0.21377070248126984, 'learning_rate': 0.0001030624889360949, 'epoch': 0.49}\n",
      "{'loss': 0.8406, 'grad_norm': 0.28180649876594543, 'learning_rate': 0.00010302708443972384, 'epoch': 0.49}\n",
      "{'loss': 0.7318, 'grad_norm': 0.248820960521698, 'learning_rate': 0.00010299167994335282, 'epoch': 0.49}\n",
      "{'loss': 0.6238, 'grad_norm': 0.21515463292598724, 'learning_rate': 0.00010295627544698176, 'epoch': 0.49}\n",
      "{'loss': 0.8673, 'grad_norm': 0.2446656972169876, 'learning_rate': 0.00010292087095061074, 'epoch': 0.49}\n",
      "{'loss': 1.0153, 'grad_norm': 0.294536292552948, 'learning_rate': 0.00010288546645423968, 'epoch': 0.49}\n",
      "{'loss': 0.765, 'grad_norm': 0.2319149225950241, 'learning_rate': 0.00010285006195786867, 'epoch': 0.49}\n",
      "{'loss': 0.9667, 'grad_norm': 0.2038014531135559, 'learning_rate': 0.0001028146574614976, 'epoch': 0.49}\n",
      "{'loss': 0.718, 'grad_norm': 0.2223721444606781, 'learning_rate': 0.00010277925296512659, 'epoch': 0.49}\n",
      "{'loss': 0.6223, 'grad_norm': 0.21092863380908966, 'learning_rate': 0.00010274384846875553, 'epoch': 0.49}\n",
      "{'loss': 0.6787, 'grad_norm': 0.2169850915670395, 'learning_rate': 0.00010270844397238451, 'epoch': 0.49}\n",
      "{'loss': 0.8305, 'grad_norm': 0.2552722990512848, 'learning_rate': 0.00010267303947601345, 'epoch': 0.49}\n",
      "{'loss': 0.5102, 'grad_norm': 0.17353972792625427, 'learning_rate': 0.00010263763497964242, 'epoch': 0.49}\n",
      "{'loss': 0.5858, 'grad_norm': 0.20371299982070923, 'learning_rate': 0.00010260223048327138, 'epoch': 0.49}\n",
      "{'loss': 0.573, 'grad_norm': 0.17755727469921112, 'learning_rate': 0.00010256682598690034, 'epoch': 0.49}\n",
      "{'loss': 0.5532, 'grad_norm': 0.18838022649288177, 'learning_rate': 0.0001025314214905293, 'epoch': 0.49}\n",
      "{'loss': 1.0995, 'grad_norm': 0.27127805352211, 'learning_rate': 0.00010249601699415827, 'epoch': 0.49}\n",
      "{'loss': 0.7207, 'grad_norm': 0.24718739092350006, 'learning_rate': 0.00010246061249778722, 'epoch': 0.49}\n",
      "{'loss': 0.7249, 'grad_norm': 0.19951148331165314, 'learning_rate': 0.00010242520800141619, 'epoch': 0.49}\n",
      "{'loss': 0.5073, 'grad_norm': 0.22953493893146515, 'learning_rate': 0.00010238980350504514, 'epoch': 0.49}\n",
      "{'loss': 0.5574, 'grad_norm': 0.22416281700134277, 'learning_rate': 0.0001023543990086741, 'epoch': 0.49}\n",
      "{'loss': 0.7936, 'grad_norm': 0.2862328290939331, 'learning_rate': 0.00010231899451230307, 'epoch': 0.49}\n",
      "{'loss': 0.7112, 'grad_norm': 0.21211035549640656, 'learning_rate': 0.00010228359001593202, 'epoch': 0.49}\n",
      "{'loss': 0.8126, 'grad_norm': 0.22207632660865784, 'learning_rate': 0.00010224818551956099, 'epoch': 0.49}\n",
      "{'loss': 0.5418, 'grad_norm': 0.20456095039844513, 'learning_rate': 0.00010221278102318995, 'epoch': 0.49}\n",
      "{'loss': 0.9508, 'grad_norm': 0.2563379108905792, 'learning_rate': 0.00010217737652681891, 'epoch': 0.49}\n",
      "{'loss': 0.685, 'grad_norm': 0.2576889097690582, 'learning_rate': 0.00010214197203044787, 'epoch': 0.49}\n",
      "{'loss': 0.4547, 'grad_norm': 0.1952674686908722, 'learning_rate': 0.00010210656753407684, 'epoch': 0.49}\n",
      "{'loss': 0.8985, 'grad_norm': 0.31027188897132874, 'learning_rate': 0.00010207116303770579, 'epoch': 0.49}\n",
      "{'loss': 0.742, 'grad_norm': 0.27509111166000366, 'learning_rate': 0.00010203575854133476, 'epoch': 0.49}\n",
      "{'loss': 0.6909, 'grad_norm': 0.22383710741996765, 'learning_rate': 0.00010200035404496371, 'epoch': 0.49}\n",
      "{'loss': 0.85, 'grad_norm': 0.21185724437236786, 'learning_rate': 0.00010196494954859268, 'epoch': 0.49}\n",
      "{'loss': 0.7119, 'grad_norm': 0.2617868185043335, 'learning_rate': 0.00010192954505222162, 'epoch': 0.49}\n",
      "{'loss': 0.58, 'grad_norm': 0.21217423677444458, 'learning_rate': 0.0001018941405558506, 'epoch': 0.49}\n",
      "{'loss': 0.6887, 'grad_norm': 0.4541979432106018, 'learning_rate': 0.00010185873605947955, 'epoch': 0.49}\n",
      "{'loss': 0.6431, 'grad_norm': 0.19714471697807312, 'learning_rate': 0.00010182333156310853, 'epoch': 0.49}\n",
      "{'loss': 0.8279, 'grad_norm': 0.2115275114774704, 'learning_rate': 0.00010178792706673747, 'epoch': 0.49}\n",
      "{'loss': 0.674, 'grad_norm': 0.16734376549720764, 'learning_rate': 0.00010175252257036645, 'epoch': 0.49}\n",
      "{'loss': 0.8147, 'grad_norm': 0.2607478201389313, 'learning_rate': 0.0001017171180739954, 'epoch': 0.49}\n",
      "{'loss': 0.7704, 'grad_norm': 0.2352188229560852, 'learning_rate': 0.00010168171357762438, 'epoch': 0.49}\n",
      "{'loss': 0.686, 'grad_norm': 0.2724601626396179, 'learning_rate': 0.00010164630908125332, 'epoch': 0.49}\n",
      "{'loss': 0.6083, 'grad_norm': 0.24944007396697998, 'learning_rate': 0.0001016109045848823, 'epoch': 0.49}\n",
      "{'loss': 0.768, 'grad_norm': 0.19004514813423157, 'learning_rate': 0.00010157550008851124, 'epoch': 0.49}\n",
      "{'loss': 0.6853, 'grad_norm': 0.2545090317726135, 'learning_rate': 0.00010154009559214022, 'epoch': 0.49}\n",
      "{'loss': 0.6831, 'grad_norm': 0.2861522138118744, 'learning_rate': 0.00010150469109576916, 'epoch': 0.49}\n",
      "{'loss': 0.7715, 'grad_norm': 0.22078344225883484, 'learning_rate': 0.00010146928659939814, 'epoch': 0.49}\n",
      "{'loss': 1.1736, 'grad_norm': 0.24329303205013275, 'learning_rate': 0.00010143388210302709, 'epoch': 0.49}\n",
      "{'loss': 0.6498, 'grad_norm': 0.22061787545681, 'learning_rate': 0.00010139847760665604, 'epoch': 0.49}\n",
      "{'loss': 0.707, 'grad_norm': 0.20069478452205658, 'learning_rate': 0.00010136307311028501, 'epoch': 0.49}\n",
      "{'loss': 0.9306, 'grad_norm': 0.30641037225723267, 'learning_rate': 0.00010132766861391396, 'epoch': 0.49}\n",
      "{'loss': 0.6252, 'grad_norm': 0.22469505667686462, 'learning_rate': 0.00010129226411754293, 'epoch': 0.49}\n",
      "{'loss': 0.5617, 'grad_norm': 0.15809479355812073, 'learning_rate': 0.00010125685962117189, 'epoch': 0.49}\n",
      "{'loss': 0.9008, 'grad_norm': 0.2107924073934555, 'learning_rate': 0.00010122145512480086, 'epoch': 0.49}\n",
      "{'loss': 0.8059, 'grad_norm': 0.21756786108016968, 'learning_rate': 0.00010118605062842981, 'epoch': 0.49}\n",
      "{'loss': 0.9352, 'grad_norm': 0.3023206889629364, 'learning_rate': 0.00010115064613205878, 'epoch': 0.49}\n",
      "{'loss': 0.8188, 'grad_norm': 0.24089518189430237, 'learning_rate': 0.00010111524163568773, 'epoch': 0.49}\n",
      "{'loss': 0.8002, 'grad_norm': 0.1983477622270584, 'learning_rate': 0.0001010798371393167, 'epoch': 0.5}\n",
      "{'loss': 0.7613, 'grad_norm': 0.2220839262008667, 'learning_rate': 0.00010104443264294566, 'epoch': 0.5}\n",
      "{'loss': 0.871, 'grad_norm': 0.2138621062040329, 'learning_rate': 0.00010100902814657462, 'epoch': 0.5}\n",
      "{'loss': 1.0433, 'grad_norm': 0.2458069920539856, 'learning_rate': 0.00010097362365020358, 'epoch': 0.5}\n",
      "{'loss': 0.9599, 'grad_norm': 0.26483073830604553, 'learning_rate': 0.00010093821915383255, 'epoch': 0.5}\n",
      "{'loss': 0.7127, 'grad_norm': 0.20627763867378235, 'learning_rate': 0.0001009028146574615, 'epoch': 0.5}\n",
      "{'loss': 0.5945, 'grad_norm': 0.24315232038497925, 'learning_rate': 0.00010086741016109047, 'epoch': 0.5}\n",
      "{'loss': 0.5339, 'grad_norm': 0.18072597682476044, 'learning_rate': 0.00010083200566471943, 'epoch': 0.5}\n",
      "{'loss': 0.8309, 'grad_norm': 0.2153531163930893, 'learning_rate': 0.0001007966011683484, 'epoch': 0.5}\n",
      "{'loss': 0.709, 'grad_norm': 0.33809322118759155, 'learning_rate': 0.00010076119667197735, 'epoch': 0.5}\n",
      "{'loss': 0.6323, 'grad_norm': 0.2115057408809662, 'learning_rate': 0.00010072579217560632, 'epoch': 0.5}\n",
      "{'loss': 0.6187, 'grad_norm': 0.2714277505874634, 'learning_rate': 0.00010069038767923527, 'epoch': 0.5}\n",
      "{'loss': 0.7493, 'grad_norm': 0.19859333336353302, 'learning_rate': 0.00010065498318286424, 'epoch': 0.5}\n",
      "{'loss': 0.5604, 'grad_norm': 0.22422108054161072, 'learning_rate': 0.00010061957868649318, 'epoch': 0.5}\n",
      "{'loss': 0.5496, 'grad_norm': 0.19379602372646332, 'learning_rate': 0.00010058417419012216, 'epoch': 0.5}\n",
      "{'loss': 1.221, 'grad_norm': 0.27394580841064453, 'learning_rate': 0.0001005487696937511, 'epoch': 0.5}\n",
      "{'loss': 0.7855, 'grad_norm': 0.2650500237941742, 'learning_rate': 0.00010051336519738009, 'epoch': 0.5}\n",
      "{'loss': 0.8024, 'grad_norm': 0.25773727893829346, 'learning_rate': 0.00010047796070100903, 'epoch': 0.5}\n",
      "{'loss': 0.76, 'grad_norm': 0.18980854749679565, 'learning_rate': 0.00010044255620463798, 'epoch': 0.5}\n",
      "{'loss': 0.8111, 'grad_norm': 0.2270306795835495, 'learning_rate': 0.00010040715170826695, 'epoch': 0.5}\n",
      "{'loss': 0.8811, 'grad_norm': 0.23864607512950897, 'learning_rate': 0.0001003717472118959, 'epoch': 0.5}\n",
      "{'loss': 0.8994, 'grad_norm': 0.24722562730312347, 'learning_rate': 0.00010033634271552487, 'epoch': 0.5}\n",
      "{'loss': 0.6334, 'grad_norm': 0.1900489181280136, 'learning_rate': 0.00010030093821915383, 'epoch': 0.5}\n",
      "{'loss': 0.4942, 'grad_norm': 0.20111370086669922, 'learning_rate': 0.0001002655337227828, 'epoch': 0.5}\n",
      "{'loss': 0.6074, 'grad_norm': 0.24264918267726898, 'learning_rate': 0.00010023012922641175, 'epoch': 0.5}\n",
      "{'loss': 0.4872, 'grad_norm': 0.21122334897518158, 'learning_rate': 0.00010019472473004072, 'epoch': 0.5}\n",
      "{'loss': 1.0532, 'grad_norm': 0.27774566411972046, 'learning_rate': 0.00010015932023366967, 'epoch': 0.5}\n",
      "{'loss': 0.7333, 'grad_norm': 0.3861859440803528, 'learning_rate': 0.00010012391573729864, 'epoch': 0.5}\n",
      "{'loss': 0.7042, 'grad_norm': 0.24148379266262054, 'learning_rate': 0.0001000885112409276, 'epoch': 0.5}\n",
      "{'loss': 0.6769, 'grad_norm': 0.21330343186855316, 'learning_rate': 0.00010005310674455657, 'epoch': 0.5}\n",
      "{'loss': 0.627, 'grad_norm': 0.21466155350208282, 'learning_rate': 0.00010001770224818552, 'epoch': 0.5}\n",
      "{'loss': 0.9571, 'grad_norm': 0.2651554048061371, 'learning_rate': 9.998229775181448e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9283, 'grad_norm': 0.3095510005950928, 'learning_rate': 9.994689325544344e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5688, 'grad_norm': 0.26221954822540283, 'learning_rate': 9.99114887590724e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7606, 'grad_norm': 0.26094502210617065, 'learning_rate': 9.987608426270137e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6913, 'grad_norm': 0.17186059057712555, 'learning_rate': 9.984067976633032e-05, 'epoch': 0.5}\n",
      "{'loss': 1.111, 'grad_norm': 0.2755296528339386, 'learning_rate': 9.980527526995929e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4114, 'grad_norm': 0.15814602375030518, 'learning_rate': 9.976987077358824e-05, 'epoch': 0.5}\n",
      "{'loss': 0.615, 'grad_norm': 0.18171517550945282, 'learning_rate': 9.973446627721721e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6158, 'grad_norm': 0.1942029595375061, 'learning_rate': 9.969906178084617e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6861, 'grad_norm': 0.2812466621398926, 'learning_rate': 9.966365728447514e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2102, 'grad_norm': 0.1757666915655136, 'learning_rate': 9.962825278810409e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8376, 'grad_norm': 0.25882843136787415, 'learning_rate': 9.959284829173306e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7531, 'grad_norm': 0.23985400795936584, 'learning_rate': 9.955744379536201e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8258, 'grad_norm': 0.2537493407726288, 'learning_rate': 9.952203929899098e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8209, 'grad_norm': 0.23592710494995117, 'learning_rate': 9.948663480261994e-05, 'epoch': 0.5}\n",
      "{'loss': 0.9612, 'grad_norm': 0.27417129278182983, 'learning_rate': 9.94512303062489e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6457, 'grad_norm': 0.20359955728054047, 'learning_rate': 9.941582580987786e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5624, 'grad_norm': 0.27266114950180054, 'learning_rate': 9.938042131350683e-05, 'epoch': 0.5}\n",
      "{'loss': 1.0412, 'grad_norm': 0.2582342326641083, 'learning_rate': 9.934501681713578e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5592, 'grad_norm': 0.23981767892837524, 'learning_rate': 9.930961232076474e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6558, 'grad_norm': 0.23292425274848938, 'learning_rate': 9.92742078243937e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5292, 'grad_norm': 0.2159758359193802, 'learning_rate': 9.923880332802266e-05, 'epoch': 0.5}\n",
      "{'loss': 0.6001, 'grad_norm': 0.22895869612693787, 'learning_rate': 9.920339883165163e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5404, 'grad_norm': 0.18355602025985718, 'learning_rate': 9.916799433528058e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8288, 'grad_norm': 0.26285791397094727, 'learning_rate': 9.913258983890955e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8493, 'grad_norm': 0.228184774518013, 'learning_rate': 9.909718534253851e-05, 'epoch': 0.5}\n",
      "{'loss': 0.7706, 'grad_norm': 0.2228291928768158, 'learning_rate': 9.906178084616748e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7101, 'grad_norm': 0.23776650428771973, 'learning_rate': 9.902637634979643e-05, 'epoch': 0.51}\n",
      "{'loss': 1.1393, 'grad_norm': 0.22261713445186615, 'learning_rate': 9.899097185342539e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7222, 'grad_norm': 0.27607375383377075, 'learning_rate': 9.895556735705434e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8152, 'grad_norm': 0.24903397262096405, 'learning_rate': 9.892016286068331e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6087, 'grad_norm': 0.21961291134357452, 'learning_rate': 9.888475836431226e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8311, 'grad_norm': 0.2367069125175476, 'learning_rate': 9.884935386794123e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6649, 'grad_norm': 0.1766025573015213, 'learning_rate': 9.881394937157019e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7369, 'grad_norm': 0.27947500348091125, 'learning_rate': 9.877854487519915e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8289, 'grad_norm': 0.20225894451141357, 'learning_rate': 9.874314037882811e-05, 'epoch': 0.51}\n",
      "{'loss': 0.72, 'grad_norm': 0.21647830307483673, 'learning_rate': 9.870773588245708e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8192, 'grad_norm': 0.2712591886520386, 'learning_rate': 9.867233138608603e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6045, 'grad_norm': 0.18780700862407684, 'learning_rate': 9.8636926889715e-05, 'epoch': 0.51}\n",
      "{'loss': 0.61, 'grad_norm': 0.17480576038360596, 'learning_rate': 9.860152239334396e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7637, 'grad_norm': 0.21189047396183014, 'learning_rate': 9.856611789697292e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5903, 'grad_norm': 0.21097703278064728, 'learning_rate': 9.853071340060188e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6564, 'grad_norm': 0.20460852980613708, 'learning_rate': 9.849530890423085e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6565, 'grad_norm': 0.26146289706230164, 'learning_rate': 9.84599044078598e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8511, 'grad_norm': 0.34289634227752686, 'learning_rate': 9.842449991148877e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5707, 'grad_norm': 0.21125392615795135, 'learning_rate': 9.838909541511772e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5817, 'grad_norm': 0.1829487383365631, 'learning_rate': 9.835369091874669e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9642, 'grad_norm': 0.3840232789516449, 'learning_rate': 9.831828642237565e-05, 'epoch': 0.51}\n",
      "{'loss': 0.4278, 'grad_norm': 0.15807054936885834, 'learning_rate': 9.828288192600462e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6866, 'grad_norm': 0.19778291881084442, 'learning_rate': 9.824747742963357e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9823, 'grad_norm': 0.2730824947357178, 'learning_rate': 9.821207293326254e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5683, 'grad_norm': 0.17944538593292236, 'learning_rate': 9.81766684368915e-05, 'epoch': 0.51}\n",
      "{'loss': 0.853, 'grad_norm': 0.21981410682201385, 'learning_rate': 9.814126394052046e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5096, 'grad_norm': 0.17806126177310944, 'learning_rate': 9.810585944414942e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5539, 'grad_norm': 0.24065907299518585, 'learning_rate': 9.807045494777837e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7998, 'grad_norm': 0.24922999739646912, 'learning_rate': 9.803505045140733e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6174, 'grad_norm': 0.19938504695892334, 'learning_rate': 9.79996459550363e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6932, 'grad_norm': 0.2839444875717163, 'learning_rate': 9.796424145866525e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8605, 'grad_norm': 0.22323492169380188, 'learning_rate': 9.792883696229422e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6068, 'grad_norm': 0.17182417213916779, 'learning_rate': 9.789343246592317e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8556, 'grad_norm': 0.2919783890247345, 'learning_rate': 9.785802796955214e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6027, 'grad_norm': 0.1946810930967331, 'learning_rate': 9.78226234731811e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8451, 'grad_norm': 0.2177397757768631, 'learning_rate': 9.778721897681006e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9846, 'grad_norm': 0.2899777889251709, 'learning_rate': 9.775181448043902e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5941, 'grad_norm': 0.2351548820734024, 'learning_rate': 9.771640998406799e-05, 'epoch': 0.51}\n",
      "{'loss': 0.626, 'grad_norm': 0.24365201592445374, 'learning_rate': 9.768100548769694e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7808, 'grad_norm': 0.27188146114349365, 'learning_rate': 9.76456009913259e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6071, 'grad_norm': 0.17529670894145966, 'learning_rate': 9.761019649495486e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8261, 'grad_norm': 0.2488546073436737, 'learning_rate': 9.757479199858382e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6808, 'grad_norm': 0.22244617342948914, 'learning_rate': 9.753938750221279e-05, 'epoch': 0.51}\n",
      "{'loss': 0.641, 'grad_norm': 0.196712926030159, 'learning_rate': 9.750398300584174e-05, 'epoch': 0.51}\n",
      "{'loss': 1.2059, 'grad_norm': 0.25529053807258606, 'learning_rate': 9.746857850947071e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6717, 'grad_norm': 0.25614699721336365, 'learning_rate': 9.743317401309967e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8339, 'grad_norm': 0.1759108304977417, 'learning_rate': 9.739776951672863e-05, 'epoch': 0.51}\n",
      "{'loss': 0.6423, 'grad_norm': 0.21800605952739716, 'learning_rate': 9.736236502035759e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8347, 'grad_norm': 0.26058900356292725, 'learning_rate': 9.732696052398656e-05, 'epoch': 0.51}\n",
      "{'loss': 0.505, 'grad_norm': 0.23366816341876984, 'learning_rate': 9.729155602761551e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8251, 'grad_norm': 0.2360956072807312, 'learning_rate': 9.725615153124448e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7199, 'grad_norm': 0.26286953687667847, 'learning_rate': 9.722074703487344e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7388, 'grad_norm': 0.18786880373954773, 'learning_rate': 9.71853425385024e-05, 'epoch': 0.51}\n",
      "{'loss': 0.773, 'grad_norm': 0.2070777714252472, 'learning_rate': 9.714993804213134e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5824, 'grad_norm': 0.16983245313167572, 'learning_rate': 9.711453354576031e-05, 'epoch': 0.51}\n",
      "{'loss': 0.7185, 'grad_norm': 0.2578360140323639, 'learning_rate': 9.707912904938927e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7734, 'grad_norm': 0.28032559156417847, 'learning_rate': 9.704372455301824e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7224, 'grad_norm': 0.20189350843429565, 'learning_rate': 9.700832005664719e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9265, 'grad_norm': 0.27809834480285645, 'learning_rate': 9.697291556027616e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6933, 'grad_norm': 0.23296117782592773, 'learning_rate': 9.693751106390511e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8381, 'grad_norm': 0.2555195093154907, 'learning_rate': 9.690210656753408e-05, 'epoch': 0.52}\n",
      "{'loss': 0.982, 'grad_norm': 0.2934943437576294, 'learning_rate': 9.686670207116304e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5822, 'grad_norm': 0.19650690257549286, 'learning_rate': 9.6831297574792e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4934, 'grad_norm': 0.20838773250579834, 'learning_rate': 9.679589307842096e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8424, 'grad_norm': 0.2312794029712677, 'learning_rate': 9.676048858204993e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5513, 'grad_norm': 0.18803541362285614, 'learning_rate': 9.672508408567888e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6082, 'grad_norm': 0.21345758438110352, 'learning_rate': 9.668967958930785e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7216, 'grad_norm': 0.2396208643913269, 'learning_rate': 9.66542750929368e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5024, 'grad_norm': 0.25165343284606934, 'learning_rate': 9.661887059656577e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7307, 'grad_norm': 0.18900156021118164, 'learning_rate': 9.658346610019473e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7125, 'grad_norm': 0.2603086233139038, 'learning_rate': 9.65480616038237e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5983, 'grad_norm': 0.2062697410583496, 'learning_rate': 9.651265710745265e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6462, 'grad_norm': 0.22213123738765717, 'learning_rate': 9.647725261108162e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8425, 'grad_norm': 0.2035239338874817, 'learning_rate': 9.644184811471058e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8194, 'grad_norm': 0.19534538686275482, 'learning_rate': 9.640644361833954e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6596, 'grad_norm': 0.23986288905143738, 'learning_rate': 9.63710391219685e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6013, 'grad_norm': 0.18974144756793976, 'learning_rate': 9.633563462559745e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7775, 'grad_norm': 0.24060596525669098, 'learning_rate': 9.630023012922642e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5535, 'grad_norm': 0.20057635009288788, 'learning_rate': 9.626482563285538e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6196, 'grad_norm': 0.23184634745121002, 'learning_rate': 9.622942113648434e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7719, 'grad_norm': 0.2731238603591919, 'learning_rate': 9.61940166401133e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7019, 'grad_norm': 0.2554132640361786, 'learning_rate': 9.615861214374225e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8368, 'grad_norm': 0.28982576727867126, 'learning_rate': 9.612320764737122e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8864, 'grad_norm': 0.3340909779071808, 'learning_rate': 9.608780315100018e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6075, 'grad_norm': 0.24766668677330017, 'learning_rate': 9.605239865462913e-05, 'epoch': 0.52}\n",
      "{'loss': 1.1268, 'grad_norm': 0.24496354162693024, 'learning_rate': 9.60169941582581e-05, 'epoch': 0.52}\n",
      "{'loss': 1.2143, 'grad_norm': 0.3021003305912018, 'learning_rate': 9.598158966188706e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0423, 'grad_norm': 0.24634382128715515, 'learning_rate': 9.594618516551602e-05, 'epoch': 0.52}\n",
      "{'loss': 0.9725, 'grad_norm': 0.25278714299201965, 'learning_rate': 9.591078066914498e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5267, 'grad_norm': 0.23057790100574493, 'learning_rate': 9.587537617277395e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5763, 'grad_norm': 0.2395452857017517, 'learning_rate': 9.58399716764029e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6013, 'grad_norm': 0.23295362293720245, 'learning_rate': 9.580456718003187e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8551, 'grad_norm': 0.24476750195026398, 'learning_rate': 9.576916268366082e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7136, 'grad_norm': 0.21788179874420166, 'learning_rate': 9.573375818728979e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7935, 'grad_norm': 0.2171146422624588, 'learning_rate': 9.569835369091875e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5999, 'grad_norm': 0.2244732677936554, 'learning_rate': 9.566294919454772e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4886, 'grad_norm': 0.1873776912689209, 'learning_rate': 9.562754469817667e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7253, 'grad_norm': 0.24184618890285492, 'learning_rate': 9.559214020180564e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8134, 'grad_norm': 0.2035464197397232, 'learning_rate': 9.55567357054346e-05, 'epoch': 0.52}\n",
      "{'loss': 0.8569, 'grad_norm': 0.22526074945926666, 'learning_rate': 9.552133120906356e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7632, 'grad_norm': 0.17385311424732208, 'learning_rate': 9.548592671269252e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6872, 'grad_norm': 0.2182617485523224, 'learning_rate': 9.545052221632149e-05, 'epoch': 0.52}\n",
      "{'loss': 1.0687, 'grad_norm': 0.33531251549720764, 'learning_rate': 9.541511771995044e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7413, 'grad_norm': 0.27794933319091797, 'learning_rate': 9.537971322357941e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7393, 'grad_norm': 0.252032071352005, 'learning_rate': 9.534430872720836e-05, 'epoch': 0.52}\n",
      "{'loss': 0.509, 'grad_norm': 0.20320196449756622, 'learning_rate': 9.530890423083733e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7996, 'grad_norm': 0.2748419940471649, 'learning_rate': 9.527349973446629e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5216, 'grad_norm': 0.22084064781665802, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6743, 'grad_norm': 0.267113596200943, 'learning_rate': 9.52026907417242e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7036, 'grad_norm': 0.23777355253696442, 'learning_rate': 9.516728624535316e-05, 'epoch': 0.52}\n",
      "{'loss': 0.7772, 'grad_norm': 0.29268521070480347, 'learning_rate': 9.513188174898212e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6895, 'grad_norm': 0.19645333290100098, 'learning_rate': 9.509647725261109e-05, 'epoch': 0.52}\n",
      "{'loss': 0.6531, 'grad_norm': 0.26049044728279114, 'learning_rate': 9.506107275624004e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6671, 'grad_norm': 0.21828776597976685, 'learning_rate': 9.502566825986901e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8208, 'grad_norm': 0.24048663675785065, 'learning_rate': 9.499026376349796e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6972, 'grad_norm': 0.2516842186450958, 'learning_rate': 9.495485926712693e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8119, 'grad_norm': 0.28080397844314575, 'learning_rate': 9.491945477075589e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5922, 'grad_norm': 0.20956842601299286, 'learning_rate': 9.488405027438486e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8438, 'grad_norm': 0.26027315855026245, 'learning_rate': 9.484864577801381e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8527, 'grad_norm': 0.2964176833629608, 'learning_rate': 9.481324128164278e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6932, 'grad_norm': 0.24541261792182922, 'learning_rate': 9.477783678527173e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6084, 'grad_norm': 0.2524184584617615, 'learning_rate': 9.474243228890069e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8899, 'grad_norm': 0.2472061961889267, 'learning_rate': 9.470702779252966e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0732, 'grad_norm': 0.2957558035850525, 'learning_rate': 9.467162329615861e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5853, 'grad_norm': 0.21713177859783173, 'learning_rate': 9.463621879978758e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5337, 'grad_norm': 0.23664817214012146, 'learning_rate': 9.460081430341653e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7491, 'grad_norm': 0.238261416554451, 'learning_rate': 9.45654098070455e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3576, 'grad_norm': 0.35040754079818726, 'learning_rate': 9.453000531067446e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7302, 'grad_norm': 0.21908380091190338, 'learning_rate': 9.449460081430343e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7198, 'grad_norm': 0.2493165284395218, 'learning_rate': 9.445919631793238e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7831, 'grad_norm': 0.24517904222011566, 'learning_rate': 9.442379182156135e-05, 'epoch': 0.53}\n",
      "{'loss': 1.0989, 'grad_norm': 0.32294347882270813, 'learning_rate': 9.43883873251903e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9004, 'grad_norm': 0.20479369163513184, 'learning_rate': 9.435298282881927e-05, 'epoch': 0.53}\n",
      "{'loss': 1.1305, 'grad_norm': 0.2985992431640625, 'learning_rate': 9.431757833244823e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7596, 'grad_norm': 0.24942712485790253, 'learning_rate': 9.428217383607718e-05, 'epoch': 0.53}\n",
      "{'loss': 0.431, 'grad_norm': 0.1506240963935852, 'learning_rate': 9.424676933970614e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6481, 'grad_norm': 0.18810398876667023, 'learning_rate': 9.42113648433351e-05, 'epoch': 0.53}\n",
      "{'loss': 0.803, 'grad_norm': 0.2277001589536667, 'learning_rate': 9.417596034696406e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8132, 'grad_norm': 0.274364709854126, 'learning_rate': 9.414055585059303e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8113, 'grad_norm': 0.24359937012195587, 'learning_rate': 9.410515135422198e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6095, 'grad_norm': 0.1876898854970932, 'learning_rate': 9.406974685785095e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8193, 'grad_norm': 0.217010959982872, 'learning_rate': 9.40343423614799e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6162, 'grad_norm': 0.21624024212360382, 'learning_rate': 9.399893786510887e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9823, 'grad_norm': 0.36730432510375977, 'learning_rate': 9.396353336873783e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee378629e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: a4f32298-94e8-4ca3-a08c-89aa5e58ea09)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6128, 'grad_norm': 0.2251749038696289, 'learning_rate': 9.39281288723668e-05, 'epoch': 0.53}\n",
      "{'loss': 0.887, 'grad_norm': 0.2277165949344635, 'learning_rate': 9.389272437599575e-05, 'epoch': 0.53}\n",
      "{'loss': 0.4767, 'grad_norm': 0.17773526906967163, 'learning_rate': 9.385731987962472e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5515, 'grad_norm': 0.205086812376976, 'learning_rate': 9.382191538325368e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7033, 'grad_norm': 0.20704199373722076, 'learning_rate': 9.378651088688264e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6033, 'grad_norm': 0.25107741355895996, 'learning_rate': 9.37511063905116e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5945, 'grad_norm': 0.24195027351379395, 'learning_rate': 9.371570189414057e-05, 'epoch': 0.53}\n",
      "{'loss': 0.818, 'grad_norm': 0.2949829399585724, 'learning_rate': 9.368029739776952e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8531, 'grad_norm': 0.23077252507209778, 'learning_rate': 9.364489290139849e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7484, 'grad_norm': 0.2248276025056839, 'learning_rate': 9.360948840502744e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7755, 'grad_norm': 0.2242237776517868, 'learning_rate': 9.357408390865641e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7987, 'grad_norm': 0.2149679809808731, 'learning_rate': 9.353867941228537e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5554, 'grad_norm': 0.22128477692604065, 'learning_rate': 9.350327491591434e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5064, 'grad_norm': 0.20657417178153992, 'learning_rate': 9.346787041954329e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7841, 'grad_norm': 0.22013942897319794, 'learning_rate': 9.343246592317225e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8351, 'grad_norm': 0.2920951545238495, 'learning_rate': 9.339706142680121e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6485, 'grad_norm': 0.24275562167167664, 'learning_rate': 9.336165693043017e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5555, 'grad_norm': 0.18055397272109985, 'learning_rate': 9.332625243405912e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5478, 'grad_norm': 0.18105342984199524, 'learning_rate': 9.329084793768809e-05, 'epoch': 0.53}\n",
      "{'loss': 0.7304, 'grad_norm': 0.2249317616224289, 'learning_rate': 9.325544344131705e-05, 'epoch': 0.53}\n",
      "{'loss': 0.9316, 'grad_norm': 0.20710694789886475, 'learning_rate': 9.322003894494601e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5978, 'grad_norm': 0.21136528253555298, 'learning_rate': 9.318463444857497e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8356, 'grad_norm': 0.2607724964618683, 'learning_rate': 9.314922995220394e-05, 'epoch': 0.53}\n",
      "{'loss': 0.8942, 'grad_norm': 0.29937905073165894, 'learning_rate': 9.311382545583289e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6822, 'grad_norm': 0.19427303969860077, 'learning_rate': 9.307842095946185e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6551, 'grad_norm': 0.2327050119638443, 'learning_rate': 9.304301646309082e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6012, 'grad_norm': 0.27525144815444946, 'learning_rate': 9.300761196671977e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7222, 'grad_norm': 0.24339710175991058, 'learning_rate': 9.297220747034874e-05, 'epoch': 0.54}\n",
      "{'loss': 1.1187, 'grad_norm': 0.2814422845840454, 'learning_rate': 9.29368029739777e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6053, 'grad_norm': 0.18104983866214752, 'learning_rate': 9.290139847760666e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5557, 'grad_norm': 0.1646256446838379, 'learning_rate': 9.286599398123562e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8222, 'grad_norm': 0.2154139131307602, 'learning_rate': 9.283058948486458e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5801, 'grad_norm': 0.20623457431793213, 'learning_rate': 9.279518498849354e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6336, 'grad_norm': 0.23484081029891968, 'learning_rate': 9.275978049212251e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6807, 'grad_norm': 0.2287110835313797, 'learning_rate': 9.272437599575146e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6913, 'grad_norm': 0.2607404589653015, 'learning_rate': 9.268897149938043e-05, 'epoch': 0.54}\n",
      "{'loss': 0.59, 'grad_norm': 0.2215244621038437, 'learning_rate': 9.265356700300939e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6604, 'grad_norm': 0.17280586063861847, 'learning_rate': 9.261816250663835e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6785, 'grad_norm': 0.19511783123016357, 'learning_rate': 9.258275801026731e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6812, 'grad_norm': 0.19852207601070404, 'learning_rate': 9.254735351389628e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5352, 'grad_norm': 0.2640807628631592, 'learning_rate': 9.251194901752523e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6259, 'grad_norm': 0.27106183767318726, 'learning_rate': 9.24765445211542e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6269, 'grad_norm': 0.25404226779937744, 'learning_rate': 9.244114002478316e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7968, 'grad_norm': 0.19357217848300934, 'learning_rate': 9.240573552841211e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7393, 'grad_norm': 0.27531781792640686, 'learning_rate': 9.237033103204106e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6865, 'grad_norm': 0.21115948259830475, 'learning_rate': 9.233492653567003e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8573, 'grad_norm': 0.41959527134895325, 'learning_rate': 9.229952203929899e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7793, 'grad_norm': 0.2320520579814911, 'learning_rate': 9.226411754292796e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6577, 'grad_norm': 0.22476966679096222, 'learning_rate': 9.222871304655691e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5661, 'grad_norm': 0.2517145574092865, 'learning_rate': 9.219330855018588e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5774, 'grad_norm': 0.20207229256629944, 'learning_rate': 9.215790405381483e-05, 'epoch': 0.54}\n",
      "{'loss': 0.619, 'grad_norm': 0.23719751834869385, 'learning_rate': 9.21224995574438e-05, 'epoch': 0.54}\n",
      "{'loss': 0.4193, 'grad_norm': 0.24463334679603577, 'learning_rate': 9.208709506107276e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9853, 'grad_norm': 0.26380062103271484, 'learning_rate': 9.205169056470173e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6298, 'grad_norm': 0.16216056048870087, 'learning_rate': 9.201628606833068e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8848, 'grad_norm': 0.31468498706817627, 'learning_rate': 9.198088157195965e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7634, 'grad_norm': 0.2729904055595398, 'learning_rate': 9.19454770755886e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8182, 'grad_norm': 0.37011992931365967, 'learning_rate': 9.191007257921757e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6683, 'grad_norm': 0.24934644997119904, 'learning_rate': 9.187466808284653e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5834, 'grad_norm': 0.28637969493865967, 'learning_rate': 9.18392635864755e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6856, 'grad_norm': 0.3306852877140045, 'learning_rate': 9.180385909010445e-05, 'epoch': 0.54}\n",
      "{'loss': 0.663, 'grad_norm': 0.18942441046237946, 'learning_rate': 9.17684545937334e-05, 'epoch': 0.54}\n",
      "{'loss': 1.082, 'grad_norm': 0.3057103753089905, 'learning_rate': 9.173305009736237e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6653, 'grad_norm': 0.18801791965961456, 'learning_rate': 9.169764560099133e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9252, 'grad_norm': 0.24738535284996033, 'learning_rate': 9.16622411046203e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5305, 'grad_norm': 0.20846439898014069, 'learning_rate': 9.162683660824925e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8579, 'grad_norm': 0.26997092366218567, 'learning_rate': 9.159143211187822e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7641, 'grad_norm': 0.23126636445522308, 'learning_rate': 9.155602761550717e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7682, 'grad_norm': 0.26487448811531067, 'learning_rate': 9.152062311913614e-05, 'epoch': 0.54}\n",
      "{'loss': 0.8485, 'grad_norm': 0.20959153771400452, 'learning_rate': 9.14852186227651e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5698, 'grad_norm': 0.2253732979297638, 'learning_rate': 9.144981412639405e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7752, 'grad_norm': 0.28220054507255554, 'learning_rate': 9.1414409630023e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6244, 'grad_norm': 0.23070530593395233, 'learning_rate': 9.137900513365197e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9198, 'grad_norm': 0.26819247007369995, 'learning_rate': 9.134360063728093e-05, 'epoch': 0.54}\n",
      "{'loss': 0.9011, 'grad_norm': 0.24292509257793427, 'learning_rate': 9.13081961409099e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7141, 'grad_norm': 0.2251097410917282, 'learning_rate': 9.127279164453885e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5285, 'grad_norm': 0.23175789415836334, 'learning_rate': 9.123738714816782e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7608, 'grad_norm': 0.21918755769729614, 'learning_rate': 9.120198265179678e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7294, 'grad_norm': 0.40230292081832886, 'learning_rate': 9.116657815542574e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7275, 'grad_norm': 0.26576483249664307, 'learning_rate': 9.11311736590547e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7872, 'grad_norm': 0.1805725395679474, 'learning_rate': 9.109576916268367e-05, 'epoch': 0.54}\n",
      "{'loss': 0.6216, 'grad_norm': 0.21638034284114838, 'learning_rate': 9.106036466631262e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6955, 'grad_norm': 0.27093297243118286, 'learning_rate': 9.102496016994159e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9943, 'grad_norm': 0.18803541362285614, 'learning_rate': 9.098955567357054e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9039, 'grad_norm': 0.2749313414096832, 'learning_rate': 9.095415117719951e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7866, 'grad_norm': 0.2585567533969879, 'learning_rate': 9.091874668082847e-05, 'epoch': 0.55}\n",
      "{'loss': 0.789, 'grad_norm': 0.23309163749217987, 'learning_rate': 9.088334218445744e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5516, 'grad_norm': 0.23798631131649017, 'learning_rate': 9.084793768808639e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6029, 'grad_norm': 0.20862971246242523, 'learning_rate': 9.081253319171536e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6514, 'grad_norm': 0.25958091020584106, 'learning_rate': 9.077712869534431e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6728, 'grad_norm': 0.20209923386573792, 'learning_rate': 9.074172419897328e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6494, 'grad_norm': 0.3313860595226288, 'learning_rate': 9.070631970260224e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8421, 'grad_norm': 0.2362065315246582, 'learning_rate': 9.06709152062312e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5709, 'grad_norm': 0.1640893518924713, 'learning_rate': 9.063551070986016e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5915, 'grad_norm': 0.21273398399353027, 'learning_rate': 9.060010621348913e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9453, 'grad_norm': 0.23586426675319672, 'learning_rate': 9.056470171711808e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1737, 'grad_norm': 0.2543069124221802, 'learning_rate': 9.052929722074705e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6849, 'grad_norm': 0.24331285059452057, 'learning_rate': 9.049389272437599e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7553, 'grad_norm': 0.25017741322517395, 'learning_rate': 9.045848822800496e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9261, 'grad_norm': 0.185777947306633, 'learning_rate': 9.042308373163392e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0377, 'grad_norm': 0.21818378567695618, 'learning_rate': 9.038767923526288e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8271, 'grad_norm': 0.24519146978855133, 'learning_rate': 9.035227473889184e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5129, 'grad_norm': 0.22075392305850983, 'learning_rate': 9.031687024252081e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5965, 'grad_norm': 0.22834442555904388, 'learning_rate': 9.028146574614976e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5382, 'grad_norm': 0.1913730949163437, 'learning_rate': 9.024606124977873e-05, 'epoch': 0.55}\n",
      "{'loss': 0.671, 'grad_norm': 0.22015191614627838, 'learning_rate': 9.021065675340768e-05, 'epoch': 0.55}\n",
      "{'loss': 0.4696, 'grad_norm': 0.2042367160320282, 'learning_rate': 9.017525225703665e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7753, 'grad_norm': 0.2362639158964157, 'learning_rate': 9.013984776066561e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7853, 'grad_norm': 0.24975161254405975, 'learning_rate': 9.010444326429456e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8905, 'grad_norm': 0.2191658914089203, 'learning_rate': 9.006903876792353e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7661, 'grad_norm': 0.23680631816387177, 'learning_rate': 9.003363427155249e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5801, 'grad_norm': 0.21595515310764313, 'learning_rate': 8.999822977518145e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9601, 'grad_norm': 0.23517973721027374, 'learning_rate': 8.996282527881041e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7311, 'grad_norm': 0.22522425651550293, 'learning_rate': 8.992742078243938e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7356, 'grad_norm': 0.280219703912735, 'learning_rate': 8.989201628606833e-05, 'epoch': 0.55}\n",
      "{'loss': 1.1318, 'grad_norm': 0.2774319350719452, 'learning_rate': 8.98566117896973e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7061, 'grad_norm': 0.264864444732666, 'learning_rate': 8.982120729332626e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9662, 'grad_norm': 0.26636117696762085, 'learning_rate': 8.978580279695522e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8659, 'grad_norm': 0.29005202651023865, 'learning_rate': 8.975039830058418e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6997, 'grad_norm': 0.2154828906059265, 'learning_rate': 8.971499380421315e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6067, 'grad_norm': 0.20269666612148285, 'learning_rate': 8.96795893078421e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7179, 'grad_norm': 0.3254069983959198, 'learning_rate': 8.964418481147107e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6593, 'grad_norm': 0.2326831817626953, 'learning_rate': 8.960878031510002e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8533, 'grad_norm': 0.24874906241893768, 'learning_rate': 8.957337581872898e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8417, 'grad_norm': 0.271994948387146, 'learning_rate': 8.953797132235793e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0195, 'grad_norm': 0.42770248651504517, 'learning_rate': 8.95025668259869e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7609, 'grad_norm': 0.285230427980423, 'learning_rate': 8.946716232961586e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8407, 'grad_norm': 0.2197207808494568, 'learning_rate': 8.943175783324483e-05, 'epoch': 0.55}\n",
      "{'loss': 1.0196, 'grad_norm': 0.21311822533607483, 'learning_rate': 8.939635333687378e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7547, 'grad_norm': 0.177506223320961, 'learning_rate': 8.936094884050275e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7422, 'grad_norm': 0.27206921577453613, 'learning_rate': 8.93255443441317e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6517, 'grad_norm': 0.20970728993415833, 'learning_rate': 8.929013984776067e-05, 'epoch': 0.55}\n",
      "{'loss': 0.6477, 'grad_norm': 0.2529781460762024, 'learning_rate': 8.925473535138963e-05, 'epoch': 0.55}\n",
      "{'loss': 0.8519, 'grad_norm': 0.24170471727848053, 'learning_rate': 8.92193308550186e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5546, 'grad_norm': 0.18048010766506195, 'learning_rate': 8.918392635864755e-05, 'epoch': 0.55}\n",
      "{'loss': 0.9147, 'grad_norm': 0.281197726726532, 'learning_rate': 8.914852186227652e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5949, 'grad_norm': 0.22778081893920898, 'learning_rate': 8.911311736590547e-05, 'epoch': 0.55}\n",
      "{'loss': 0.7932, 'grad_norm': 0.19713780283927917, 'learning_rate': 8.907771286953444e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7567, 'grad_norm': 0.20935264229774475, 'learning_rate': 8.90423083731634e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5343, 'grad_norm': 0.19217807054519653, 'learning_rate': 8.900690387679236e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9388, 'grad_norm': 0.23973926901817322, 'learning_rate': 8.897149938042132e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6868, 'grad_norm': 0.32870417833328247, 'learning_rate': 8.893609488405029e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5977, 'grad_norm': 0.2551778256893158, 'learning_rate': 8.890069038767924e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5767, 'grad_norm': 0.2125936895608902, 'learning_rate': 8.886528589130821e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6012, 'grad_norm': 0.20541927218437195, 'learning_rate': 8.882988139493716e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0338, 'grad_norm': 0.2463982254266739, 'learning_rate': 8.879447689856612e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5879, 'grad_norm': 0.23587101697921753, 'learning_rate': 8.875907240219509e-05, 'epoch': 0.56}\n",
      "{'loss': 0.684, 'grad_norm': 0.2161787450313568, 'learning_rate': 8.872366790582404e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8291, 'grad_norm': 0.2824951410293579, 'learning_rate': 8.868826340945301e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7013, 'grad_norm': 0.30900639295578003, 'learning_rate': 8.865285891308197e-05, 'epoch': 0.56}\n",
      "{'loss': 0.975, 'grad_norm': 0.22573725879192352, 'learning_rate': 8.861745441671092e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8273, 'grad_norm': 0.23164908587932587, 'learning_rate': 8.858204992033989e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9016, 'grad_norm': 0.23704096674919128, 'learning_rate': 8.854664542396884e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9633, 'grad_norm': 0.33735403418540955, 'learning_rate': 8.85112409275978e-05, 'epoch': 0.56}\n",
      "{'loss': 1.135, 'grad_norm': 0.2754804193973541, 'learning_rate': 8.847583643122677e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6407, 'grad_norm': 0.24815145134925842, 'learning_rate': 8.844043193485572e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7122, 'grad_norm': 0.30428144335746765, 'learning_rate': 8.840502743848469e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6937, 'grad_norm': 0.2129136025905609, 'learning_rate': 8.836962294211364e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6905, 'grad_norm': 0.304021954536438, 'learning_rate': 8.833421844574261e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7775, 'grad_norm': 0.2657826244831085, 'learning_rate': 8.829881394937157e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7052, 'grad_norm': 0.20716625452041626, 'learning_rate': 8.826340945300054e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5046, 'grad_norm': 0.19383056461811066, 'learning_rate': 8.822800495662949e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6458, 'grad_norm': 0.19017964601516724, 'learning_rate': 8.819260046025846e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6583, 'grad_norm': 0.23935310542583466, 'learning_rate': 8.815719596388741e-05, 'epoch': 0.56}\n",
      "{'loss': 0.924, 'grad_norm': 0.22196407616138458, 'learning_rate': 8.812179146751638e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7047, 'grad_norm': 0.20853033661842346, 'learning_rate': 8.808638697114534e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6618, 'grad_norm': 0.1957671046257019, 'learning_rate': 8.80509824747743e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6734, 'grad_norm': 0.19805848598480225, 'learning_rate': 8.801557797840326e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4711, 'grad_norm': 0.16287240386009216, 'learning_rate': 8.798017348203223e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1406, 'grad_norm': 0.24527563154697418, 'learning_rate': 8.794476898566118e-05, 'epoch': 0.56}\n",
      "{'loss': 0.71, 'grad_norm': 0.22478638589382172, 'learning_rate': 8.790936448929015e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4883, 'grad_norm': 0.15535181760787964, 'learning_rate': 8.78739599929191e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8024, 'grad_norm': 0.21057835221290588, 'learning_rate': 8.783855549654807e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6824, 'grad_norm': 0.21252264082431793, 'learning_rate': 8.780315100017703e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7692, 'grad_norm': 0.22285276651382446, 'learning_rate': 8.7767746503806e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8048, 'grad_norm': 0.2888636291027069, 'learning_rate': 8.773234200743495e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9018, 'grad_norm': 0.2864408493041992, 'learning_rate': 8.769693751106392e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6077, 'grad_norm': 0.21218210458755493, 'learning_rate': 8.766153301469286e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8376, 'grad_norm': 0.2842850685119629, 'learning_rate': 8.762612851832183e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6636, 'grad_norm': 0.19218051433563232, 'learning_rate': 8.759072402195078e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8092, 'grad_norm': 0.18957304954528809, 'learning_rate': 8.755531952557975e-05, 'epoch': 0.56}\n",
      "{'loss': 0.9186, 'grad_norm': 0.2604794204235077, 'learning_rate': 8.751991502920871e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7883, 'grad_norm': 0.1844332218170166, 'learning_rate': 8.748451053283768e-05, 'epoch': 0.56}\n",
      "{'loss': 0.6397, 'grad_norm': 0.2649633586406708, 'learning_rate': 8.744910603646663e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7017, 'grad_norm': 0.23422543704509735, 'learning_rate': 8.74137015400956e-05, 'epoch': 0.56}\n",
      "{'loss': 0.7418, 'grad_norm': 0.23034356534481049, 'learning_rate': 8.737829704372455e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8246, 'grad_norm': 0.20468510687351227, 'learning_rate': 8.734289254735352e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5468, 'grad_norm': 0.16205266118049622, 'learning_rate': 8.730748805098248e-05, 'epoch': 0.56}\n",
      "{'loss': 0.529, 'grad_norm': 0.20699086785316467, 'learning_rate': 8.727208355461145e-05, 'epoch': 0.56}\n",
      "{'loss': 0.831, 'grad_norm': 0.25938308238983154, 'learning_rate': 8.72366790582404e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8103, 'grad_norm': 0.22442543506622314, 'learning_rate': 8.720127456186935e-05, 'epoch': 0.56}\n",
      "{'loss': 0.849, 'grad_norm': 0.2711181938648224, 'learning_rate': 8.716587006549832e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5306, 'grad_norm': 0.23873791098594666, 'learning_rate': 8.713046556912728e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5811, 'grad_norm': 0.19919884204864502, 'learning_rate': 8.709506107275625e-05, 'epoch': 0.56}\n",
      "{'loss': 1.0582, 'grad_norm': 0.30385154485702515, 'learning_rate': 8.70596565763852e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7627, 'grad_norm': 0.26054471731185913, 'learning_rate': 8.702425208001417e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9266, 'grad_norm': 0.22693510353565216, 'learning_rate': 8.698884758364312e-05, 'epoch': 0.57}\n",
      "{'loss': 0.69, 'grad_norm': 0.22614611685276031, 'learning_rate': 8.695344308727209e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7918, 'grad_norm': 0.22632691264152527, 'learning_rate': 8.691803859090105e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6695, 'grad_norm': 0.24651744961738586, 'learning_rate': 8.688263409453002e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1237, 'grad_norm': 0.2702997028827667, 'learning_rate': 8.684722959815897e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6708, 'grad_norm': 0.23114845156669617, 'learning_rate': 8.681182510178794e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5788, 'grad_norm': 0.18908251821994781, 'learning_rate': 8.67764206054169e-05, 'epoch': 0.57}\n",
      "{'loss': 0.609, 'grad_norm': 0.20300957560539246, 'learning_rate': 8.674101610904586e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7654, 'grad_norm': 0.2344697117805481, 'learning_rate': 8.67056116126748e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6575, 'grad_norm': 0.2279195785522461, 'learning_rate': 8.667020711630377e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7855, 'grad_norm': 0.24408428370952606, 'learning_rate': 8.663480261993273e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0149, 'grad_norm': 0.26225242018699646, 'learning_rate': 8.65993981235617e-05, 'epoch': 0.57}\n",
      "{'loss': 0.4687, 'grad_norm': 0.15930140018463135, 'learning_rate': 8.656399362719065e-05, 'epoch': 0.57}\n",
      "{'loss': 0.727, 'grad_norm': 0.2536994516849518, 'learning_rate': 8.652858913081962e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5727, 'grad_norm': 0.21197369694709778, 'learning_rate': 8.649318463444857e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6927, 'grad_norm': 0.2685677409172058, 'learning_rate': 8.645778013807754e-05, 'epoch': 0.57}\n",
      "{'loss': 0.66, 'grad_norm': 0.23049141466617584, 'learning_rate': 8.64223756417065e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8043, 'grad_norm': 0.25136926770210266, 'learning_rate': 8.638697114533546e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7922, 'grad_norm': 0.23418967425823212, 'learning_rate': 8.635156664896442e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9153, 'grad_norm': 0.21905231475830078, 'learning_rate': 8.631616215259339e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5665, 'grad_norm': 0.19117771089076996, 'learning_rate': 8.628075765622234e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7727, 'grad_norm': 0.22731941938400269, 'learning_rate': 8.624535315985131e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5796, 'grad_norm': 0.23332549631595612, 'learning_rate': 8.620994866348026e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6167, 'grad_norm': 0.17878170311450958, 'learning_rate': 8.617454416710923e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8878, 'grad_norm': 0.3012081980705261, 'learning_rate': 8.613913967073819e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6815, 'grad_norm': 0.20044754445552826, 'learning_rate': 8.610373517436716e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6746, 'grad_norm': 0.2398742139339447, 'learning_rate': 8.606833067799611e-05, 'epoch': 0.57}\n",
      "{'loss': 0.9327, 'grad_norm': 0.23490627110004425, 'learning_rate': 8.603292618162508e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5942, 'grad_norm': 0.2732807397842407, 'learning_rate': 8.599752168525403e-05, 'epoch': 0.57}\n",
      "{'loss': 1.1844, 'grad_norm': 0.3720495402812958, 'learning_rate': 8.5962117188883e-05, 'epoch': 0.57}\n",
      "{'loss': 1.0937, 'grad_norm': 0.2531834542751312, 'learning_rate': 8.592671269251196e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6111, 'grad_norm': 0.21173864603042603, 'learning_rate': 8.589130819614091e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8166, 'grad_norm': 0.27719593048095703, 'learning_rate': 8.585590369976988e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5077, 'grad_norm': 0.2965852618217468, 'learning_rate': 8.582049920339883e-05, 'epoch': 0.57}\n",
      "{'loss': 0.659, 'grad_norm': 0.20235881209373474, 'learning_rate': 8.57850947070278e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5186, 'grad_norm': 0.24326880276203156, 'learning_rate': 8.574969021065676e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7268, 'grad_norm': 0.2655467689037323, 'learning_rate': 8.571428571428571e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6979, 'grad_norm': 0.2010347992181778, 'learning_rate': 8.567888121791468e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7782, 'grad_norm': 0.23217381536960602, 'learning_rate': 8.564347672154364e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8971, 'grad_norm': 0.26112139225006104, 'learning_rate': 8.56080722251726e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8227, 'grad_norm': 0.1893441528081894, 'learning_rate': 8.557266772880156e-05, 'epoch': 0.57}\n",
      "{'loss': 0.862, 'grad_norm': 0.27042338252067566, 'learning_rate': 8.553726323243051e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7324, 'grad_norm': 0.18336321413516998, 'learning_rate': 8.550185873605948e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8459, 'grad_norm': 0.2789679169654846, 'learning_rate': 8.546645423968844e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7635, 'grad_norm': 0.27590814232826233, 'learning_rate': 8.54310497433174e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7241, 'grad_norm': 0.24216125905513763, 'learning_rate': 8.539564524694636e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6282, 'grad_norm': 0.20023122429847717, 'learning_rate': 8.536024075057533e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8619, 'grad_norm': 0.24201618134975433, 'learning_rate': 8.532483625420428e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8513, 'grad_norm': 0.21710944175720215, 'learning_rate': 8.528943175783325e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6516, 'grad_norm': 0.19494614005088806, 'learning_rate': 8.52540272614622e-05, 'epoch': 0.57}\n",
      "{'loss': 0.5675, 'grad_norm': 0.24860937893390656, 'learning_rate': 8.521862276509117e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6746, 'grad_norm': 0.21040880680084229, 'learning_rate': 8.518321826872013e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7221, 'grad_norm': 0.2098173350095749, 'learning_rate': 8.51478137723491e-05, 'epoch': 0.57}\n",
      "{'loss': 0.833, 'grad_norm': 0.34083789587020874, 'learning_rate': 8.511240927597805e-05, 'epoch': 0.57}\n",
      "{'loss': 0.6308, 'grad_norm': 0.24274089932441711, 'learning_rate': 8.507700477960702e-05, 'epoch': 0.57}\n",
      "{'loss': 0.8984, 'grad_norm': 0.22990164160728455, 'learning_rate': 8.504160028323598e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8516, 'grad_norm': 0.24303318560123444, 'learning_rate': 8.500619578686494e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7646, 'grad_norm': 0.233431875705719, 'learning_rate': 8.49707912904939e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6845, 'grad_norm': 0.24387399852275848, 'learning_rate': 8.493538679412287e-05, 'epoch': 0.58}\n",
      "{'loss': 0.613, 'grad_norm': 0.20414604246616364, 'learning_rate': 8.489998229775182e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6804, 'grad_norm': 0.20809276401996613, 'learning_rate': 8.486457780138079e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6903, 'grad_norm': 0.22109414637088776, 'learning_rate': 8.482917330500973e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8207, 'grad_norm': 0.19224287569522858, 'learning_rate': 8.47937688086387e-05, 'epoch': 0.58}\n",
      "{'loss': 1.1314, 'grad_norm': 0.2867977023124695, 'learning_rate': 8.475836431226765e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6695, 'grad_norm': 0.21954196691513062, 'learning_rate': 8.472295981589662e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9347, 'grad_norm': 0.2352798581123352, 'learning_rate': 8.468755531952558e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6818, 'grad_norm': 0.23930814862251282, 'learning_rate': 8.465215082315455e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5636, 'grad_norm': 0.1852802336215973, 'learning_rate': 8.46167463267835e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7668, 'grad_norm': 0.2647193372249603, 'learning_rate': 8.458134183041247e-05, 'epoch': 0.58}\n",
      "{'loss': 0.81, 'grad_norm': 0.24621842801570892, 'learning_rate': 8.454593733404142e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5703, 'grad_norm': 0.21522989869117737, 'learning_rate': 8.451053283767039e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7595, 'grad_norm': 0.26149725914001465, 'learning_rate': 8.447512834129935e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7067, 'grad_norm': 0.2566995918750763, 'learning_rate': 8.443972384492831e-05, 'epoch': 0.58}\n",
      "{'loss': 0.655, 'grad_norm': 0.21626006066799164, 'learning_rate': 8.440431934855727e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8398, 'grad_norm': 0.20200234651565552, 'learning_rate': 8.436891485218624e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5465, 'grad_norm': 0.24050813913345337, 'learning_rate': 8.433351035581519e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9336, 'grad_norm': 0.24168866872787476, 'learning_rate': 8.429810585944416e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7977, 'grad_norm': 0.2536807060241699, 'learning_rate': 8.426270136307312e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7369, 'grad_norm': 0.20007257163524628, 'learning_rate': 8.422729686670207e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5631, 'grad_norm': 0.22491517663002014, 'learning_rate': 8.419189237033104e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4896, 'grad_norm': 0.28124284744262695, 'learning_rate': 8.415648787396e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6957, 'grad_norm': 0.2295723706483841, 'learning_rate': 8.412108337758896e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8846, 'grad_norm': 0.2588360011577606, 'learning_rate': 8.408567888121792e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4803, 'grad_norm': 0.19853505492210388, 'learning_rate': 8.405027438484688e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6394, 'grad_norm': 0.2083345502614975, 'learning_rate': 8.401486988847584e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7063, 'grad_norm': 0.22532016038894653, 'learning_rate': 8.397946539210481e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8554, 'grad_norm': 0.2616530656814575, 'learning_rate': 8.394406089573376e-05, 'epoch': 0.58}\n",
      "{'loss': 0.529, 'grad_norm': 0.193441241979599, 'learning_rate': 8.390865639936273e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8636, 'grad_norm': 0.23739862442016602, 'learning_rate': 8.387325190299167e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6744, 'grad_norm': 0.21366272866725922, 'learning_rate': 8.383784740662064e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6055, 'grad_norm': 0.21257467567920685, 'learning_rate': 8.38024429102496e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6134, 'grad_norm': 0.2567322254180908, 'learning_rate': 8.376703841387856e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6014, 'grad_norm': 0.21223977208137512, 'learning_rate': 8.373163391750752e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8615, 'grad_norm': 0.2801458239555359, 'learning_rate': 8.369622942113649e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7369, 'grad_norm': 0.23386816680431366, 'learning_rate': 8.366082492476544e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4751, 'grad_norm': 0.19656577706336975, 'learning_rate': 8.362542042839441e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6361, 'grad_norm': 0.17709484696388245, 'learning_rate': 8.359001593202336e-05, 'epoch': 0.58}\n",
      "{'loss': 0.8272, 'grad_norm': 0.22066250443458557, 'learning_rate': 8.355461143565233e-05, 'epoch': 0.58}\n",
      "{'loss': 0.9367, 'grad_norm': 0.239240363240242, 'learning_rate': 8.351920693928129e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4852, 'grad_norm': 0.3993374705314636, 'learning_rate': 8.348380244291026e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6037, 'grad_norm': 0.20106595754623413, 'learning_rate': 8.344839794653921e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6652, 'grad_norm': 0.22953300178050995, 'learning_rate': 8.341299345016818e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7327, 'grad_norm': 0.24761433899402618, 'learning_rate': 8.337758895379713e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6948, 'grad_norm': 0.2437056005001068, 'learning_rate': 8.33421844574261e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5325, 'grad_norm': 0.2820192575454712, 'learning_rate': 8.330677996105506e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7814, 'grad_norm': 0.21970532834529877, 'learning_rate': 8.327137546468403e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7286, 'grad_norm': 0.16900457441806793, 'learning_rate': 8.323597096831298e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7214, 'grad_norm': 0.26168397068977356, 'learning_rate': 8.320056647194195e-05, 'epoch': 0.58}\n",
      "{'loss': 0.5674, 'grad_norm': 0.2047399878501892, 'learning_rate': 8.31651619755709e-05, 'epoch': 0.58}\n",
      "{'loss': 0.732, 'grad_norm': 0.22128455340862274, 'learning_rate': 8.312975747919987e-05, 'epoch': 0.58}\n",
      "{'loss': 0.594, 'grad_norm': 0.19484980404376984, 'learning_rate': 8.309435298282883e-05, 'epoch': 0.58}\n",
      "{'loss': 0.7143, 'grad_norm': 0.21683433651924133, 'learning_rate': 8.30589484864578e-05, 'epoch': 0.59}\n",
      "{'loss': 0.697, 'grad_norm': 0.3012573719024658, 'learning_rate': 8.302354399008675e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8439, 'grad_norm': 0.24306578934192657, 'learning_rate': 8.298813949371572e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6608, 'grad_norm': 0.23130886256694794, 'learning_rate': 8.295273499734467e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9389, 'grad_norm': 0.294711709022522, 'learning_rate': 8.291733050097363e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6041, 'grad_norm': 0.2219071388244629, 'learning_rate': 8.288192600460258e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6695, 'grad_norm': 0.22067424654960632, 'learning_rate': 8.284652150823155e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7849, 'grad_norm': 0.2614470422267914, 'learning_rate': 8.28111170118605e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8617, 'grad_norm': 0.23760785162448883, 'learning_rate': 8.277571251548947e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9161, 'grad_norm': 0.20808304846286774, 'learning_rate': 8.274030801911843e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6328, 'grad_norm': 0.18241892755031586, 'learning_rate': 8.27049035227474e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5097, 'grad_norm': 0.1778506487607956, 'learning_rate': 8.266949902637635e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8343, 'grad_norm': 0.27043506503105164, 'learning_rate': 8.263409453000532e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7194, 'grad_norm': 0.2222544550895691, 'learning_rate': 8.259869003363427e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8719, 'grad_norm': 0.2730845510959625, 'learning_rate': 8.256328553726323e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6537, 'grad_norm': 0.22095772624015808, 'learning_rate': 8.25278810408922e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6903, 'grad_norm': 0.22471828758716583, 'learning_rate': 8.249247654452115e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7829, 'grad_norm': 0.2405456304550171, 'learning_rate': 8.245707204815012e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7318, 'grad_norm': 0.21558724343776703, 'learning_rate': 8.242166755177908e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7374, 'grad_norm': 0.22045035660266876, 'learning_rate': 8.238626305540804e-05, 'epoch': 0.59}\n",
      "{'loss': 0.4916, 'grad_norm': 0.2019864022731781, 'learning_rate': 8.2350858559037e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6604, 'grad_norm': 0.21429449319839478, 'learning_rate': 8.231545406266597e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6234, 'grad_norm': 0.29284897446632385, 'learning_rate': 8.228004956629492e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7491, 'grad_norm': 0.251356303691864, 'learning_rate': 8.224464506992389e-05, 'epoch': 0.59}\n",
      "{'loss': 0.902, 'grad_norm': 0.28310301899909973, 'learning_rate': 8.220924057355284e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5542, 'grad_norm': 0.1926782876253128, 'learning_rate': 8.217383607718181e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7202, 'grad_norm': 0.344629168510437, 'learning_rate': 8.213843158081077e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0601, 'grad_norm': 0.32324284315109253, 'learning_rate': 8.210302708443974e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0551, 'grad_norm': 0.23271287977695465, 'learning_rate': 8.206762258806869e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6601, 'grad_norm': 0.25550976395606995, 'learning_rate': 8.203221809169766e-05, 'epoch': 0.59}\n",
      "{'loss': 1.002, 'grad_norm': 0.25907641649246216, 'learning_rate': 8.199681359532661e-05, 'epoch': 0.59}\n",
      "{'loss': 0.797, 'grad_norm': 0.2046099752187729, 'learning_rate': 8.196140909895557e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5937, 'grad_norm': 0.20229609310626984, 'learning_rate': 8.192600460258452e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7363, 'grad_norm': 0.24937647581100464, 'learning_rate': 8.189060010621349e-05, 'epoch': 0.59}\n",
      "{'loss': 1.0953, 'grad_norm': 0.24407614767551422, 'learning_rate': 8.185519560984245e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6563, 'grad_norm': 0.23776167631149292, 'learning_rate': 8.181979111347141e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7661, 'grad_norm': 0.20919619500637054, 'learning_rate': 8.178438661710037e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7369, 'grad_norm': 0.31231680512428284, 'learning_rate': 8.174898212072934e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9209, 'grad_norm': 0.22387348115444183, 'learning_rate': 8.171357762435829e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5615, 'grad_norm': 0.18046213686466217, 'learning_rate': 8.167817312798726e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6992, 'grad_norm': 0.2004454880952835, 'learning_rate': 8.164276863161622e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7855, 'grad_norm': 0.243027463555336, 'learning_rate': 8.160736413524518e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5651, 'grad_norm': 0.23572532832622528, 'learning_rate': 8.157195963887414e-05, 'epoch': 0.59}\n",
      "{'loss': 0.944, 'grad_norm': 0.24813410639762878, 'learning_rate': 8.153655514250311e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9777, 'grad_norm': 0.22882333397865295, 'learning_rate': 8.150115064613206e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6035, 'grad_norm': 0.17794525623321533, 'learning_rate': 8.146574614976103e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6222, 'grad_norm': 0.22498685121536255, 'learning_rate': 8.143034165338998e-05, 'epoch': 0.59}\n",
      "{'loss': 0.777, 'grad_norm': 0.23536387085914612, 'learning_rate': 8.139493715701895e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8581, 'grad_norm': 0.27572038769721985, 'learning_rate': 8.135953266064791e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5482, 'grad_norm': 0.24656009674072266, 'learning_rate': 8.132412816427688e-05, 'epoch': 0.59}\n",
      "{'loss': 0.9182, 'grad_norm': 0.3026626408100128, 'learning_rate': 8.128872366790583e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6919, 'grad_norm': 0.24136362969875336, 'learning_rate': 8.125331917153479e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7583, 'grad_norm': 0.25637632608413696, 'learning_rate': 8.121791467516375e-05, 'epoch': 0.59}\n",
      "{'loss': 0.8016, 'grad_norm': 0.2002716362476349, 'learning_rate': 8.118251017879271e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6285, 'grad_norm': 0.23474307358264923, 'learning_rate': 8.114710568242168e-05, 'epoch': 0.59}\n",
      "{'loss': 0.6913, 'grad_norm': 0.1899237483739853, 'learning_rate': 8.111170118605063e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7089, 'grad_norm': 0.26425138115882874, 'learning_rate': 8.10762966896796e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7523, 'grad_norm': 0.20888926088809967, 'learning_rate': 8.104089219330855e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8755, 'grad_norm': 0.27694228291511536, 'learning_rate': 8.100548769693751e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8047, 'grad_norm': 0.23013675212860107, 'learning_rate': 8.097008320056646e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6817, 'grad_norm': 0.3261178731918335, 'learning_rate': 8.093467870419543e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5461, 'grad_norm': 0.18568572402000427, 'learning_rate': 8.089927420782439e-05, 'epoch': 0.6}\n",
      "{'loss': 1.0275, 'grad_norm': 0.2682091295719147, 'learning_rate': 8.086386971145336e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7881, 'grad_norm': 0.24368028342723846, 'learning_rate': 8.082846521508231e-05, 'epoch': 0.6}\n",
      "{'loss': 0.751, 'grad_norm': 0.23262107372283936, 'learning_rate': 8.079306071871128e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5269, 'grad_norm': 0.20227676630020142, 'learning_rate': 8.075765622234023e-05, 'epoch': 0.6}\n",
      "{'loss': 0.68, 'grad_norm': 0.17879047989845276, 'learning_rate': 8.07222517259692e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7046, 'grad_norm': 0.18562112748622894, 'learning_rate': 8.068684722959816e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4954, 'grad_norm': 0.1813480108976364, 'learning_rate': 8.065144273322713e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6943, 'grad_norm': 0.26471731066703796, 'learning_rate': 8.061603823685608e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7127, 'grad_norm': 0.21358191967010498, 'learning_rate': 8.058063374048505e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4784, 'grad_norm': 0.17671218514442444, 'learning_rate': 8.0545229244114e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6825, 'grad_norm': 0.1962912380695343, 'learning_rate': 8.050982474774297e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7511, 'grad_norm': 0.256476491689682, 'learning_rate': 8.047442025137193e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8921, 'grad_norm': 0.2843259572982788, 'learning_rate': 8.04390157550009e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5166, 'grad_norm': 0.22771449387073517, 'learning_rate': 8.040361125862985e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5743, 'grad_norm': 0.1841951310634613, 'learning_rate': 8.036820676225882e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8299, 'grad_norm': 0.24699528515338898, 'learning_rate': 8.033280226588777e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9778, 'grad_norm': 0.20774641633033752, 'learning_rate': 8.029739776951674e-05, 'epoch': 0.6}\n",
      "{'loss': 0.846, 'grad_norm': 0.23643139004707336, 'learning_rate': 8.02619932731457e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8216, 'grad_norm': 0.27481770515441895, 'learning_rate': 8.022658877677466e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8855, 'grad_norm': 0.2935722768306732, 'learning_rate': 8.019118428040362e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7296, 'grad_norm': 0.29523274302482605, 'learning_rate': 8.015577978403259e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5333, 'grad_norm': 0.19694052636623383, 'learning_rate': 8.012037528766154e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7917, 'grad_norm': 0.27591845393180847, 'learning_rate': 8.00849707912905e-05, 'epoch': 0.6}\n",
      "{'loss': 0.9179, 'grad_norm': 0.24998849630355835, 'learning_rate': 8.004956629491945e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8697, 'grad_norm': 0.2885095179080963, 'learning_rate': 8.001416179854842e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6683, 'grad_norm': 0.20624442398548126, 'learning_rate': 7.997875730217737e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5551, 'grad_norm': 0.22366148233413696, 'learning_rate': 7.994335280580634e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7688, 'grad_norm': 0.23527005314826965, 'learning_rate': 7.99079483094353e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7218, 'grad_norm': 0.258188933134079, 'learning_rate': 7.987254381306427e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5496, 'grad_norm': 0.23851153254508972, 'learning_rate': 7.983713931669322e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6453, 'grad_norm': 0.28204700350761414, 'learning_rate': 7.980173482032219e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6922, 'grad_norm': 0.20502804219722748, 'learning_rate': 7.976633032395114e-05, 'epoch': 0.6}\n",
      "{'loss': 0.679, 'grad_norm': 0.1995994597673416, 'learning_rate': 7.973092582758011e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7031, 'grad_norm': 0.278020977973938, 'learning_rate': 7.969552133120907e-05, 'epoch': 0.6}\n",
      "{'loss': 1.1603, 'grad_norm': 0.2991688847541809, 'learning_rate': 7.966011683483802e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8759, 'grad_norm': 0.2930455803871155, 'learning_rate': 7.962471233846699e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5914, 'grad_norm': 0.254547655582428, 'learning_rate': 7.958930784209594e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8337, 'grad_norm': 0.24005748331546783, 'learning_rate': 7.955390334572491e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5953, 'grad_norm': 0.18839582800865173, 'learning_rate': 7.951849884935387e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6963, 'grad_norm': 0.25426825881004333, 'learning_rate': 7.948309435298284e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7218, 'grad_norm': 0.1868295669555664, 'learning_rate': 7.944768985661179e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6985, 'grad_norm': 0.20893561840057373, 'learning_rate': 7.941228536024076e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6892, 'grad_norm': 0.2251671403646469, 'learning_rate': 7.937688086386971e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6425, 'grad_norm': 0.21387417614459991, 'learning_rate': 7.934147636749868e-05, 'epoch': 0.6}\n",
      "{'loss': 0.768, 'grad_norm': 0.24184219539165497, 'learning_rate': 7.930607187112764e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8408, 'grad_norm': 0.2245383858680725, 'learning_rate': 7.92706673747566e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6696, 'grad_norm': 0.32499033212661743, 'learning_rate': 7.923526287838556e-05, 'epoch': 0.6}\n",
      "{'loss': 0.8203, 'grad_norm': 0.21439045667648315, 'learning_rate': 7.919985838201453e-05, 'epoch': 0.6}\n",
      "{'loss': 0.599, 'grad_norm': 0.18922214210033417, 'learning_rate': 7.916445388564348e-05, 'epoch': 0.6}\n",
      "{'loss': 0.5591, 'grad_norm': 0.17172746360301971, 'learning_rate': 7.912904938927244e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7029, 'grad_norm': 0.222402423620224, 'learning_rate': 7.909364489290139e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6018, 'grad_norm': 0.21902190148830414, 'learning_rate': 7.905824039653036e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5164, 'grad_norm': 0.22925573587417603, 'learning_rate': 7.902283590015932e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6851, 'grad_norm': 0.23403410613536835, 'learning_rate': 7.898743140378828e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6209, 'grad_norm': 0.28835374116897583, 'learning_rate': 7.895202690741724e-05, 'epoch': 0.61}\n",
      "{'loss': 0.4981, 'grad_norm': 0.19161345064640045, 'learning_rate': 7.89166224110462e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7802, 'grad_norm': 0.2349715530872345, 'learning_rate': 7.888121791467516e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6044, 'grad_norm': 0.20265498757362366, 'learning_rate': 7.884581341830413e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6209, 'grad_norm': 0.20094043016433716, 'learning_rate': 7.881040892193308e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0216, 'grad_norm': 0.23853488266468048, 'learning_rate': 7.877500442556205e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6835, 'grad_norm': 0.22975817322731018, 'learning_rate': 7.873959992919101e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6607, 'grad_norm': 0.20845435559749603, 'learning_rate': 7.870419543281998e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7328, 'grad_norm': 0.2007519006729126, 'learning_rate': 7.866879093644893e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6435, 'grad_norm': 0.2174430787563324, 'learning_rate': 7.86333864400779e-05, 'epoch': 0.61}\n",
      "{'loss': 0.8561, 'grad_norm': 0.2835131883621216, 'learning_rate': 7.859798194370685e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5812, 'grad_norm': 0.16627588868141174, 'learning_rate': 7.856257744733582e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5894, 'grad_norm': 0.23900273442268372, 'learning_rate': 7.852717295096478e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6201, 'grad_norm': 0.21174116432666779, 'learning_rate': 7.849176845459375e-05, 'epoch': 0.61}\n",
      "{'loss': 0.519, 'grad_norm': 0.2294291853904724, 'learning_rate': 7.84563639582227e-05, 'epoch': 0.61}\n",
      "{'loss': 0.698, 'grad_norm': 0.33246755599975586, 'learning_rate': 7.842095946185167e-05, 'epoch': 0.61}\n",
      "{'loss': 0.8112, 'grad_norm': 0.20360660552978516, 'learning_rate': 7.838555496548062e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7214, 'grad_norm': 0.2329171597957611, 'learning_rate': 7.835015046910958e-05, 'epoch': 0.61}\n",
      "{'loss': 0.671, 'grad_norm': 0.1895076185464859, 'learning_rate': 7.831474597273855e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7888, 'grad_norm': 0.22769789397716522, 'learning_rate': 7.82793414763675e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6764, 'grad_norm': 0.23978155851364136, 'learning_rate': 7.824393697999647e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7308, 'grad_norm': 0.2711191773414612, 'learning_rate': 7.820853248362542e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5713, 'grad_norm': 0.3172488212585449, 'learning_rate': 7.817312798725438e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5946, 'grad_norm': 0.3072543740272522, 'learning_rate': 7.813772349088335e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7218, 'grad_norm': 0.20513266324996948, 'learning_rate': 7.81023189945123e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6046, 'grad_norm': 0.24377399682998657, 'learning_rate': 7.806691449814127e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6546, 'grad_norm': 0.22061258554458618, 'learning_rate': 7.803151000177022e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7989, 'grad_norm': 0.2595554292201996, 'learning_rate': 7.799610550539918e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7115, 'grad_norm': 0.20332682132720947, 'learning_rate': 7.796070100902815e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6492, 'grad_norm': 0.2547647953033447, 'learning_rate': 7.79252965126571e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1006, 'grad_norm': 0.3037116825580597, 'learning_rate': 7.788989201628607e-05, 'epoch': 0.61}\n",
      "{'loss': 0.8961, 'grad_norm': 0.27399855852127075, 'learning_rate': 7.785448751991503e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6775, 'grad_norm': 0.21916231513023376, 'learning_rate': 7.7819083023544e-05, 'epoch': 0.61}\n",
      "{'loss': 0.877, 'grad_norm': 0.22152362763881683, 'learning_rate': 7.778367852717295e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0388, 'grad_norm': 0.3356941342353821, 'learning_rate': 7.774827403080192e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6444, 'grad_norm': 0.20634503662586212, 'learning_rate': 7.771286953443087e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6432, 'grad_norm': 0.21733593940734863, 'learning_rate': 7.767746503805984e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5497, 'grad_norm': 0.18786421418190002, 'learning_rate': 7.76420605416888e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6414, 'grad_norm': 0.25555363297462463, 'learning_rate': 7.760665604531776e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7667, 'grad_norm': 0.2560111880302429, 'learning_rate': 7.757125154894672e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6503, 'grad_norm': 0.25852376222610474, 'learning_rate': 7.753584705257569e-05, 'epoch': 0.61}\n",
      "{'loss': 1.1324, 'grad_norm': 0.23101815581321716, 'learning_rate': 7.750044255620464e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7156, 'grad_norm': 0.26687681674957275, 'learning_rate': 7.746503805983361e-05, 'epoch': 0.61}\n",
      "{'loss': 0.803, 'grad_norm': 0.2187509387731552, 'learning_rate': 7.742963356346256e-05, 'epoch': 0.61}\n",
      "{'loss': 1.0277, 'grad_norm': 0.32919207215309143, 'learning_rate': 7.739422906709153e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7284, 'grad_norm': 0.2061704695224762, 'learning_rate': 7.735882457072049e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6582, 'grad_norm': 0.20236806571483612, 'learning_rate': 7.732342007434946e-05, 'epoch': 0.61}\n",
      "{'loss': 0.693, 'grad_norm': 0.20980313420295715, 'learning_rate': 7.728801557797841e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6655, 'grad_norm': 0.29608193039894104, 'learning_rate': 7.725261108160738e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6119, 'grad_norm': 0.2687193751335144, 'learning_rate': 7.721720658523632e-05, 'epoch': 0.61}\n",
      "{'loss': 0.783, 'grad_norm': 0.21435295045375824, 'learning_rate': 7.718180208886529e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6729, 'grad_norm': 0.20436638593673706, 'learning_rate': 7.714639759249424e-05, 'epoch': 0.61}\n",
      "{'loss': 0.725, 'grad_norm': 0.24155886471271515, 'learning_rate': 7.711099309612321e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5926, 'grad_norm': 0.23701556026935577, 'learning_rate': 7.707558859975217e-05, 'epoch': 0.61}\n",
      "{'loss': 0.9733, 'grad_norm': 0.2885726988315582, 'learning_rate': 7.704018410338113e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4918, 'grad_norm': 0.1555614322423935, 'learning_rate': 7.700477960701009e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5567, 'grad_norm': 0.18469589948654175, 'learning_rate': 7.696937511063906e-05, 'epoch': 0.62}\n",
      "{'loss': 1.0085, 'grad_norm': 0.28390222787857056, 'learning_rate': 7.693397061426801e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8437, 'grad_norm': 0.3311390280723572, 'learning_rate': 7.689856611789698e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7028, 'grad_norm': 0.2635568678379059, 'learning_rate': 7.686316162152594e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5339, 'grad_norm': 0.18850213289260864, 'learning_rate': 7.68277571251549e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7072, 'grad_norm': 0.22072508931159973, 'learning_rate': 7.679235262878386e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1874, 'grad_norm': 0.23739482462406158, 'learning_rate': 7.675694813241283e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5844, 'grad_norm': 0.21860724687576294, 'learning_rate': 7.672154363604178e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9271, 'grad_norm': 0.2073991745710373, 'learning_rate': 7.668613913967074e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7826, 'grad_norm': 0.23598696291446686, 'learning_rate': 7.66507346432997e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7214, 'grad_norm': 0.32838210463523865, 'learning_rate': 7.661533014692866e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7229, 'grad_norm': 0.23944255709648132, 'learning_rate': 7.657992565055763e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9113, 'grad_norm': 0.20220591127872467, 'learning_rate': 7.654452115418658e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7771, 'grad_norm': 0.22751356661319733, 'learning_rate': 7.650911665781555e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7055, 'grad_norm': 0.22407573461532593, 'learning_rate': 7.64737121614445e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9726, 'grad_norm': 0.3467142879962921, 'learning_rate': 7.643830766507347e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8727, 'grad_norm': 0.23570862412452698, 'learning_rate': 7.640290316870243e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5993, 'grad_norm': 0.1852678805589676, 'learning_rate': 7.63674986723314e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5721, 'grad_norm': 0.18862180411815643, 'learning_rate': 7.633209417596035e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5488, 'grad_norm': 0.2021443396806717, 'learning_rate': 7.62966896795893e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5583, 'grad_norm': 0.2096472829580307, 'learning_rate': 7.626128518321826e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee3794f220>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 932d2f67-9520-417d-8be4-09e8c1bd6f7e)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8086, 'grad_norm': 0.24997824430465698, 'learning_rate': 7.622588068684723e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5649, 'grad_norm': 0.2368565797805786, 'learning_rate': 7.619047619047618e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6535, 'grad_norm': 0.20250937342643738, 'learning_rate': 7.615507169410515e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7053, 'grad_norm': 0.27611875534057617, 'learning_rate': 7.611966719773411e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6388, 'grad_norm': 0.2672882676124573, 'learning_rate': 7.608426270136308e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7386, 'grad_norm': 0.21487553417682648, 'learning_rate': 7.604885820499203e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6917, 'grad_norm': 0.2893073558807373, 'learning_rate': 7.6013453708621e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5603, 'grad_norm': 0.2019592523574829, 'learning_rate': 7.597804921224995e-05, 'epoch': 0.62}\n",
      "{'loss': 0.749, 'grad_norm': 0.2201721966266632, 'learning_rate': 7.594264471587892e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7327, 'grad_norm': 0.25742068886756897, 'learning_rate': 7.590724021950788e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6929, 'grad_norm': 0.22620362043380737, 'learning_rate': 7.587183572313685e-05, 'epoch': 0.62}\n",
      "{'loss': 0.874, 'grad_norm': 0.2978998124599457, 'learning_rate': 7.58364312267658e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9447, 'grad_norm': 0.21897003054618835, 'learning_rate': 7.580102673039477e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8416, 'grad_norm': 0.2342039793729782, 'learning_rate': 7.576562223402372e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7539, 'grad_norm': 0.25130072236061096, 'learning_rate': 7.573021773765269e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8479, 'grad_norm': 0.2741389274597168, 'learning_rate': 7.569481324128165e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7712, 'grad_norm': 0.24267391860485077, 'learning_rate': 7.565940874491061e-05, 'epoch': 0.62}\n",
      "{'loss': 0.8275, 'grad_norm': 0.21919207274913788, 'learning_rate': 7.562400424853957e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5017, 'grad_norm': 0.17049771547317505, 'learning_rate': 7.558859975216854e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7838, 'grad_norm': 0.24028514325618744, 'learning_rate': 7.555319525579749e-05, 'epoch': 0.62}\n",
      "{'loss': 0.7097, 'grad_norm': 0.2830852270126343, 'learning_rate': 7.551779075942646e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4914, 'grad_norm': 0.1952197402715683, 'learning_rate': 7.548238626305542e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1828, 'grad_norm': 0.38964131474494934, 'learning_rate': 7.544698176668438e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5311, 'grad_norm': 0.406911700963974, 'learning_rate': 7.541157727031334e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9478, 'grad_norm': 0.26290127635002136, 'learning_rate': 7.537617277394229e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6484, 'grad_norm': 0.1834859549999237, 'learning_rate': 7.534076827757125e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5413, 'grad_norm': 0.21687977015972137, 'learning_rate': 7.530536378120022e-05, 'epoch': 0.62}\n",
      "{'loss': 0.5082, 'grad_norm': 0.3155273497104645, 'learning_rate': 7.526995928482917e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6342, 'grad_norm': 0.25270068645477295, 'learning_rate': 7.523455478845814e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4643, 'grad_norm': 0.17973878979682922, 'learning_rate': 7.51991502920871e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6252, 'grad_norm': 0.19021102786064148, 'learning_rate': 7.516374579571606e-05, 'epoch': 0.62}\n",
      "{'loss': 1.1108, 'grad_norm': 0.27430829405784607, 'learning_rate': 7.512834129934502e-05, 'epoch': 0.62}\n",
      "{'loss': 0.9968, 'grad_norm': 0.2811315059661865, 'learning_rate': 7.509293680297399e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6178, 'grad_norm': 0.25670862197875977, 'learning_rate': 7.505753230660294e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8231, 'grad_norm': 0.24263858795166016, 'learning_rate': 7.50221278102319e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8015, 'grad_norm': 0.21727165579795837, 'learning_rate': 7.498672331386086e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6244, 'grad_norm': 0.20734776556491852, 'learning_rate': 7.495131881748982e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6177, 'grad_norm': 0.21412509679794312, 'learning_rate': 7.491591432111879e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5488, 'grad_norm': 0.18134167790412903, 'learning_rate': 7.488050982474774e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6021, 'grad_norm': 0.16488416492938995, 'learning_rate': 7.484510532837671e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6926, 'grad_norm': 0.24771469831466675, 'learning_rate': 7.480970083200566e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6841, 'grad_norm': 0.21288536489009857, 'learning_rate': 7.477429633563463e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4643, 'grad_norm': 0.20676247775554657, 'learning_rate': 7.473889183926359e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8396, 'grad_norm': 0.2832798957824707, 'learning_rate': 7.470348734289256e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8025, 'grad_norm': 0.2501717507839203, 'learning_rate': 7.466808284652151e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5826, 'grad_norm': 0.2105787843465805, 'learning_rate': 7.463267835015048e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6529, 'grad_norm': 0.24129623174667358, 'learning_rate': 7.459727385377943e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7993, 'grad_norm': 0.25118640065193176, 'learning_rate': 7.45618693574084e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5882, 'grad_norm': 0.21627679467201233, 'learning_rate': 7.452646486103736e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8112, 'grad_norm': 0.3404426574707031, 'learning_rate': 7.449106036466632e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8442, 'grad_norm': 0.27784907817840576, 'learning_rate': 7.445565586829528e-05, 'epoch': 0.63}\n",
      "{'loss': 1.041, 'grad_norm': 0.22503821551799774, 'learning_rate': 7.442025137192425e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6807, 'grad_norm': 0.22550947964191437, 'learning_rate': 7.438484687555319e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5957, 'grad_norm': 0.21432657539844513, 'learning_rate': 7.434944237918216e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7743, 'grad_norm': 0.22776181995868683, 'learning_rate': 7.431403788281111e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8575, 'grad_norm': 0.22062408924102783, 'learning_rate': 7.427863338644008e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5926, 'grad_norm': 0.24639533460140228, 'learning_rate': 7.424322889006904e-05, 'epoch': 0.63}\n",
      "{'loss': 0.788, 'grad_norm': 0.20392481982707977, 'learning_rate': 7.4207824393698e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6156, 'grad_norm': 0.22333520650863647, 'learning_rate': 7.417241989732696e-05, 'epoch': 0.63}\n",
      "{'loss': 1.0753, 'grad_norm': 0.2912568151950836, 'learning_rate': 7.413701540095593e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4834, 'grad_norm': 0.20870095491409302, 'learning_rate': 7.410161090458488e-05, 'epoch': 0.63}\n",
      "{'loss': 0.9099, 'grad_norm': 0.29058629274368286, 'learning_rate': 7.406620640821385e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4882, 'grad_norm': 0.21589115262031555, 'learning_rate': 7.40308019118428e-05, 'epoch': 0.63}\n",
      "{'loss': 0.655, 'grad_norm': 0.22859087586402893, 'learning_rate': 7.399539741547177e-05, 'epoch': 0.63}\n",
      "{'loss': 0.749, 'grad_norm': 0.20833761990070343, 'learning_rate': 7.395999291910073e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6696, 'grad_norm': 0.21709281206130981, 'learning_rate': 7.39245884227297e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8798, 'grad_norm': 0.2394845336675644, 'learning_rate': 7.388918392635865e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6936, 'grad_norm': 0.18028192222118378, 'learning_rate': 7.385377942998762e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7364, 'grad_norm': 0.22407445311546326, 'learning_rate': 7.381837493361657e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7664, 'grad_norm': 0.25505444407463074, 'learning_rate': 7.378297043724554e-05, 'epoch': 0.63}\n",
      "{'loss': 0.697, 'grad_norm': 0.22198833525180817, 'learning_rate': 7.37475659408745e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6434, 'grad_norm': 0.2195414900779724, 'learning_rate': 7.371216144450345e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5864, 'grad_norm': 0.21186615526676178, 'learning_rate': 7.367675694813242e-05, 'epoch': 0.63}\n",
      "{'loss': 1.192, 'grad_norm': 0.24492661654949188, 'learning_rate': 7.364135245176137e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5446, 'grad_norm': 0.21234829723834991, 'learning_rate': 7.360594795539034e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6071, 'grad_norm': 0.17185422778129578, 'learning_rate': 7.35705434590193e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6589, 'grad_norm': 0.22452667355537415, 'learning_rate': 7.353513896264827e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7462, 'grad_norm': 0.2626311182975769, 'learning_rate': 7.349973446627722e-05, 'epoch': 0.63}\n",
      "{'loss': 0.724, 'grad_norm': 0.2839701771736145, 'learning_rate': 7.346432996990619e-05, 'epoch': 0.63}\n",
      "{'loss': 1.1003, 'grad_norm': 0.2979390621185303, 'learning_rate': 7.342892547353513e-05, 'epoch': 0.63}\n",
      "{'loss': 0.631, 'grad_norm': 0.25938355922698975, 'learning_rate': 7.33935209771641e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7983, 'grad_norm': 0.20402002334594727, 'learning_rate': 7.335811648079305e-05, 'epoch': 0.63}\n",
      "{'loss': 0.87, 'grad_norm': 0.26666584610939026, 'learning_rate': 7.332271198442202e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8037, 'grad_norm': 0.2396511286497116, 'learning_rate': 7.328730748805098e-05, 'epoch': 0.63}\n",
      "{'loss': 0.8269, 'grad_norm': 0.22651086747646332, 'learning_rate': 7.325190299167995e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5751, 'grad_norm': 0.20659160614013672, 'learning_rate': 7.32164984953089e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7415, 'grad_norm': 0.24821296334266663, 'learning_rate': 7.318109399893787e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5475, 'grad_norm': 0.21705955266952515, 'learning_rate': 7.314568950256682e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6607, 'grad_norm': 0.21527299284934998, 'learning_rate': 7.311028500619579e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4777, 'grad_norm': 0.20875778794288635, 'learning_rate': 7.307488050982475e-05, 'epoch': 0.63}\n",
      "{'loss': 0.6856, 'grad_norm': 0.23129697144031525, 'learning_rate': 7.303947601345371e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9002, 'grad_norm': 0.29184475541114807, 'learning_rate': 7.300407151708267e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5289, 'grad_norm': 0.18177510797977448, 'learning_rate': 7.296866702071164e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8048, 'grad_norm': 0.2853969633579254, 'learning_rate': 7.293326252434059e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6211, 'grad_norm': 0.21300996840000153, 'learning_rate': 7.289785802796956e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0156, 'grad_norm': 0.1924000382423401, 'learning_rate': 7.286245353159852e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1583, 'grad_norm': 0.322125643491745, 'learning_rate': 7.282704903522748e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6186, 'grad_norm': 0.17338679730892181, 'learning_rate': 7.279164453885644e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7686, 'grad_norm': 0.21695983409881592, 'learning_rate': 7.27562400424854e-05, 'epoch': 0.64}\n",
      "{'loss': 0.62, 'grad_norm': 0.20009279251098633, 'learning_rate': 7.272083554611436e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6322, 'grad_norm': 0.20139537751674652, 'learning_rate': 7.268543104974333e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8979, 'grad_norm': 0.2337557077407837, 'learning_rate': 7.265002655337228e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9407, 'grad_norm': 0.2736491858959198, 'learning_rate': 7.261462205700125e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7888, 'grad_norm': 0.26727330684661865, 'learning_rate': 7.257921756063021e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5421, 'grad_norm': 0.21264001727104187, 'learning_rate': 7.254381306425918e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9367, 'grad_norm': 0.24527594447135925, 'learning_rate': 7.250840856788812e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6253, 'grad_norm': 0.32129770517349243, 'learning_rate': 7.247300407151709e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6844, 'grad_norm': 0.22318696975708008, 'learning_rate': 7.243759957514604e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0093, 'grad_norm': 0.3180048167705536, 'learning_rate': 7.240219507877501e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7544, 'grad_norm': 0.2550975978374481, 'learning_rate': 7.236679058240396e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5736, 'grad_norm': 0.20001046359539032, 'learning_rate': 7.233138608603293e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9951, 'grad_norm': 0.2845017611980438, 'learning_rate': 7.229598158966189e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7672, 'grad_norm': 0.22713035345077515, 'learning_rate': 7.226057709329085e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0927, 'grad_norm': 0.32971397042274475, 'learning_rate': 7.222517259691981e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5646, 'grad_norm': 0.258529931306839, 'learning_rate': 7.218976810054878e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5337, 'grad_norm': 0.22366666793823242, 'learning_rate': 7.215436360417773e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6392, 'grad_norm': 0.21377786993980408, 'learning_rate': 7.211895910780669e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6245, 'grad_norm': 0.2792617678642273, 'learning_rate': 7.208355461143566e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5344, 'grad_norm': 0.20968566834926605, 'learning_rate': 7.204815011506461e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8328, 'grad_norm': 0.25236228108406067, 'learning_rate': 7.201274561869358e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7732, 'grad_norm': 0.22606267035007477, 'learning_rate': 7.197734112232253e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7521, 'grad_norm': 0.19211368262767792, 'learning_rate': 7.19419366259515e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5055, 'grad_norm': 0.19438529014587402, 'learning_rate': 7.190653212958046e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9545, 'grad_norm': 0.28039729595184326, 'learning_rate': 7.187112763320942e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6715, 'grad_norm': 0.25393033027648926, 'learning_rate': 7.183572313683838e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6887, 'grad_norm': 0.24601179361343384, 'learning_rate': 7.180031864046735e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5611, 'grad_norm': 0.2274714857339859, 'learning_rate': 7.17649141440963e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6554, 'grad_norm': 0.19709394872188568, 'learning_rate': 7.172950964772527e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6978, 'grad_norm': 0.25440314412117004, 'learning_rate': 7.169410515135423e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7599, 'grad_norm': 0.2541854977607727, 'learning_rate': 7.16587006549832e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6623, 'grad_norm': 0.18093997240066528, 'learning_rate': 7.162329615861215e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7436, 'grad_norm': 0.23617099225521088, 'learning_rate': 7.158789166224112e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7355, 'grad_norm': 0.21907594799995422, 'learning_rate': 7.155248716587006e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3603, 'grad_norm': 0.1894283890724182, 'learning_rate': 7.151708266949903e-05, 'epoch': 0.64}\n",
      "{'loss': 0.4175, 'grad_norm': 0.18670687079429626, 'learning_rate': 7.148167817312798e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7097, 'grad_norm': 0.22877927124500275, 'learning_rate': 7.144627367675695e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6039, 'grad_norm': 0.23862138390541077, 'learning_rate': 7.14108691803859e-05, 'epoch': 0.64}\n",
      "{'loss': 0.827, 'grad_norm': 0.200547993183136, 'learning_rate': 7.137546468401487e-05, 'epoch': 0.64}\n",
      "{'loss': 1.1403, 'grad_norm': 0.25201907753944397, 'learning_rate': 7.134006018764383e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7415, 'grad_norm': 0.21630117297172546, 'learning_rate': 7.13046556912728e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8148, 'grad_norm': 0.2971222996711731, 'learning_rate': 7.126925119490175e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9131, 'grad_norm': 0.21785175800323486, 'learning_rate': 7.123384669853072e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6347, 'grad_norm': 0.20087656378746033, 'learning_rate': 7.119844220215967e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8009, 'grad_norm': 0.2701764702796936, 'learning_rate': 7.116303770578864e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6119, 'grad_norm': 0.2298128753900528, 'learning_rate': 7.11276332094176e-05, 'epoch': 0.64}\n",
      "{'loss': 1.277, 'grad_norm': 0.24487246572971344, 'learning_rate': 7.109222871304657e-05, 'epoch': 0.64}\n",
      "{'loss': 0.9375, 'grad_norm': 0.19937898218631744, 'learning_rate': 7.105682421667552e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7448, 'grad_norm': 0.23635461926460266, 'learning_rate': 7.102141972030449e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6056, 'grad_norm': 0.2942033112049103, 'learning_rate': 7.098601522393344e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6177, 'grad_norm': 0.18545286357402802, 'learning_rate': 7.095061072756241e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6461, 'grad_norm': 0.1908799558877945, 'learning_rate': 7.091520623119137e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6182, 'grad_norm': 0.19361606240272522, 'learning_rate': 7.087980173482033e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9166, 'grad_norm': 0.2235025018453598, 'learning_rate': 7.084439723844929e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6733, 'grad_norm': 0.22129620611667633, 'learning_rate': 7.080899274207824e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7255, 'grad_norm': 0.2158660739660263, 'learning_rate': 7.077358824570721e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7842, 'grad_norm': 0.2892296016216278, 'learning_rate': 7.073818374933617e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6696, 'grad_norm': 0.24160075187683105, 'learning_rate': 7.070277925296514e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5233, 'grad_norm': 0.25504863262176514, 'learning_rate': 7.066737475659409e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8429, 'grad_norm': 0.2507380247116089, 'learning_rate': 7.063197026022306e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7062, 'grad_norm': 0.2617124319076538, 'learning_rate': 7.059656576385201e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5892, 'grad_norm': 0.2115551233291626, 'learning_rate': 7.056116126748097e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5316, 'grad_norm': 0.1836470663547516, 'learning_rate': 7.052575677110994e-05, 'epoch': 0.65}\n",
      "{'loss': 0.79, 'grad_norm': 0.2687019407749176, 'learning_rate': 7.049035227473889e-05, 'epoch': 0.65}\n",
      "{'loss': 0.699, 'grad_norm': 0.19303761422634125, 'learning_rate': 7.045494777836785e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7011, 'grad_norm': 0.22776494920253754, 'learning_rate': 7.041954328199681e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8208, 'grad_norm': 0.20954297482967377, 'learning_rate': 7.038413878562577e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9036, 'grad_norm': 0.22250890731811523, 'learning_rate': 7.034873428925474e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7353, 'grad_norm': 0.26195579767227173, 'learning_rate': 7.031332979288369e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5867, 'grad_norm': 0.22587338089942932, 'learning_rate': 7.027792529651266e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6034, 'grad_norm': 0.20198795199394226, 'learning_rate': 7.024252080014162e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7511, 'grad_norm': 0.2391437441110611, 'learning_rate': 7.020711630377058e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7328, 'grad_norm': 0.2764549255371094, 'learning_rate': 7.017171180739954e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8473, 'grad_norm': 0.31565985083580017, 'learning_rate': 7.01363073110285e-05, 'epoch': 0.65}\n",
      "{'loss': 0.678, 'grad_norm': 0.26354101300239563, 'learning_rate': 7.010090281465746e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7484, 'grad_norm': 0.25543051958084106, 'learning_rate': 7.006549831828643e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5766, 'grad_norm': 0.1952303946018219, 'learning_rate': 7.003009382191538e-05, 'epoch': 0.65}\n",
      "{'loss': 0.9708, 'grad_norm': 0.22656702995300293, 'learning_rate': 6.999468932554435e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7285, 'grad_norm': 0.20684084296226501, 'learning_rate': 6.995928482917331e-05, 'epoch': 0.65}\n",
      "{'loss': 0.586, 'grad_norm': 0.22100822627544403, 'learning_rate': 6.992388033280228e-05, 'epoch': 0.65}\n",
      "{'loss': 0.47, 'grad_norm': 0.2341938614845276, 'learning_rate': 6.988847583643123e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8451, 'grad_norm': 0.25073206424713135, 'learning_rate': 6.98530713400602e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5063, 'grad_norm': 0.2146083414554596, 'learning_rate': 6.981766684368915e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6593, 'grad_norm': 0.19098243117332458, 'learning_rate': 6.978226234731812e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5578, 'grad_norm': 0.23368920385837555, 'learning_rate': 6.974685785094708e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5895, 'grad_norm': 0.18319305777549744, 'learning_rate': 6.971145335457605e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6404, 'grad_norm': 0.21697551012039185, 'learning_rate': 6.9676048858205e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8637, 'grad_norm': 0.2362070232629776, 'learning_rate': 6.964064436183395e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5132, 'grad_norm': 0.23158305883407593, 'learning_rate': 6.960523986546291e-05, 'epoch': 0.65}\n",
      "{'loss': 0.471, 'grad_norm': 0.16085302829742432, 'learning_rate': 6.956983536909188e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6021, 'grad_norm': 0.2769339382648468, 'learning_rate': 6.953443087272083e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5459, 'grad_norm': 0.17107051610946655, 'learning_rate': 6.94990263763498e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6844, 'grad_norm': 0.22964441776275635, 'learning_rate': 6.946362187997876e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5845, 'grad_norm': 0.21964077651500702, 'learning_rate': 6.942821738360772e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5319, 'grad_norm': 0.1703946441411972, 'learning_rate': 6.939281288723668e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6282, 'grad_norm': 0.2422717958688736, 'learning_rate': 6.935740839086565e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8046, 'grad_norm': 0.2197638601064682, 'learning_rate': 6.93220038944946e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7757, 'grad_norm': 0.22896382212638855, 'learning_rate': 6.928659939812357e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5922, 'grad_norm': 0.23752298951148987, 'learning_rate': 6.925119490175252e-05, 'epoch': 0.65}\n",
      "{'loss': 1.1963, 'grad_norm': 0.26884400844573975, 'learning_rate': 6.921579040538149e-05, 'epoch': 0.65}\n",
      "{'loss': 0.6199, 'grad_norm': 0.2147984653711319, 'learning_rate': 6.918038590901045e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5591, 'grad_norm': 0.23050665855407715, 'learning_rate': 6.91449814126394e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8776, 'grad_norm': 0.24020110070705414, 'learning_rate': 6.910957691626837e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5099, 'grad_norm': 0.1769823133945465, 'learning_rate': 6.907417241989733e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7782, 'grad_norm': 0.25301653146743774, 'learning_rate': 6.90387679235263e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7557, 'grad_norm': 0.29323089122772217, 'learning_rate': 6.900336342715525e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5614, 'grad_norm': 0.2281719297170639, 'learning_rate': 6.896795893078422e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6002, 'grad_norm': 0.21057432889938354, 'learning_rate': 6.893255443441317e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8406, 'grad_norm': 0.244994655251503, 'learning_rate': 6.889714993804214e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7742, 'grad_norm': 0.25464972853660583, 'learning_rate': 6.88617454416711e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8391, 'grad_norm': 0.2347826510667801, 'learning_rate': 6.882634094530006e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7546, 'grad_norm': 0.2300383746623993, 'learning_rate': 6.879093644892902e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8408, 'grad_norm': 0.24455849826335907, 'learning_rate': 6.875553195255799e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7757, 'grad_norm': 0.35398274660110474, 'learning_rate': 6.872012745618693e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0442, 'grad_norm': 0.268025279045105, 'learning_rate': 6.86847229598159e-05, 'epoch': 0.66}\n",
      "{'loss': 0.591, 'grad_norm': 0.19776882231235504, 'learning_rate': 6.864931846344485e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8863, 'grad_norm': 0.2502630949020386, 'learning_rate': 6.861391396707382e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0686, 'grad_norm': 0.34891247749328613, 'learning_rate': 6.857850947070277e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6935, 'grad_norm': 0.25140222907066345, 'learning_rate': 6.854310497433174e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6423, 'grad_norm': 0.2635934352874756, 'learning_rate': 6.85077004779607e-05, 'epoch': 0.66}\n",
      "{'loss': 0.672, 'grad_norm': 0.22899451851844788, 'learning_rate': 6.847229598158967e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7376, 'grad_norm': 0.2800953984260559, 'learning_rate': 6.843689148521862e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7826, 'grad_norm': 0.29456672072410583, 'learning_rate': 6.840148698884759e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6941, 'grad_norm': 0.2893584966659546, 'learning_rate': 6.836608249247654e-05, 'epoch': 0.66}\n",
      "{'loss': 0.515, 'grad_norm': 0.23410068452358246, 'learning_rate': 6.833067799610551e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8293, 'grad_norm': 0.272115558385849, 'learning_rate': 6.829527349973447e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6195, 'grad_norm': 0.2140916883945465, 'learning_rate': 6.825986900336343e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6902, 'grad_norm': 0.315163791179657, 'learning_rate': 6.822446450699239e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6374, 'grad_norm': 0.20879626274108887, 'learning_rate': 6.818906001062136e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7705, 'grad_norm': 0.20334623754024506, 'learning_rate': 6.815365551425031e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8715, 'grad_norm': 0.2844236493110657, 'learning_rate': 6.811825101787928e-05, 'epoch': 0.66}\n",
      "{'loss': 1.5, 'grad_norm': 0.25693944096565247, 'learning_rate': 6.808284652150824e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6153, 'grad_norm': 0.21418766677379608, 'learning_rate': 6.80474420251372e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6524, 'grad_norm': 0.2932111620903015, 'learning_rate': 6.801203752876616e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7454, 'grad_norm': 0.23275183141231537, 'learning_rate': 6.797663303239513e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7016, 'grad_norm': 0.22945700585842133, 'learning_rate': 6.794122853602408e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6471, 'grad_norm': 0.19543489813804626, 'learning_rate': 6.790582403965305e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8182, 'grad_norm': 0.5286384224891663, 'learning_rate': 6.7870419543282e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6212, 'grad_norm': 0.20938320457935333, 'learning_rate': 6.783501504691096e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5975, 'grad_norm': 0.1663074940443039, 'learning_rate': 6.779961055053993e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8459, 'grad_norm': 0.2521645426750183, 'learning_rate': 6.776420605416888e-05, 'epoch': 0.66}\n",
      "{'loss': 0.885, 'grad_norm': 0.3249039053916931, 'learning_rate': 6.772880155779784e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7123, 'grad_norm': 0.2551969587802887, 'learning_rate': 6.76933970614268e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5297, 'grad_norm': 0.22374951839447021, 'learning_rate': 6.765799256505576e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5826, 'grad_norm': 0.20211215317249298, 'learning_rate': 6.762258806868473e-05, 'epoch': 0.66}\n",
      "{'loss': 1.023, 'grad_norm': 0.3607119619846344, 'learning_rate': 6.758718357231368e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5454, 'grad_norm': 0.16600891947746277, 'learning_rate': 6.755177907594265e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8423, 'grad_norm': 0.32029861211776733, 'learning_rate': 6.75163745795716e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6078, 'grad_norm': 0.25255119800567627, 'learning_rate': 6.748097008320056e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8917, 'grad_norm': 0.3061283230781555, 'learning_rate': 6.744556558682953e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7405, 'grad_norm': 0.26643678545951843, 'learning_rate': 6.741016109045848e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5208, 'grad_norm': 0.2898746728897095, 'learning_rate': 6.737475659408745e-05, 'epoch': 0.66}\n",
      "{'loss': 1.0284, 'grad_norm': 0.3071826100349426, 'learning_rate': 6.733935209771641e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5351, 'grad_norm': 0.1882956624031067, 'learning_rate': 6.730394760134538e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7179, 'grad_norm': 0.24993601441383362, 'learning_rate': 6.726854310497433e-05, 'epoch': 0.66}\n",
      "{'loss': 0.9728, 'grad_norm': 0.2650688588619232, 'learning_rate': 6.72331386086033e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6684, 'grad_norm': 0.27460232377052307, 'learning_rate': 6.719773411223225e-05, 'epoch': 0.66}\n",
      "{'loss': 0.8747, 'grad_norm': 0.31038862466812134, 'learning_rate': 6.716232961586122e-05, 'epoch': 0.66}\n",
      "{'loss': 0.5402, 'grad_norm': 0.1764632761478424, 'learning_rate': 6.712692511949018e-05, 'epoch': 0.66}\n",
      "{'loss': 0.627, 'grad_norm': 0.23158541321754456, 'learning_rate': 6.709152062311914e-05, 'epoch': 0.66}\n",
      "{'loss': 0.6383, 'grad_norm': 0.29237768054008484, 'learning_rate': 6.70561161267481e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6016, 'grad_norm': 0.22943904995918274, 'learning_rate': 6.702071163037707e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0625, 'grad_norm': 0.26581206917762756, 'learning_rate': 6.698530713400602e-05, 'epoch': 0.67}\n",
      "{'loss': 0.658, 'grad_norm': 0.21212993562221527, 'learning_rate': 6.694990263763499e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5081, 'grad_norm': 0.19909986853599548, 'learning_rate': 6.691449814126395e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9607, 'grad_norm': 0.25745415687561035, 'learning_rate': 6.687909364489291e-05, 'epoch': 0.67}\n",
      "{'loss': 0.589, 'grad_norm': 0.23980100452899933, 'learning_rate': 6.684368914852187e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5921, 'grad_norm': 0.2080373466014862, 'learning_rate': 6.680828465215082e-05, 'epoch': 0.67}\n",
      "{'loss': 0.694, 'grad_norm': 0.25217413902282715, 'learning_rate': 6.677288015577978e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8151, 'grad_norm': 0.27129942178726196, 'learning_rate': 6.673747565940875e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7737, 'grad_norm': 0.2910907566547394, 'learning_rate': 6.67020711630377e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7284, 'grad_norm': 0.2116732895374298, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9531, 'grad_norm': 0.24711021780967712, 'learning_rate': 6.663126217029562e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8336, 'grad_norm': 0.24801814556121826, 'learning_rate': 6.659585767392459e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7157, 'grad_norm': 0.23781844973564148, 'learning_rate': 6.656045317755355e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6481, 'grad_norm': 0.20719115436077118, 'learning_rate': 6.652504868118252e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1509, 'grad_norm': 0.2433968037366867, 'learning_rate': 6.648964418481147e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6761, 'grad_norm': 0.2105027586221695, 'learning_rate': 6.645423968844044e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7369, 'grad_norm': 0.31103771924972534, 'learning_rate': 6.64188351920694e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6176, 'grad_norm': 0.22976979613304138, 'learning_rate': 6.638343069569836e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7576, 'grad_norm': 0.31992435455322266, 'learning_rate': 6.634802619932732e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0284, 'grad_norm': 0.4488420784473419, 'learning_rate': 6.631262170295629e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6265, 'grad_norm': 0.23030632734298706, 'learning_rate': 6.627721720658524e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6221, 'grad_norm': 0.2125043123960495, 'learning_rate': 6.624181271021421e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6353, 'grad_norm': 0.22324320673942566, 'learning_rate': 6.620640821384316e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5036, 'grad_norm': 0.19086268544197083, 'learning_rate': 6.617100371747212e-05, 'epoch': 0.67}\n",
      "{'loss': 0.4253, 'grad_norm': 0.18273235857486725, 'learning_rate': 6.613559922110109e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5247, 'grad_norm': 0.19856666028499603, 'learning_rate': 6.610019472473004e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8655, 'grad_norm': 0.2375062108039856, 'learning_rate': 6.606479022835901e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7791, 'grad_norm': 0.2270001769065857, 'learning_rate': 6.602938573198796e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8011, 'grad_norm': 0.20647040009498596, 'learning_rate': 6.599398123561693e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5973, 'grad_norm': 0.20632345974445343, 'learning_rate': 6.595857673924589e-05, 'epoch': 0.67}\n",
      "{'loss': 0.886, 'grad_norm': 0.23794716596603394, 'learning_rate': 6.592317224287486e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9011, 'grad_norm': 0.22609184682369232, 'learning_rate': 6.588776774650381e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6597, 'grad_norm': 0.26659056544303894, 'learning_rate': 6.585236325013277e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7195, 'grad_norm': 0.17772185802459717, 'learning_rate': 6.581695875376172e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1988, 'grad_norm': 0.2894355058670044, 'learning_rate': 6.578155425739069e-05, 'epoch': 0.67}\n",
      "{'loss': 1.1528, 'grad_norm': 0.2886182963848114, 'learning_rate': 6.574614976101964e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8226, 'grad_norm': 0.23575423657894135, 'learning_rate': 6.571074526464861e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5934, 'grad_norm': 0.20945264399051666, 'learning_rate': 6.567534076827757e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8915, 'grad_norm': 0.34660807251930237, 'learning_rate': 6.563993627190653e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5056, 'grad_norm': 0.22215420007705688, 'learning_rate': 6.560453177553549e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5389, 'grad_norm': 0.22468301653862, 'learning_rate': 6.556912727916446e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7223, 'grad_norm': 0.2595827281475067, 'learning_rate': 6.553372278279341e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5356, 'grad_norm': 0.24185404181480408, 'learning_rate': 6.549831828642238e-05, 'epoch': 0.67}\n",
      "{'loss': 0.9866, 'grad_norm': 0.2940882444381714, 'learning_rate': 6.546291379005134e-05, 'epoch': 0.67}\n",
      "{'loss': 0.43, 'grad_norm': 0.17855575680732727, 'learning_rate': 6.54275092936803e-05, 'epoch': 0.67}\n",
      "{'loss': 0.677, 'grad_norm': 0.28024229407310486, 'learning_rate': 6.539210479730926e-05, 'epoch': 0.67}\n",
      "{'loss': 0.655, 'grad_norm': 0.15502752363681793, 'learning_rate': 6.535670030093823e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7137, 'grad_norm': 0.25363075733184814, 'learning_rate': 6.532129580456718e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8036, 'grad_norm': 0.29476839303970337, 'learning_rate': 6.528589130819615e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7115, 'grad_norm': 0.2728100121021271, 'learning_rate': 6.52504868118251e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6439, 'grad_norm': 0.1907559633255005, 'learning_rate': 6.521508231545407e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6274, 'grad_norm': 0.24199070036411285, 'learning_rate': 6.517967781908303e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6648, 'grad_norm': 0.200352281332016, 'learning_rate': 6.5144273322712e-05, 'epoch': 0.67}\n",
      "{'loss': 0.7375, 'grad_norm': 0.23574821650981903, 'learning_rate': 6.510886882634095e-05, 'epoch': 0.67}\n",
      "{'loss': 0.8337, 'grad_norm': 0.2757343351840973, 'learning_rate': 6.507346432996992e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5005, 'grad_norm': 0.23995548486709595, 'learning_rate': 6.503805983359887e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6868, 'grad_norm': 0.264717161655426, 'learning_rate': 6.500265533722784e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7774, 'grad_norm': 0.2837120294570923, 'learning_rate': 6.49672508408568e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5624, 'grad_norm': 0.1859966367483139, 'learning_rate': 6.493184634448575e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6105, 'grad_norm': 0.2023146003484726, 'learning_rate': 6.48964418481147e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6564, 'grad_norm': 0.25993451476097107, 'learning_rate': 6.486103735174367e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8059, 'grad_norm': 0.2783738076686859, 'learning_rate': 6.482563285537263e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7139, 'grad_norm': 0.2392808496952057, 'learning_rate': 6.47902283590016e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8101, 'grad_norm': 0.24926486611366272, 'learning_rate': 6.475482386263055e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9331, 'grad_norm': 0.24510100483894348, 'learning_rate': 6.471941936625952e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8711, 'grad_norm': 0.2060931771993637, 'learning_rate': 6.468401486988848e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8578, 'grad_norm': 0.2567557096481323, 'learning_rate': 6.464861037351744e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5685, 'grad_norm': 0.26562976837158203, 'learning_rate': 6.46132058771464e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7191, 'grad_norm': 0.19387409090995789, 'learning_rate': 6.457780138077535e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6013, 'grad_norm': 0.1705053597688675, 'learning_rate': 6.454239688440432e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7368, 'grad_norm': 0.21758227050304413, 'learning_rate': 6.450699238803328e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7764, 'grad_norm': 0.2712978720664978, 'learning_rate': 6.447158789166224e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6872, 'grad_norm': 0.22534601390361786, 'learning_rate': 6.44361833952912e-05, 'epoch': 0.68}\n",
      "{'loss': 0.662, 'grad_norm': 0.20696184039115906, 'learning_rate': 6.440077889892017e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7762, 'grad_norm': 0.24692665040493011, 'learning_rate': 6.436537440254912e-05, 'epoch': 0.68}\n",
      "{'loss': 0.602, 'grad_norm': 0.17453207075595856, 'learning_rate': 6.432996990617809e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6773, 'grad_norm': 0.2053174376487732, 'learning_rate': 6.429456540980705e-05, 'epoch': 0.68}\n",
      "{'loss': 0.668, 'grad_norm': 0.23862464725971222, 'learning_rate': 6.425916091343601e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8912, 'grad_norm': 0.23144562542438507, 'learning_rate': 6.422375641706497e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5529, 'grad_norm': 0.22283218801021576, 'learning_rate': 6.418835192069394e-05, 'epoch': 0.68}\n",
      "{'loss': 0.664, 'grad_norm': 0.23198994994163513, 'learning_rate': 6.415294742432289e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7868, 'grad_norm': 0.23216141760349274, 'learning_rate': 6.411754292795186e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7126, 'grad_norm': 0.3170824944972992, 'learning_rate': 6.408213843158081e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7405, 'grad_norm': 0.2820618748664856, 'learning_rate': 6.404673393520978e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7782, 'grad_norm': 0.21235086023807526, 'learning_rate': 6.401132943883874e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7848, 'grad_norm': 0.2682705819606781, 'learning_rate': 6.397592494246769e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7997, 'grad_norm': 0.21799300611019135, 'learning_rate': 6.394052044609665e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7908, 'grad_norm': 0.2822805345058441, 'learning_rate': 6.390511594972562e-05, 'epoch': 0.68}\n",
      "{'loss': 0.9136, 'grad_norm': 0.2853735387325287, 'learning_rate': 6.386971145335457e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7144, 'grad_norm': 0.1863919198513031, 'learning_rate': 6.383430695698354e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3063, 'grad_norm': 0.28175464272499084, 'learning_rate': 6.37989024606125e-05, 'epoch': 0.68}\n",
      "{'loss': 0.507, 'grad_norm': 0.19513311982154846, 'learning_rate': 6.376349796424146e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6588, 'grad_norm': 0.21368511021137238, 'learning_rate': 6.372809346787042e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7016, 'grad_norm': 0.23383642733097076, 'learning_rate': 6.369268897149939e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7547, 'grad_norm': 0.21932871639728546, 'learning_rate': 6.365728447512834e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7721, 'grad_norm': 0.2100396752357483, 'learning_rate': 6.362187997875731e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7763, 'grad_norm': 0.21078704297542572, 'learning_rate': 6.358647548238626e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7177, 'grad_norm': 0.2644263505935669, 'learning_rate': 6.355107098601523e-05, 'epoch': 0.68}\n",
      "{'loss': 0.4676, 'grad_norm': 0.22882547974586487, 'learning_rate': 6.351566648964419e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6128, 'grad_norm': 0.27676016092300415, 'learning_rate': 6.348026199327315e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6789, 'grad_norm': 0.2094765305519104, 'learning_rate': 6.344485749690211e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8276, 'grad_norm': 0.261728435754776, 'learning_rate': 6.340945300053108e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7063, 'grad_norm': 0.23945914208889008, 'learning_rate': 6.337404850416003e-05, 'epoch': 0.68}\n",
      "{'loss': 0.4955, 'grad_norm': 0.21532253921031952, 'learning_rate': 6.3338644007789e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0735, 'grad_norm': 0.2741602659225464, 'learning_rate': 6.330323951141796e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6356, 'grad_norm': 0.23229967057704926, 'learning_rate': 6.326783501504691e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8499, 'grad_norm': 0.26800787448883057, 'learning_rate': 6.323243051867588e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6042, 'grad_norm': 0.2575790584087372, 'learning_rate': 6.319702602230483e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6767, 'grad_norm': 0.2377053201198578, 'learning_rate': 6.31616215259338e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6202, 'grad_norm': 0.21249647438526154, 'learning_rate': 6.312621702956276e-05, 'epoch': 0.68}\n",
      "{'loss': 0.7729, 'grad_norm': 0.28250497579574585, 'learning_rate': 6.309081253319172e-05, 'epoch': 0.68}\n",
      "{'loss': 0.8914, 'grad_norm': 0.2749795913696289, 'learning_rate': 6.305540803682068e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0156, 'grad_norm': 0.29871678352355957, 'learning_rate': 6.302000354044963e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4443, 'grad_norm': 0.16106297075748444, 'learning_rate': 6.29845990440786e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6016, 'grad_norm': 0.1660526692867279, 'learning_rate': 6.294919454770756e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6425, 'grad_norm': 0.23496845364570618, 'learning_rate': 6.291379005133651e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7126, 'grad_norm': 0.2382696270942688, 'learning_rate': 6.287838555496548e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7527, 'grad_norm': 0.1898469179868698, 'learning_rate': 6.284298105859444e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5927, 'grad_norm': 0.2594428062438965, 'learning_rate': 6.28075765622234e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5472, 'grad_norm': 0.20062606036663055, 'learning_rate': 6.277217206585236e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6016, 'grad_norm': 0.21948829293251038, 'learning_rate': 6.273676756948133e-05, 'epoch': 0.69}\n",
      "{'loss': 0.566, 'grad_norm': 0.23770985007286072, 'learning_rate': 6.270136307311028e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6292, 'grad_norm': 0.1961711198091507, 'learning_rate': 6.266595857673925e-05, 'epoch': 0.69}\n",
      "{'loss': 0.886, 'grad_norm': 0.2696473002433777, 'learning_rate': 6.26305540803682e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8667, 'grad_norm': 0.2711634635925293, 'learning_rate': 6.259514958399717e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7826, 'grad_norm': 0.2102927565574646, 'learning_rate': 6.255974508762613e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6372, 'grad_norm': 0.2537934482097626, 'learning_rate': 6.25243405912551e-05, 'epoch': 0.69}\n",
      "{'loss': 0.549, 'grad_norm': 0.21192964911460876, 'learning_rate': 6.248893609488405e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6088, 'grad_norm': 0.21914558112621307, 'learning_rate': 6.245353159851302e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5509, 'grad_norm': 0.23857381939888, 'learning_rate': 6.241812710214197e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7605, 'grad_norm': 0.2179507464170456, 'learning_rate': 6.238272260577094e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7861, 'grad_norm': 0.27082395553588867, 'learning_rate': 6.23473181093999e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5259, 'grad_norm': 0.22986048460006714, 'learning_rate': 6.231191361302886e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8376, 'grad_norm': 0.29100939631462097, 'learning_rate': 6.227650911665782e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1073, 'grad_norm': 0.28144246339797974, 'learning_rate': 6.224110462028679e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1438, 'grad_norm': 0.2880619764328003, 'learning_rate': 6.220570012391574e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7583, 'grad_norm': 0.2466694414615631, 'learning_rate': 6.217029562754471e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7462, 'grad_norm': 0.24207259714603424, 'learning_rate': 6.213489113117367e-05, 'epoch': 0.69}\n",
      "{'loss': 0.84, 'grad_norm': 0.2355291247367859, 'learning_rate': 6.209948663480263e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7798, 'grad_norm': 0.2465428113937378, 'learning_rate': 6.206408213843158e-05, 'epoch': 0.69}\n",
      "{'loss': 0.58, 'grad_norm': 0.26141053438186646, 'learning_rate': 6.202867764206054e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7677, 'grad_norm': 0.24055802822113037, 'learning_rate': 6.19932731456895e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8458, 'grad_norm': 0.255761057138443, 'learning_rate': 6.195786864931847e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6246, 'grad_norm': 0.19704492390155792, 'learning_rate': 6.192246415294742e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7772, 'grad_norm': 0.20690250396728516, 'learning_rate': 6.188705965657639e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6841, 'grad_norm': 0.22649993002414703, 'learning_rate': 6.185165516020534e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7787, 'grad_norm': 0.23666997253894806, 'learning_rate': 6.181625066383431e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6081, 'grad_norm': 0.19308063387870789, 'learning_rate': 6.178084616746327e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8136, 'grad_norm': 0.22641366720199585, 'learning_rate': 6.174544167109224e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5218, 'grad_norm': 0.18182916939258575, 'learning_rate': 6.171003717472119e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7985, 'grad_norm': 0.22492411732673645, 'learning_rate': 6.167463267835016e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4463, 'grad_norm': 0.1732911914587021, 'learning_rate': 6.163922818197911e-05, 'epoch': 0.69}\n",
      "{'loss': 1.1606, 'grad_norm': 0.3046454191207886, 'learning_rate': 6.160382368560807e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4969, 'grad_norm': 0.185971200466156, 'learning_rate': 6.156841918923704e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5314, 'grad_norm': 0.1863391399383545, 'learning_rate': 6.153301469286599e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5678, 'grad_norm': 0.15909132361412048, 'learning_rate': 6.149761019649496e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5901, 'grad_norm': 0.20950648188591003, 'learning_rate': 6.146220570012391e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7614, 'grad_norm': 0.18203046917915344, 'learning_rate': 6.142680120375288e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4824, 'grad_norm': 0.2877122461795807, 'learning_rate': 6.139139670738184e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7571, 'grad_norm': 0.2699103057384491, 'learning_rate': 6.13559922110108e-05, 'epoch': 0.69}\n",
      "{'loss': 0.605, 'grad_norm': 0.23612189292907715, 'learning_rate': 6.132058771463976e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7279, 'grad_norm': 0.2575026750564575, 'learning_rate': 6.128518321826873e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4917, 'grad_norm': 0.15978270769119263, 'learning_rate': 6.124977872189768e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7966, 'grad_norm': 0.3158538341522217, 'learning_rate': 6.121437422552665e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6072, 'grad_norm': 0.17946013808250427, 'learning_rate': 6.117896972915561e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5414, 'grad_norm': 0.21732081472873688, 'learning_rate': 6.114356523278458e-05, 'epoch': 0.69}\n",
      "{'loss': 0.8896, 'grad_norm': 0.2847411036491394, 'learning_rate': 6.110816073641352e-05, 'epoch': 0.69}\n",
      "{'loss': 0.7749, 'grad_norm': 0.207092747092247, 'learning_rate': 6.107275624004249e-05, 'epoch': 0.69}\n",
      "{'loss': 1.0958, 'grad_norm': 0.2059173434972763, 'learning_rate': 6.103735174367144e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7108, 'grad_norm': 0.2256106436252594, 'learning_rate': 6.100194724730041e-05, 'epoch': 0.7}\n",
      "{'loss': 0.4325, 'grad_norm': 0.17489396035671234, 'learning_rate': 6.096654275092937e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7896, 'grad_norm': 0.2160474807024002, 'learning_rate': 6.093113825455833e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6567, 'grad_norm': 0.2611386775970459, 'learning_rate': 6.089573375818729e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6028, 'grad_norm': 0.200357124209404, 'learning_rate': 6.0860329261816254e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7344, 'grad_norm': 0.21382078528404236, 'learning_rate': 6.0824924765445216e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6851, 'grad_norm': 0.3072028160095215, 'learning_rate': 6.078952026907417e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9984, 'grad_norm': 0.24818439781665802, 'learning_rate': 6.075411577270313e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5942, 'grad_norm': 0.2776724398136139, 'learning_rate': 6.0718711276332094e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8147, 'grad_norm': 0.19739514589309692, 'learning_rate': 6.0683306779961055e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7941, 'grad_norm': 0.3324526846408844, 'learning_rate': 6.064790228359002e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6598, 'grad_norm': 0.18310700356960297, 'learning_rate': 6.061249778721898e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5918, 'grad_norm': 0.2552845776081085, 'learning_rate': 6.057709329084794e-05, 'epoch': 0.7}\n",
      "{'loss': 0.669, 'grad_norm': 0.21068063378334045, 'learning_rate': 6.05416887944769e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6203, 'grad_norm': 0.2614762485027313, 'learning_rate': 6.050628429810586e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5146, 'grad_norm': 0.18214792013168335, 'learning_rate': 6.0470879801734825e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9153, 'grad_norm': 0.2842564582824707, 'learning_rate': 6.0435475305363786e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9057, 'grad_norm': 0.2672848701477051, 'learning_rate': 6.040007080899275e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6201, 'grad_norm': 0.183491513133049, 'learning_rate': 6.036466631262171e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8324, 'grad_norm': 0.329802542924881, 'learning_rate': 6.032926181625067e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8944, 'grad_norm': 0.24762403964996338, 'learning_rate': 6.029385731987963e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6059, 'grad_norm': 0.20983365178108215, 'learning_rate': 6.0258452823508594e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7995, 'grad_norm': 0.2862738370895386, 'learning_rate': 6.0223048327137555e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8595, 'grad_norm': 0.24294748902320862, 'learning_rate': 6.01876438307665e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8536, 'grad_norm': 0.22801831364631653, 'learning_rate': 6.0152239334395465e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9823, 'grad_norm': 0.30339667201042175, 'learning_rate': 6.0116834838024426e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7307, 'grad_norm': 0.19397450983524323, 'learning_rate': 6.008143034165339e-05, 'epoch': 0.7}\n",
      "{'loss': 0.732, 'grad_norm': 0.2379157543182373, 'learning_rate': 6.004602584528235e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6246, 'grad_norm': 0.2092980593442917, 'learning_rate': 6.001062134891131e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8638, 'grad_norm': 0.2451627403497696, 'learning_rate': 5.997521685254027e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8819, 'grad_norm': 0.23121245205402374, 'learning_rate': 5.9939812356169234e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9433, 'grad_norm': 0.2682250440120697, 'learning_rate': 5.9904407859798196e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6821, 'grad_norm': 0.1892363727092743, 'learning_rate': 5.986900336342716e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5275, 'grad_norm': 0.21374458074569702, 'learning_rate': 5.983359886705612e-05, 'epoch': 0.7}\n",
      "{'loss': 0.9103, 'grad_norm': 0.29301008582115173, 'learning_rate': 5.979819437068508e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8072, 'grad_norm': 0.2452690750360489, 'learning_rate': 5.976278987431404e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5946, 'grad_norm': 0.21232134103775024, 'learning_rate': 5.9727385377943e-05, 'epoch': 0.7}\n",
      "{'loss': 0.627, 'grad_norm': 0.18138985335826874, 'learning_rate': 5.9691980881571965e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8615, 'grad_norm': 0.23246660828590393, 'learning_rate': 5.9656576385200926e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7107, 'grad_norm': 0.24995775520801544, 'learning_rate': 5.962117188882989e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8368, 'grad_norm': 0.23328585922718048, 'learning_rate': 5.958576739245885e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6843, 'grad_norm': 0.23333439230918884, 'learning_rate': 5.955036289608781e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7296, 'grad_norm': 0.2638983130455017, 'learning_rate': 5.951495839971677e-05, 'epoch': 0.7}\n",
      "{'loss': 0.654, 'grad_norm': 0.26516193151474, 'learning_rate': 5.947955390334573e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6655, 'grad_norm': 0.18616212904453278, 'learning_rate': 5.944414940697469e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8963, 'grad_norm': 0.26120808720588684, 'learning_rate': 5.940874491060365e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6148, 'grad_norm': 0.22371459007263184, 'learning_rate': 5.937334041423261e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7646, 'grad_norm': 0.23959898948669434, 'learning_rate': 5.9337935917861574e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0129, 'grad_norm': 0.31365343928337097, 'learning_rate': 5.9302531421490535e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5073, 'grad_norm': 0.1716686487197876, 'learning_rate': 5.92671269251195e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7787, 'grad_norm': 0.2947365343570709, 'learning_rate': 5.923172242874845e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5506, 'grad_norm': 0.24952612817287445, 'learning_rate': 5.919631793237741e-05, 'epoch': 0.7}\n",
      "{'loss': 1.0793, 'grad_norm': 0.246505469083786, 'learning_rate': 5.916091343600637e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6619, 'grad_norm': 0.24135518074035645, 'learning_rate': 5.912550893963533e-05, 'epoch': 0.7}\n",
      "{'loss': 0.8978, 'grad_norm': 0.3162094056606293, 'learning_rate': 5.909010444326429e-05, 'epoch': 0.7}\n",
      "{'loss': 1.1111, 'grad_norm': 0.2739223539829254, 'learning_rate': 5.905469994689325e-05, 'epoch': 0.7}\n",
      "{'loss': 0.5937, 'grad_norm': 0.24745838344097137, 'learning_rate': 5.9019295450522214e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7634, 'grad_norm': 0.188720241189003, 'learning_rate': 5.8983890954151175e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6242, 'grad_norm': 0.24577361345291138, 'learning_rate': 5.894848645778014e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8765, 'grad_norm': 0.20041444897651672, 'learning_rate': 5.89130819614091e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9156, 'grad_norm': 0.2786344289779663, 'learning_rate': 5.887767746503806e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8378, 'grad_norm': 0.2118869125843048, 'learning_rate': 5.884227296866702e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7427, 'grad_norm': 0.2619057595729828, 'learning_rate': 5.880686847229598e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6933, 'grad_norm': 0.25972822308540344, 'learning_rate': 5.8771463975924945e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5315, 'grad_norm': 0.20187588036060333, 'learning_rate': 5.8736059479553906e-05, 'epoch': 0.71}\n",
      "{'loss': 0.4465, 'grad_norm': 0.20930275321006775, 'learning_rate': 5.870065498318287e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6832, 'grad_norm': 0.3060407042503357, 'learning_rate': 5.866525048681183e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6822, 'grad_norm': 0.19309288263320923, 'learning_rate': 5.862984599044079e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7116, 'grad_norm': 0.23217689990997314, 'learning_rate': 5.859444149406975e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5137, 'grad_norm': 0.2045154869556427, 'learning_rate': 5.8559036997698714e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee3794f370>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 7e5911c5-5052-4c1d-93a9-c544803639cd)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5564, 'grad_norm': 0.23265300691127777, 'learning_rate': 5.8523632501327675e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7101, 'grad_norm': 0.21547646820545197, 'learning_rate': 5.848822800495664e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6537, 'grad_norm': 0.3136346936225891, 'learning_rate': 5.84528235085856e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6824, 'grad_norm': 0.2742202579975128, 'learning_rate': 5.841741901221456e-05, 'epoch': 0.71}\n",
      "{'loss': 0.611, 'grad_norm': 0.17679938673973083, 'learning_rate': 5.838201451584352e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8445, 'grad_norm': 0.22496432065963745, 'learning_rate': 5.834661001947248e-05, 'epoch': 0.71}\n",
      "{'loss': 0.662, 'grad_norm': 0.22333352267742157, 'learning_rate': 5.8311205523101445e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5, 'grad_norm': 0.23228107392787933, 'learning_rate': 5.827580102673039e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7332, 'grad_norm': 0.23300538957118988, 'learning_rate': 5.8240396530359354e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7326, 'grad_norm': 0.26597335934638977, 'learning_rate': 5.8204992033988316e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6976, 'grad_norm': 0.24116535484790802, 'learning_rate': 5.816958753761728e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8024, 'grad_norm': 0.22442059218883514, 'learning_rate': 5.813418304124624e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7496, 'grad_norm': 0.2606269419193268, 'learning_rate': 5.80987785448752e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0775, 'grad_norm': 0.20994916558265686, 'learning_rate': 5.806337404850416e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8196, 'grad_norm': 0.20263786613941193, 'learning_rate': 5.8027969552133124e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7253, 'grad_norm': 0.26640263199806213, 'learning_rate': 5.7992565055762085e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8113, 'grad_norm': 0.2539972960948944, 'learning_rate': 5.7957160559391047e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8255, 'grad_norm': 0.30962204933166504, 'learning_rate': 5.792175606302001e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5998, 'grad_norm': 0.238101989030838, 'learning_rate': 5.788635156664897e-05, 'epoch': 0.71}\n",
      "{'loss': 0.5624, 'grad_norm': 0.2234215885400772, 'learning_rate': 5.7850947070277924e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7508, 'grad_norm': 0.22157810628414154, 'learning_rate': 5.7815542573906886e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7463, 'grad_norm': 0.276643306016922, 'learning_rate': 5.778013807753585e-05, 'epoch': 0.71}\n",
      "{'loss': 0.845, 'grad_norm': 0.25962141156196594, 'learning_rate': 5.774473358116481e-05, 'epoch': 0.71}\n",
      "{'loss': 0.893, 'grad_norm': 0.28502506017684937, 'learning_rate': 5.770932908479377e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0837, 'grad_norm': 0.35173216462135315, 'learning_rate': 5.767392458842273e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6658, 'grad_norm': 0.23739935457706451, 'learning_rate': 5.7638520092051694e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7814, 'grad_norm': 0.26526039838790894, 'learning_rate': 5.7603115595680655e-05, 'epoch': 0.71}\n",
      "{'loss': 0.4715, 'grad_norm': 0.20810499787330627, 'learning_rate': 5.756771109930962e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7314, 'grad_norm': 0.2229931801557541, 'learning_rate': 5.753230660293858e-05, 'epoch': 0.71}\n",
      "{'loss': 0.58, 'grad_norm': 0.232809916138649, 'learning_rate': 5.749690210656754e-05, 'epoch': 0.71}\n",
      "{'loss': 0.825, 'grad_norm': 0.2999284863471985, 'learning_rate': 5.74614976101965e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0425, 'grad_norm': 0.3081950545310974, 'learning_rate': 5.742609311382546e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6149, 'grad_norm': 0.2571857273578644, 'learning_rate': 5.7390688617454424e-05, 'epoch': 0.71}\n",
      "{'loss': 0.708, 'grad_norm': 0.23668639361858368, 'learning_rate': 5.7355284121083386e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8153, 'grad_norm': 0.3243212401866913, 'learning_rate': 5.7319879624712334e-05, 'epoch': 0.71}\n",
      "{'loss': 0.71, 'grad_norm': 0.24480284750461578, 'learning_rate': 5.7284475128341296e-05, 'epoch': 0.71}\n",
      "{'loss': 0.6851, 'grad_norm': 0.23267263174057007, 'learning_rate': 5.724907063197026e-05, 'epoch': 0.71}\n",
      "{'loss': 0.747, 'grad_norm': 0.2360532134771347, 'learning_rate': 5.721366613559922e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7683, 'grad_norm': 0.2272951900959015, 'learning_rate': 5.717826163922818e-05, 'epoch': 0.71}\n",
      "{'loss': 0.4109, 'grad_norm': 0.24949152767658234, 'learning_rate': 5.714285714285714e-05, 'epoch': 0.71}\n",
      "{'loss': 0.7775, 'grad_norm': 0.22709091007709503, 'learning_rate': 5.71074526464861e-05, 'epoch': 0.71}\n",
      "{'loss': 0.9148, 'grad_norm': 0.24786214530467987, 'learning_rate': 5.7072048150115065e-05, 'epoch': 0.71}\n",
      "{'loss': 1.0044, 'grad_norm': 0.24831216037273407, 'learning_rate': 5.7036643653744026e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7924, 'grad_norm': 0.26801425218582153, 'learning_rate': 5.700123915737299e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7988, 'grad_norm': 0.25413674116134644, 'learning_rate': 5.696583466100195e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5463, 'grad_norm': 0.2476164698600769, 'learning_rate': 5.693043016463091e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7517, 'grad_norm': 0.21476998925209045, 'learning_rate': 5.689502566825987e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7754, 'grad_norm': 0.2402089387178421, 'learning_rate': 5.6859621171888834e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9038, 'grad_norm': 0.2484966516494751, 'learning_rate': 5.6824216675517796e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9123, 'grad_norm': 0.3174678385257721, 'learning_rate': 5.678881217914676e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7506, 'grad_norm': 0.22100500762462616, 'learning_rate': 5.675340768277572e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6229, 'grad_norm': 0.27373725175857544, 'learning_rate': 5.671800318640468e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6406, 'grad_norm': 0.19639812409877777, 'learning_rate': 5.668259869003364e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0571, 'grad_norm': 0.30578500032424927, 'learning_rate': 5.66471941936626e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7323, 'grad_norm': 0.24574054777622223, 'learning_rate': 5.6611789697291565e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0709, 'grad_norm': 0.2850882411003113, 'learning_rate': 5.6576385200920526e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1399, 'grad_norm': 0.3516572117805481, 'learning_rate': 5.654098070454948e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8269, 'grad_norm': 0.22738680243492126, 'learning_rate': 5.650557620817844e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7787, 'grad_norm': 0.23384161293506622, 'learning_rate': 5.6470171711807404e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8545, 'grad_norm': 0.26571914553642273, 'learning_rate': 5.6434767215436366e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7, 'grad_norm': 0.20358727872371674, 'learning_rate': 5.639936271906533e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7802, 'grad_norm': 0.3048248589038849, 'learning_rate': 5.636395822269428e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4901, 'grad_norm': 0.20632611215114594, 'learning_rate': 5.6328553726323244e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6143, 'grad_norm': 0.3101189434528351, 'learning_rate': 5.6293149229952205e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7558, 'grad_norm': 0.26956480741500854, 'learning_rate': 5.625774473358117e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7538, 'grad_norm': 0.2114379107952118, 'learning_rate': 5.622234023721013e-05, 'epoch': 0.72}\n",
      "{'loss': 0.864, 'grad_norm': 0.32408028841018677, 'learning_rate': 5.618693574083908e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7564, 'grad_norm': 0.21445246040821075, 'learning_rate': 5.6151531244468045e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7042, 'grad_norm': 0.20727410912513733, 'learning_rate': 5.6116126748097006e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7794, 'grad_norm': 0.23279249668121338, 'learning_rate': 5.608072225172597e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6031, 'grad_norm': 0.22856265306472778, 'learning_rate': 5.604531775535493e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8644, 'grad_norm': 0.2680451571941376, 'learning_rate': 5.600991325898389e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5491, 'grad_norm': 0.19456495344638824, 'learning_rate': 5.597450876261285e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4309, 'grad_norm': 0.2233654111623764, 'learning_rate': 5.5939104266241814e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6273, 'grad_norm': 0.2395772486925125, 'learning_rate': 5.5903699769870775e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7854, 'grad_norm': 0.2644200026988983, 'learning_rate': 5.586829527349974e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6297, 'grad_norm': 0.3002878427505493, 'learning_rate': 5.58328907771287e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6103, 'grad_norm': 0.23503915965557098, 'learning_rate': 5.579748628075766e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8895, 'grad_norm': 0.24210500717163086, 'learning_rate': 5.576208178438662e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0311, 'grad_norm': 0.25669997930526733, 'learning_rate': 5.572667728801558e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1035, 'grad_norm': 0.29908961057662964, 'learning_rate': 5.5691272791644545e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8673, 'grad_norm': 0.220021054148674, 'learning_rate': 5.5655868295273506e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8505, 'grad_norm': 0.20381245017051697, 'learning_rate': 5.562046379890247e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8914, 'grad_norm': 0.25820696353912354, 'learning_rate': 5.558505930253143e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5685, 'grad_norm': 0.255930095911026, 'learning_rate': 5.554965480616039e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6232, 'grad_norm': 0.24566985666751862, 'learning_rate': 5.551425030978935e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8501, 'grad_norm': 0.26098451018333435, 'learning_rate': 5.5478845813418314e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6642, 'grad_norm': 0.20763154327869415, 'learning_rate': 5.544344131704726e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6995, 'grad_norm': 0.22047019004821777, 'learning_rate': 5.5408036820676223e-05, 'epoch': 0.72}\n",
      "{'loss': 0.754, 'grad_norm': 0.24263645708560944, 'learning_rate': 5.5372632324305185e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7333, 'grad_norm': 0.2674524784088135, 'learning_rate': 5.5337227827934146e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0517, 'grad_norm': 0.29966551065444946, 'learning_rate': 5.530182333156311e-05, 'epoch': 0.72}\n",
      "{'loss': 1.081, 'grad_norm': 0.22909952700138092, 'learning_rate': 5.526641883519207e-05, 'epoch': 0.72}\n",
      "{'loss': 0.8516, 'grad_norm': 0.2511419355869293, 'learning_rate': 5.523101433882103e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6578, 'grad_norm': 0.20377036929130554, 'learning_rate': 5.519560984244999e-05, 'epoch': 0.72}\n",
      "{'loss': 0.7005, 'grad_norm': 0.21856273710727692, 'learning_rate': 5.5160205346078954e-05, 'epoch': 0.72}\n",
      "{'loss': 0.9271, 'grad_norm': 0.2570599913597107, 'learning_rate': 5.5124800849707916e-05, 'epoch': 0.72}\n",
      "{'loss': 1.1597, 'grad_norm': 0.2729952037334442, 'learning_rate': 5.508939635333688e-05, 'epoch': 0.72}\n",
      "{'loss': 0.681, 'grad_norm': 0.243081733584404, 'learning_rate': 5.505399185696584e-05, 'epoch': 0.72}\n",
      "{'loss': 0.5094, 'grad_norm': 0.21042625606060028, 'learning_rate': 5.50185873605948e-05, 'epoch': 0.73}\n",
      "{'loss': 0.691, 'grad_norm': 0.23259426653385162, 'learning_rate': 5.498318286422376e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7671, 'grad_norm': 0.19210077822208405, 'learning_rate': 5.4947778367852723e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0385, 'grad_norm': 0.34846264123916626, 'learning_rate': 5.491237387148168e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7378, 'grad_norm': 0.27174845337867737, 'learning_rate': 5.487696937511064e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7487, 'grad_norm': 0.16298240423202515, 'learning_rate': 5.48415648787396e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6923, 'grad_norm': 0.2012781947851181, 'learning_rate': 5.480616038236856e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6981, 'grad_norm': 0.305329829454422, 'learning_rate': 5.4770755885997524e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8555, 'grad_norm': 0.21094679832458496, 'learning_rate': 5.4735351389626486e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8663, 'grad_norm': 0.25723403692245483, 'learning_rate': 5.469994689325545e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7231, 'grad_norm': 0.23864208161830902, 'learning_rate': 5.466454239688441e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8324, 'grad_norm': 0.20882989466190338, 'learning_rate': 5.462913790051337e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6746, 'grad_norm': 0.20135465264320374, 'learning_rate': 5.459373340414233e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8384, 'grad_norm': 0.27597659826278687, 'learning_rate': 5.4558328907771294e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9963, 'grad_norm': 0.25968196988105774, 'learning_rate': 5.4522924411400255e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7523, 'grad_norm': 0.22664813697338104, 'learning_rate': 5.44875199150292e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8861, 'grad_norm': 0.23224610090255737, 'learning_rate': 5.4452115418658165e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5249, 'grad_norm': 0.18149493634700775, 'learning_rate': 5.4416710922287126e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0823, 'grad_norm': 0.3245009779930115, 'learning_rate': 5.438130642591609e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6225, 'grad_norm': 0.17793017625808716, 'learning_rate': 5.434590192954505e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8293, 'grad_norm': 0.2481752336025238, 'learning_rate': 5.431049743317401e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6575, 'grad_norm': 0.23498806357383728, 'learning_rate': 5.427509293680297e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7999, 'grad_norm': 0.19464632868766785, 'learning_rate': 5.4239688440431934e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9983, 'grad_norm': 0.27450087666511536, 'learning_rate': 5.4204283944060896e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7124, 'grad_norm': 0.22648531198501587, 'learning_rate': 5.416887944768986e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7114, 'grad_norm': 0.19900257885456085, 'learning_rate': 5.413347495131882e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6432, 'grad_norm': 0.2755153179168701, 'learning_rate': 5.409807045494778e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5989, 'grad_norm': 0.19919823110103607, 'learning_rate': 5.406266595857674e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7108, 'grad_norm': 0.22512561082839966, 'learning_rate': 5.40272614622057e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8981, 'grad_norm': 0.2533224821090698, 'learning_rate': 5.3991856965834665e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5581, 'grad_norm': 0.2119235396385193, 'learning_rate': 5.3956452469463626e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7608, 'grad_norm': 0.20093192160129547, 'learning_rate': 5.392104797309259e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5564, 'grad_norm': 0.1811978965997696, 'learning_rate': 5.388564347672155e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2178, 'grad_norm': 0.2711006999015808, 'learning_rate': 5.385023898035051e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8493, 'grad_norm': 0.24928399920463562, 'learning_rate': 5.381483448397947e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8516, 'grad_norm': 0.28090807795524597, 'learning_rate': 5.3779429987608434e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6099, 'grad_norm': 0.22621914744377136, 'learning_rate': 5.3744025491237396e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5674, 'grad_norm': 0.16193319857120514, 'learning_rate': 5.370862099486636e-05, 'epoch': 0.73}\n",
      "{'loss': 0.7251, 'grad_norm': 0.21432019770145416, 'learning_rate': 5.367321649849532e-05, 'epoch': 0.73}\n",
      "{'loss': 0.699, 'grad_norm': 0.256307452917099, 'learning_rate': 5.363781200212428e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6348, 'grad_norm': 0.22417473793029785, 'learning_rate': 5.3602407505753235e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6265, 'grad_norm': 0.1761314868927002, 'learning_rate': 5.3567003009382196e-05, 'epoch': 0.73}\n",
      "{'loss': 0.5547, 'grad_norm': 0.22889728844165802, 'learning_rate': 5.353159851301115e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8583, 'grad_norm': 0.2512451410293579, 'learning_rate': 5.349619401664011e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8336, 'grad_norm': 0.2654885947704315, 'learning_rate': 5.3460789520269074e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8742, 'grad_norm': 0.1769106090068817, 'learning_rate': 5.3425385023898036e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6212, 'grad_norm': 0.1830105185508728, 'learning_rate': 5.3389980527527e-05, 'epoch': 0.73}\n",
      "{'loss': 1.0635, 'grad_norm': 0.3112504780292511, 'learning_rate': 5.335457603115596e-05, 'epoch': 0.73}\n",
      "{'loss': 0.8071, 'grad_norm': 0.23362351953983307, 'learning_rate': 5.331917153478492e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6908, 'grad_norm': 0.23602518439292908, 'learning_rate': 5.328376703841388e-05, 'epoch': 0.73}\n",
      "{'loss': 0.4548, 'grad_norm': 0.20636692643165588, 'learning_rate': 5.324836254204284e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6098, 'grad_norm': 0.250903457403183, 'learning_rate': 5.32129580456718e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9361, 'grad_norm': 0.3284326493740082, 'learning_rate': 5.317755354930076e-05, 'epoch': 0.73}\n",
      "{'loss': 1.2008, 'grad_norm': 0.3011957108974457, 'learning_rate': 5.314214905292972e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9511, 'grad_norm': 0.2336617410182953, 'learning_rate': 5.310674455655868e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6368, 'grad_norm': 0.20507735013961792, 'learning_rate': 5.3071340060187645e-05, 'epoch': 0.73}\n",
      "{'loss': 0.9209, 'grad_norm': 0.22854332625865936, 'learning_rate': 5.3035935563816606e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6179, 'grad_norm': 0.20245762169361115, 'learning_rate': 5.300053106744557e-05, 'epoch': 0.74}\n",
      "{'loss': 0.69, 'grad_norm': 0.21540120244026184, 'learning_rate': 5.296512657107453e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8809, 'grad_norm': 0.23661582171916962, 'learning_rate': 5.292972207470349e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7122, 'grad_norm': 0.2633267045021057, 'learning_rate': 5.289431757833245e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6868, 'grad_norm': 0.20735324919223785, 'learning_rate': 5.2858913081961414e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6446, 'grad_norm': 0.20743772387504578, 'learning_rate': 5.2823508585590375e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5426, 'grad_norm': 0.18829461932182312, 'learning_rate': 5.278810408921934e-05, 'epoch': 0.74}\n",
      "{'loss': 0.817, 'grad_norm': 0.19389396905899048, 'learning_rate': 5.27526995928483e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6572, 'grad_norm': 0.2783794403076172, 'learning_rate': 5.271729509647726e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5368, 'grad_norm': 0.16537068784236908, 'learning_rate': 5.268189060010622e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8032, 'grad_norm': 0.2297801822423935, 'learning_rate': 5.264648610373518e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6379, 'grad_norm': 0.27672407031059265, 'learning_rate': 5.2611081607364145e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6893, 'grad_norm': 0.24901054799556732, 'learning_rate': 5.257567711099309e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5845, 'grad_norm': 0.20630408823490143, 'learning_rate': 5.2540272614622054e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6674, 'grad_norm': 0.241599902510643, 'learning_rate': 5.2504868118251016e-05, 'epoch': 0.74}\n",
      "{'loss': 1.0123, 'grad_norm': 0.24813228845596313, 'learning_rate': 5.246946362187998e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7077, 'grad_norm': 0.23204560577869415, 'learning_rate': 5.243405912550894e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6254, 'grad_norm': 0.22502169013023376, 'learning_rate': 5.23986546291379e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6183, 'grad_norm': 0.22508452832698822, 'learning_rate': 5.236325013276686e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7993, 'grad_norm': 0.20308761298656464, 'learning_rate': 5.232784563639582e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5961, 'grad_norm': 0.1765657663345337, 'learning_rate': 5.2292441140024785e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6533, 'grad_norm': 0.24815422296524048, 'learning_rate': 5.2257036643653746e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6471, 'grad_norm': 0.22009961307048798, 'learning_rate': 5.222163214728271e-05, 'epoch': 0.74}\n",
      "{'loss': 0.591, 'grad_norm': 0.18735459446907043, 'learning_rate': 5.218622765091167e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7499, 'grad_norm': 0.2606995105743408, 'learning_rate': 5.215082315454063e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7816, 'grad_norm': 0.18750037252902985, 'learning_rate': 5.211541865816959e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8372, 'grad_norm': 0.2367885261774063, 'learning_rate': 5.2080014161798554e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5172, 'grad_norm': 0.18101149797439575, 'learning_rate': 5.2044609665427516e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8902, 'grad_norm': 0.22060929238796234, 'learning_rate': 5.200920516905648e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7053, 'grad_norm': 0.25989145040512085, 'learning_rate': 5.197380067268544e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7981, 'grad_norm': 0.24506549537181854, 'learning_rate': 5.1938396176314394e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5819, 'grad_norm': 0.2180926650762558, 'learning_rate': 5.1902991679943355e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8594, 'grad_norm': 0.2543804943561554, 'learning_rate': 5.186758718357232e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6854, 'grad_norm': 0.22585169970989227, 'learning_rate': 5.183218268720128e-05, 'epoch': 0.74}\n",
      "{'loss': 0.9167, 'grad_norm': 0.24327492713928223, 'learning_rate': 5.179677819083024e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8345, 'grad_norm': 0.2409321516752243, 'learning_rate': 5.17613736944592e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5671, 'grad_norm': 0.25143882632255554, 'learning_rate': 5.172596919808816e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5178, 'grad_norm': 0.20427092909812927, 'learning_rate': 5.1690564701717124e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8672, 'grad_norm': 0.22879067063331604, 'learning_rate': 5.165516020534608e-05, 'epoch': 0.74}\n",
      "{'loss': 0.8482, 'grad_norm': 0.23106226325035095, 'learning_rate': 5.1619755708975034e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6702, 'grad_norm': 0.29009512066841125, 'learning_rate': 5.1584351212603995e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7837, 'grad_norm': 0.33491453528404236, 'learning_rate': 5.154894671623296e-05, 'epoch': 0.74}\n",
      "{'loss': 0.725, 'grad_norm': 0.21353642642498016, 'learning_rate': 5.151354221986192e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7349, 'grad_norm': 0.20453201234340668, 'learning_rate': 5.147813772349088e-05, 'epoch': 0.74}\n",
      "{'loss': 0.543, 'grad_norm': 0.19967970252037048, 'learning_rate': 5.144273322711984e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7146, 'grad_norm': 0.3071736693382263, 'learning_rate': 5.14073287307488e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7027, 'grad_norm': 0.21448718011379242, 'learning_rate': 5.1371924234377765e-05, 'epoch': 0.74}\n",
      "{'loss': 0.5889, 'grad_norm': 0.17005954682826996, 'learning_rate': 5.1336519738006726e-05, 'epoch': 0.74}\n",
      "{'loss': 0.7634, 'grad_norm': 0.21074151992797852, 'learning_rate': 5.130111524163569e-05, 'epoch': 0.74}\n",
      "{'loss': 0.9414, 'grad_norm': 0.27384716272354126, 'learning_rate': 5.126571074526465e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6647, 'grad_norm': 0.20134419202804565, 'learning_rate': 5.123030624889361e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6711, 'grad_norm': 0.24679219722747803, 'learning_rate': 5.119490175252257e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3988, 'grad_norm': 0.1558281034231186, 'learning_rate': 5.1159497256151534e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6835, 'grad_norm': 0.2991064488887787, 'learning_rate': 5.1124092759780495e-05, 'epoch': 0.74}\n",
      "{'loss': 0.4952, 'grad_norm': 0.22242888808250427, 'learning_rate': 5.108868826340946e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6725, 'grad_norm': 0.259578675031662, 'learning_rate': 5.105328376703842e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6413, 'grad_norm': 0.2339317947626114, 'learning_rate': 5.101787927066738e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6749, 'grad_norm': 0.24507877230644226, 'learning_rate': 5.098247477429634e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8373, 'grad_norm': 0.26741790771484375, 'learning_rate': 5.09470702779253e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5353, 'grad_norm': 0.1815384328365326, 'learning_rate': 5.0911665781554265e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7729, 'grad_norm': 0.22762146592140198, 'learning_rate': 5.0876261285183226e-05, 'epoch': 0.75}\n",
      "{'loss': 1.011, 'grad_norm': 0.22295376658439636, 'learning_rate': 5.084085678881219e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8024, 'grad_norm': 0.21388524770736694, 'learning_rate': 5.080545229244115e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9779, 'grad_norm': 0.29457157850265503, 'learning_rate': 5.077004779607011e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7289, 'grad_norm': 0.23957739770412445, 'learning_rate': 5.073464329969907e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6711, 'grad_norm': 0.23476432263851166, 'learning_rate': 5.069923880332802e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5799, 'grad_norm': 0.24327881634235382, 'learning_rate': 5.066383430695698e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8774, 'grad_norm': 0.21825648844242096, 'learning_rate': 5.0628429810585943e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9037, 'grad_norm': 0.247677281498909, 'learning_rate': 5.0593025314214905e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6716, 'grad_norm': 0.2149602770805359, 'learning_rate': 5.0557620817843867e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6665, 'grad_norm': 0.20766985416412354, 'learning_rate': 5.052221632147283e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8985, 'grad_norm': 0.264848530292511, 'learning_rate': 5.048681182510179e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6862, 'grad_norm': 0.30889108777046204, 'learning_rate': 5.045140732873075e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6357, 'grad_norm': 0.19518537819385529, 'learning_rate': 5.041600283235971e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6278, 'grad_norm': 0.2530537545681, 'learning_rate': 5.0380598335988674e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6326, 'grad_norm': 0.23449572920799255, 'learning_rate': 5.0345193839617636e-05, 'epoch': 0.75}\n",
      "{'loss': 0.766, 'grad_norm': 0.2508367598056793, 'learning_rate': 5.030978934324659e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7706, 'grad_norm': 0.2614955008029938, 'learning_rate': 5.027438484687555e-05, 'epoch': 0.75}\n",
      "{'loss': 0.68, 'grad_norm': 0.2269824892282486, 'learning_rate': 5.0238980350504514e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7271, 'grad_norm': 0.2066602259874344, 'learning_rate': 5.0203575854133475e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6341, 'grad_norm': 0.23410099744796753, 'learning_rate': 5.016817135776244e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6662, 'grad_norm': 0.2064169943332672, 'learning_rate': 5.01327668613914e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8013, 'grad_norm': 0.3015519380569458, 'learning_rate': 5.009736236502036e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9329, 'grad_norm': 0.24814218282699585, 'learning_rate': 5.006195786864932e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6867, 'grad_norm': 0.39821913838386536, 'learning_rate': 5.002655337227828e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8121, 'grad_norm': 0.30675873160362244, 'learning_rate': 4.999114887590724e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6963, 'grad_norm': 0.22879351675510406, 'learning_rate': 4.99557443795362e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6953, 'grad_norm': 0.18989838659763336, 'learning_rate': 4.992033988316516e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8696, 'grad_norm': 0.309394896030426, 'learning_rate': 4.988493538679412e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8662, 'grad_norm': 0.3797967731952667, 'learning_rate': 4.9849530890423084e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6623, 'grad_norm': 0.1874791979789734, 'learning_rate': 4.9814126394052045e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8366, 'grad_norm': 0.2113407850265503, 'learning_rate': 4.977872189768101e-05, 'epoch': 0.75}\n",
      "{'loss': 0.4893, 'grad_norm': 0.19218756258487701, 'learning_rate': 4.974331740130997e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5694, 'grad_norm': 0.1658695638179779, 'learning_rate': 4.970791290493893e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8965, 'grad_norm': 0.27385419607162476, 'learning_rate': 4.967250840856789e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5667, 'grad_norm': 0.1671850085258484, 'learning_rate': 4.963710391219685e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8288, 'grad_norm': 0.2449873983860016, 'learning_rate': 4.9601699415825815e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6474, 'grad_norm': 0.24139904975891113, 'learning_rate': 4.9566294919454776e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8264, 'grad_norm': 0.29909104108810425, 'learning_rate': 4.953089042308374e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8282, 'grad_norm': 0.20792584121227264, 'learning_rate': 4.949548592671269e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0023, 'grad_norm': 0.2886385917663574, 'learning_rate': 4.9460081430341654e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6388, 'grad_norm': 0.22104720771312714, 'learning_rate': 4.9424676933970616e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6604, 'grad_norm': 0.1874617338180542, 'learning_rate': 4.938927243759958e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7183, 'grad_norm': 0.23318976163864136, 'learning_rate': 4.935386794122854e-05, 'epoch': 0.75}\n",
      "{'loss': 0.8393, 'grad_norm': 0.2694608271121979, 'learning_rate': 4.93184634448575e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7133, 'grad_norm': 0.3185743987560272, 'learning_rate': 4.928305894848646e-05, 'epoch': 0.75}\n",
      "{'loss': 0.5813, 'grad_norm': 0.22769109904766083, 'learning_rate': 4.924765445211542e-05, 'epoch': 0.75}\n",
      "{'loss': 0.9225, 'grad_norm': 0.25137367844581604, 'learning_rate': 4.9212249955744385e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6463, 'grad_norm': 0.21402804553508759, 'learning_rate': 4.9176845459373346e-05, 'epoch': 0.75}\n",
      "{'loss': 0.839, 'grad_norm': 0.2070690393447876, 'learning_rate': 4.914144096300231e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6472, 'grad_norm': 0.1863536387681961, 'learning_rate': 4.910603646663127e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7468, 'grad_norm': 0.29308220744132996, 'learning_rate': 4.907063197026023e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7914, 'grad_norm': 0.3186492621898651, 'learning_rate': 4.9035227473889186e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6666, 'grad_norm': 0.2007155567407608, 'learning_rate': 4.899982297751815e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9395, 'grad_norm': 0.2026582807302475, 'learning_rate': 4.896441848114711e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8822, 'grad_norm': 0.2018374502658844, 'learning_rate': 4.892901398477607e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7165, 'grad_norm': 0.23243071138858795, 'learning_rate': 4.889360948840503e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6931, 'grad_norm': 0.2507465183734894, 'learning_rate': 4.8858204992033993e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8739, 'grad_norm': 0.22514915466308594, 'learning_rate': 4.882280049566295e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7013, 'grad_norm': 0.20733602344989777, 'learning_rate': 4.878739599929191e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4943, 'grad_norm': 0.19542497396469116, 'learning_rate': 4.875199150292087e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5133, 'grad_norm': 0.26783981919288635, 'learning_rate': 4.871658700654983e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8351, 'grad_norm': 0.23237818479537964, 'learning_rate': 4.8681182510178794e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7461, 'grad_norm': 0.2138732373714447, 'learning_rate': 4.8645778013807756e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7181, 'grad_norm': 0.23261846601963043, 'learning_rate': 4.861037351743672e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8902, 'grad_norm': 0.2838747203350067, 'learning_rate': 4.857496902106567e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6593, 'grad_norm': 0.2267215996980667, 'learning_rate': 4.8539564524694634e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7416, 'grad_norm': 0.22396548092365265, 'learning_rate': 4.8504160028323595e-05, 'epoch': 0.76}\n",
      "{'loss': 0.825, 'grad_norm': 0.3305215537548065, 'learning_rate': 4.846875553195256e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0725, 'grad_norm': 0.28277915716171265, 'learning_rate': 4.843335103558152e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0189, 'grad_norm': 0.2828715443611145, 'learning_rate': 4.839794653921048e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7874, 'grad_norm': 0.20197449624538422, 'learning_rate': 4.836254204283944e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7185, 'grad_norm': 0.2880040407180786, 'learning_rate': 4.83271375464684e-05, 'epoch': 0.76}\n",
      "{'loss': 0.929, 'grad_norm': 0.26649782061576843, 'learning_rate': 4.8291733050097365e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6973, 'grad_norm': 0.278575599193573, 'learning_rate': 4.8256328553726326e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7568, 'grad_norm': 0.2781200706958771, 'learning_rate': 4.822092405735529e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6504, 'grad_norm': 0.2764601707458496, 'learning_rate': 4.818551956098425e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7288, 'grad_norm': 0.31792929768562317, 'learning_rate': 4.815011506461321e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5946, 'grad_norm': 0.2830114960670471, 'learning_rate': 4.811471056824217e-05, 'epoch': 0.76}\n",
      "{'loss': 0.762, 'grad_norm': 0.2643973231315613, 'learning_rate': 4.807930607187113e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8309, 'grad_norm': 0.21523596346378326, 'learning_rate': 4.804390157550009e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7332, 'grad_norm': 0.2027071714401245, 'learning_rate': 4.800849707912905e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7619, 'grad_norm': 0.24987728893756866, 'learning_rate': 4.797309258275801e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6699, 'grad_norm': 0.21951302886009216, 'learning_rate': 4.793768808638697e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6554, 'grad_norm': 0.18597930669784546, 'learning_rate': 4.7902283590015935e-05, 'epoch': 0.76}\n",
      "{'loss': 0.9258, 'grad_norm': 0.20884069800376892, 'learning_rate': 4.7866879093644896e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0924, 'grad_norm': 0.3296510577201843, 'learning_rate': 4.783147459727386e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6381, 'grad_norm': 0.20538459718227386, 'learning_rate': 4.779607010090282e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5824, 'grad_norm': 0.2302371859550476, 'learning_rate': 4.776066560453178e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4402, 'grad_norm': 0.1890728771686554, 'learning_rate': 4.772526110816074e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6727, 'grad_norm': 0.2472945898771286, 'learning_rate': 4.7689856611789704e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6143, 'grad_norm': 0.26629602909088135, 'learning_rate': 4.7654452115418666e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5699, 'grad_norm': 0.3038835823535919, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6227, 'grad_norm': 0.2163163423538208, 'learning_rate': 4.758364312267658e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8669, 'grad_norm': 0.21349705755710602, 'learning_rate': 4.7548238626305543e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0055, 'grad_norm': 0.24092334508895874, 'learning_rate': 4.7512834129934505e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8415, 'grad_norm': 0.29131412506103516, 'learning_rate': 4.7477429633563467e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6453, 'grad_norm': 0.18437092006206512, 'learning_rate': 4.744202513719243e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5746, 'grad_norm': 0.17979231476783752, 'learning_rate': 4.740662064082139e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5601, 'grad_norm': 0.27673035860061646, 'learning_rate': 4.7371216144450344e-05, 'epoch': 0.76}\n",
      "{'loss': 0.8027, 'grad_norm': 0.21180982887744904, 'learning_rate': 4.7335811648079306e-05, 'epoch': 0.76}\n",
      "{'loss': 1.0681, 'grad_norm': 0.246710866689682, 'learning_rate': 4.730040715170827e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5588, 'grad_norm': 0.21856589615345, 'learning_rate': 4.726500265533723e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7485, 'grad_norm': 0.24189157783985138, 'learning_rate': 4.722959815896619e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6131, 'grad_norm': 0.19300861656665802, 'learning_rate': 4.719419366259515e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6554, 'grad_norm': 0.24112065136432648, 'learning_rate': 4.7158789166224114e-05, 'epoch': 0.76}\n",
      "{'loss': 0.7332, 'grad_norm': 0.20740412175655365, 'learning_rate': 4.712338466985307e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6997, 'grad_norm': 0.27469366788864136, 'learning_rate': 4.708798017348203e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6193, 'grad_norm': 0.1881265491247177, 'learning_rate': 4.705257567711099e-05, 'epoch': 0.76}\n",
      "{'loss': 0.726, 'grad_norm': 0.23701666295528412, 'learning_rate': 4.701717118073995e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5266, 'grad_norm': 0.22068528831005096, 'learning_rate': 4.6981766684368915e-05, 'epoch': 0.77}\n",
      "{'loss': 0.8014, 'grad_norm': 0.20851296186447144, 'learning_rate': 4.6946362187997876e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4969, 'grad_norm': 0.17769759893417358, 'learning_rate': 4.691095769162684e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7344, 'grad_norm': 0.2522861659526825, 'learning_rate': 4.68755531952558e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7738, 'grad_norm': 0.2402581423521042, 'learning_rate': 4.684014869888476e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7318, 'grad_norm': 0.34508827328681946, 'learning_rate': 4.680474420251372e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6045, 'grad_norm': 0.2176169753074646, 'learning_rate': 4.6769339706142684e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6189, 'grad_norm': 0.238835871219635, 'learning_rate': 4.6733935209771645e-05, 'epoch': 0.77}\n",
      "{'loss': 0.522, 'grad_norm': 0.16034191846847534, 'learning_rate': 4.669853071340061e-05, 'epoch': 0.77}\n",
      "{'loss': 0.899, 'grad_norm': 0.20009468495845795, 'learning_rate': 4.666312621702956e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5659, 'grad_norm': 0.20118191838264465, 'learning_rate': 4.662772172065852e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6306, 'grad_norm': 0.2195182889699936, 'learning_rate': 4.6592317224287485e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9776, 'grad_norm': 0.27590101957321167, 'learning_rate': 4.6556912727916446e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6792, 'grad_norm': 0.22802555561065674, 'learning_rate': 4.652150823154541e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7302, 'grad_norm': 0.21068361401557922, 'learning_rate': 4.648610373517437e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5707, 'grad_norm': 0.23077929019927979, 'learning_rate': 4.645069923880333e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9977, 'grad_norm': 0.28658297657966614, 'learning_rate': 4.641529474243229e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6422, 'grad_norm': 0.23104409873485565, 'learning_rate': 4.6379890246061254e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7022, 'grad_norm': 0.2122180461883545, 'learning_rate': 4.6344485749690216e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6045, 'grad_norm': 0.20402203500270844, 'learning_rate': 4.630908125331918e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5369, 'grad_norm': 0.22040249407291412, 'learning_rate': 4.627367675694814e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7406, 'grad_norm': 0.16778729856014252, 'learning_rate': 4.62382722605771e-05, 'epoch': 0.77}\n",
      "{'loss': 1.0653, 'grad_norm': 0.26659730076789856, 'learning_rate': 4.6202867764206055e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3734, 'grad_norm': 0.2627962529659271, 'learning_rate': 4.6167463267835016e-05, 'epoch': 0.77}\n",
      "{'loss': 0.8013, 'grad_norm': 0.25704437494277954, 'learning_rate': 4.613205877146398e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9096, 'grad_norm': 0.23975946009159088, 'learning_rate': 4.609665427509294e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6046, 'grad_norm': 0.23406480252742767, 'learning_rate': 4.60612497787219e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7245, 'grad_norm': 0.24010774493217468, 'learning_rate': 4.602584528235086e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6516, 'grad_norm': 0.289431095123291, 'learning_rate': 4.5990440785979824e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6904, 'grad_norm': 0.24949856102466583, 'learning_rate': 4.5955036289608786e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7723, 'grad_norm': 0.20990444719791412, 'learning_rate': 4.591963179323775e-05, 'epoch': 0.77}\n",
      "{'loss': 0.652, 'grad_norm': 0.258404016494751, 'learning_rate': 4.58842272968667e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7685, 'grad_norm': 0.328659325838089, 'learning_rate': 4.5848822800495664e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7067, 'grad_norm': 0.2449503093957901, 'learning_rate': 4.5813418304124625e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6587, 'grad_norm': 0.24443458020687103, 'learning_rate': 4.577801380775359e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6612, 'grad_norm': 0.17027728259563446, 'learning_rate': 4.574260931138255e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7931, 'grad_norm': 0.2447177618741989, 'learning_rate': 4.57072048150115e-05, 'epoch': 0.77}\n",
      "{'loss': 0.8103, 'grad_norm': 0.25774163007736206, 'learning_rate': 4.5671800318640465e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7112, 'grad_norm': 0.2102130949497223, 'learning_rate': 4.5636395822269426e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6131, 'grad_norm': 0.34325921535491943, 'learning_rate': 4.560099132589839e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6693, 'grad_norm': 0.22212454676628113, 'learning_rate': 4.556558682952735e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6135, 'grad_norm': 0.23237581551074982, 'learning_rate': 4.553018233315631e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4292, 'grad_norm': 0.20757050812244415, 'learning_rate': 4.549477783678527e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7047, 'grad_norm': 0.18736745417118073, 'learning_rate': 4.5459373340414234e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4974, 'grad_norm': 0.2015574723482132, 'learning_rate': 4.5423968844043195e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6245, 'grad_norm': 0.21739791333675385, 'learning_rate': 4.538856434767216e-05, 'epoch': 0.77}\n",
      "{'loss': 0.9517, 'grad_norm': 0.3041222393512726, 'learning_rate': 4.535315985130112e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6144, 'grad_norm': 0.1897973120212555, 'learning_rate': 4.531775535493008e-05, 'epoch': 0.77}\n",
      "{'loss': 0.8522, 'grad_norm': 0.24817857146263123, 'learning_rate': 4.528235085855904e-05, 'epoch': 0.77}\n",
      "{'loss': 0.8951, 'grad_norm': 0.25828734040260315, 'learning_rate': 4.5246946362187996e-05, 'epoch': 0.77}\n",
      "{'loss': 0.8792, 'grad_norm': 0.28011587262153625, 'learning_rate': 4.521154186581696e-05, 'epoch': 0.77}\n",
      "{'loss': 0.6802, 'grad_norm': 0.3096185624599457, 'learning_rate': 4.517613736944592e-05, 'epoch': 0.77}\n",
      "{'loss': 0.7693, 'grad_norm': 0.26020893454551697, 'learning_rate': 4.514073287307488e-05, 'epoch': 0.77}\n",
      "{'loss': 0.899, 'grad_norm': 0.22098146378993988, 'learning_rate': 4.510532837670384e-05, 'epoch': 0.77}\n",
      "{'loss': 0.5605, 'grad_norm': 0.230818510055542, 'learning_rate': 4.5069923880332804e-05, 'epoch': 0.77}\n",
      "{'loss': 0.736, 'grad_norm': 0.20817236602306366, 'learning_rate': 4.5034519383961765e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7313, 'grad_norm': 0.2909083366394043, 'learning_rate': 4.499911488759073e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9481, 'grad_norm': 0.3062288463115692, 'learning_rate': 4.496371039121969e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7314, 'grad_norm': 0.2359887808561325, 'learning_rate': 4.492830589484865e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9911, 'grad_norm': 0.28975310921669006, 'learning_rate': 4.489290139847761e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9165, 'grad_norm': 0.24366141855716705, 'learning_rate': 4.485749690210657e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5921, 'grad_norm': 0.22975067794322968, 'learning_rate': 4.4822092405735535e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6349, 'grad_norm': 0.2363417148590088, 'learning_rate': 4.478668790936449e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7525, 'grad_norm': 0.21517007052898407, 'learning_rate': 4.475128341299345e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8889, 'grad_norm': 0.22880889475345612, 'learning_rate': 4.471587891662241e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5941, 'grad_norm': 0.1936696618795395, 'learning_rate': 4.4680474420251374e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6004, 'grad_norm': 0.2695781886577606, 'learning_rate': 4.4645069923880336e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5571, 'grad_norm': 0.16985772550106049, 'learning_rate': 4.46096654275093e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5693, 'grad_norm': 0.23106662929058075, 'learning_rate': 4.457426093113826e-05, 'epoch': 0.78}\n",
      "{'loss': 1.1682, 'grad_norm': 0.3398021161556244, 'learning_rate': 4.453885643476722e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7999, 'grad_norm': 0.3668719530105591, 'learning_rate': 4.450345193839618e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7397, 'grad_norm': 0.19629278779029846, 'learning_rate': 4.4468047442025143e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7653, 'grad_norm': nan, 'learning_rate': 4.4468047442025143e-05, 'epoch': 0.78}\n",
      "{'loss': 0.792, 'grad_norm': 0.2424163520336151, 'learning_rate': 4.4432642945654105e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7608, 'grad_norm': 0.22487013041973114, 'learning_rate': 4.439723844928306e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6011, 'grad_norm': 0.20466740429401398, 'learning_rate': 4.436183395291202e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5863, 'grad_norm': 0.25264114141464233, 'learning_rate': 4.432642945654098e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6354, 'grad_norm': 0.2026386260986328, 'learning_rate': 4.4291024960169944e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5958, 'grad_norm': 0.24255280196666718, 'learning_rate': 4.42556204637989e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7353, 'grad_norm': 0.2128511667251587, 'learning_rate': 4.422021596742786e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6199, 'grad_norm': 0.20448879897594452, 'learning_rate': 4.418481147105682e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7198, 'grad_norm': 0.3043893575668335, 'learning_rate': 4.4149406974685784e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8616, 'grad_norm': 0.23720921576023102, 'learning_rate': 4.4114002478314745e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8056, 'grad_norm': 0.23283648490905762, 'learning_rate': 4.407859798194371e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8212, 'grad_norm': 0.24523431062698364, 'learning_rate': 4.404319348557267e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5555, 'grad_norm': 0.18754452466964722, 'learning_rate': 4.400778898920163e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6214, 'grad_norm': 0.24245965480804443, 'learning_rate': 4.397238449283059e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8881, 'grad_norm': 0.26017266511917114, 'learning_rate': 4.393697999645955e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5667, 'grad_norm': 0.25699108839035034, 'learning_rate': 4.3901575500088515e-05, 'epoch': 0.78}\n",
      "{'loss': 0.533, 'grad_norm': 0.22615382075309753, 'learning_rate': 4.3866171003717476e-05, 'epoch': 0.78}\n",
      "{'loss': 0.81, 'grad_norm': 0.22594144940376282, 'learning_rate': 4.383076650734643e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6159, 'grad_norm': 0.2121579349040985, 'learning_rate': 4.379536201097539e-05, 'epoch': 0.78}\n",
      "{'loss': 0.809, 'grad_norm': 0.25001364946365356, 'learning_rate': 4.3759957514604354e-05, 'epoch': 0.78}\n",
      "{'loss': 0.774, 'grad_norm': 0.23905833065509796, 'learning_rate': 4.3724553018233315e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6274, 'grad_norm': 0.22359201312065125, 'learning_rate': 4.368914852186228e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7241, 'grad_norm': 0.24416744709014893, 'learning_rate': 4.365374402549124e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7186, 'grad_norm': 0.2521377205848694, 'learning_rate': 4.36183395291202e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4864, 'grad_norm': 0.22043377161026, 'learning_rate': 4.358293503274916e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6752, 'grad_norm': 0.2470422238111496, 'learning_rate': 4.354753053637812e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5704, 'grad_norm': 0.2737707495689392, 'learning_rate': 4.3512126040007085e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9742, 'grad_norm': 0.2667539119720459, 'learning_rate': 4.3476721543636046e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4914, 'grad_norm': 0.21347689628601074, 'learning_rate': 4.344131704726501e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8261, 'grad_norm': 0.27178075909614563, 'learning_rate': 4.340591255089397e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7494, 'grad_norm': 0.19479389488697052, 'learning_rate': 4.337050805452293e-05, 'epoch': 0.78}\n",
      "{'loss': 0.9909, 'grad_norm': 0.3241599202156067, 'learning_rate': 4.3335103558151886e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6435, 'grad_norm': 0.258026123046875, 'learning_rate': 4.329969906178085e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7682, 'grad_norm': 0.22429995238780975, 'learning_rate': 4.326429456540981e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7901, 'grad_norm': 0.30458980798721313, 'learning_rate': 4.322889006903877e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8096, 'grad_norm': 0.22353090345859528, 'learning_rate': 4.319348557266773e-05, 'epoch': 0.78}\n",
      "{'loss': 0.894, 'grad_norm': 0.22717753052711487, 'learning_rate': 4.315808107629669e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6415, 'grad_norm': 0.21003314852714539, 'learning_rate': 4.3122676579925655e-05, 'epoch': 0.78}\n",
      "{'loss': 0.7636, 'grad_norm': 0.30087608098983765, 'learning_rate': 4.3087272083554616e-05, 'epoch': 0.78}\n",
      "{'loss': 0.5812, 'grad_norm': 0.2667912542819977, 'learning_rate': 4.305186758718358e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1589, 'grad_norm': 0.27537432312965393, 'learning_rate': 4.301646309081254e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7168, 'grad_norm': 0.1779685765504837, 'learning_rate': 4.29810585944415e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7034, 'grad_norm': 0.22626423835754395, 'learning_rate': 4.2945654098070456e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5014, 'grad_norm': 0.20827774703502655, 'learning_rate': 4.291024960169942e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5215, 'grad_norm': 0.23027217388153076, 'learning_rate': 4.287484510532838e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5768, 'grad_norm': 0.24692833423614502, 'learning_rate': 4.283944060895734e-05, 'epoch': 0.79}\n",
      "{'loss': 0.67, 'grad_norm': 0.21951530873775482, 'learning_rate': 4.28040361125863e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4538, 'grad_norm': 0.2214503139257431, 'learning_rate': 4.276863161621526e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7756, 'grad_norm': 0.2586142122745514, 'learning_rate': 4.273322711984422e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0474, 'grad_norm': 0.30561158061027527, 'learning_rate': 4.269782262347318e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7528, 'grad_norm': 0.17837221920490265, 'learning_rate': 4.266241812710214e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6739, 'grad_norm': 0.2209784984588623, 'learning_rate': 4.26270136307311e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4696, 'grad_norm': 0.1929548680782318, 'learning_rate': 4.2591609134360064e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7453, 'grad_norm': 0.2208864539861679, 'learning_rate': 4.2556204637989026e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9018, 'grad_norm': 0.28416886925697327, 'learning_rate': 4.252080014161799e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5934, 'grad_norm': 0.19929903745651245, 'learning_rate': 4.248539564524695e-05, 'epoch': 0.79}\n",
      "{'loss': 0.8591, 'grad_norm': 0.25738808512687683, 'learning_rate': 4.244999114887591e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0134, 'grad_norm': 0.2746621370315552, 'learning_rate': 4.2414586652504865e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7101, 'grad_norm': 0.31222015619277954, 'learning_rate': 4.237918215613383e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0845, 'grad_norm': 0.31621596217155457, 'learning_rate': 4.234377765976279e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5801, 'grad_norm': 0.2222803235054016, 'learning_rate': 4.230837316339175e-05, 'epoch': 0.79}\n",
      "{'loss': 0.8272, 'grad_norm': 0.21921010315418243, 'learning_rate': 4.227296866702071e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7657, 'grad_norm': 0.20309478044509888, 'learning_rate': 4.223756417064967e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0262, 'grad_norm': 0.2712536156177521, 'learning_rate': 4.2202159674278635e-05, 'epoch': 0.79}\n",
      "{'loss': 0.8247, 'grad_norm': 0.2321229875087738, 'learning_rate': 4.2166755177907596e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5784, 'grad_norm': 0.23763225972652435, 'learning_rate': 4.213135068153656e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0862, 'grad_norm': 0.31102848052978516, 'learning_rate': 4.209594618516552e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7036, 'grad_norm': 0.22114503383636475, 'learning_rate': 4.206054168879448e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7271, 'grad_norm': 0.28166431188583374, 'learning_rate': 4.202513719242344e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7589, 'grad_norm': 0.18686965107917786, 'learning_rate': 4.1989732696052404e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7715, 'grad_norm': 0.280393123626709, 'learning_rate': 4.1954328199681365e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6329, 'grad_norm': 0.21558420360088348, 'learning_rate': 4.191892370331032e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6034, 'grad_norm': 0.24758002161979675, 'learning_rate': 4.188351920693928e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5403, 'grad_norm': 0.21983112394809723, 'learning_rate': 4.184811471056824e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9416, 'grad_norm': 0.225913405418396, 'learning_rate': 4.1812710214197205e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6764, 'grad_norm': 0.1892377883195877, 'learning_rate': 4.1777305717826166e-05, 'epoch': 0.79}\n",
      "{'loss': 1.1663, 'grad_norm': 0.22139348089694977, 'learning_rate': 4.174190122145513e-05, 'epoch': 0.79}\n",
      "{'loss': 0.5555, 'grad_norm': 0.22549276053905487, 'learning_rate': 4.170649672508409e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7716, 'grad_norm': 0.23859550058841705, 'learning_rate': 4.167109222871305e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6395, 'grad_norm': 0.18827679753303528, 'learning_rate': 4.163568773234201e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7258, 'grad_norm': 0.21090714633464813, 'learning_rate': 4.1600283235970974e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4983, 'grad_norm': 0.1537601798772812, 'learning_rate': 4.1564878739599936e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6233, 'grad_norm': 0.1727314293384552, 'learning_rate': 4.15294742432289e-05, 'epoch': 0.79}\n",
      "{'loss': 0.8123, 'grad_norm': 0.23397359251976013, 'learning_rate': 4.149406974685786e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6773, 'grad_norm': 0.2108020931482315, 'learning_rate': 4.1458665250486813e-05, 'epoch': 0.79}\n",
      "{'loss': 1.0311, 'grad_norm': 0.32141372561454773, 'learning_rate': 4.1423260754115775e-05, 'epoch': 0.79}\n",
      "{'loss': 0.539, 'grad_norm': 0.18618009984493256, 'learning_rate': 4.1387856257744737e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7708, 'grad_norm': 0.28393784165382385, 'learning_rate': 4.13524517613737e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7186, 'grad_norm': 0.2766149342060089, 'learning_rate': 4.131704726500266e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7033, 'grad_norm': 0.29765498638153076, 'learning_rate': 4.1281642768631614e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6505, 'grad_norm': 0.2782368063926697, 'learning_rate': 4.1246238272260576e-05, 'epoch': 0.79}\n",
      "{'loss': 1.052, 'grad_norm': 0.29235902428627014, 'learning_rate': 4.121083377588954e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6319, 'grad_norm': 0.20488955080509186, 'learning_rate': 4.11754292795185e-05, 'epoch': 0.79}\n",
      "{'loss': 0.8633, 'grad_norm': 0.2976335883140564, 'learning_rate': 4.114002478314746e-05, 'epoch': 0.79}\n",
      "{'loss': 0.6923, 'grad_norm': 0.23859460651874542, 'learning_rate': 4.110462028677642e-05, 'epoch': 0.79}\n",
      "{'loss': 0.9024, 'grad_norm': 0.2613184154033661, 'learning_rate': 4.1069215790405384e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8106, 'grad_norm': 0.15557165443897247, 'learning_rate': 4.1033811294034345e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8233, 'grad_norm': 0.22200849652290344, 'learning_rate': 4.099840679766331e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8349, 'grad_norm': 0.23208166658878326, 'learning_rate': 4.096300230129226e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6595, 'grad_norm': 0.20999564230442047, 'learning_rate': 4.092759780492122e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8857, 'grad_norm': 0.46404793858528137, 'learning_rate': 4.0892193308550185e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee3794fb20>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: c0ce93d2-4184-4fc1-9bca-d18f14885eb3)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.647, 'grad_norm': 0.24235528707504272, 'learning_rate': 4.0856788812179146e-05, 'epoch': 0.8}\n",
      "{'loss': 0.683, 'grad_norm': 0.22401919960975647, 'learning_rate': 4.082138431580811e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7446, 'grad_norm': 0.23382307589054108, 'learning_rate': 4.078597981943707e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7729, 'grad_norm': 0.2826232314109802, 'learning_rate': 4.075057532306603e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5855, 'grad_norm': 0.16573365032672882, 'learning_rate': 4.071517082669499e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6149, 'grad_norm': 0.2068246454000473, 'learning_rate': 4.0679766330323954e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6292, 'grad_norm': 0.25116756558418274, 'learning_rate': 4.0644361833952915e-05, 'epoch': 0.8}\n",
      "{'loss': 0.749, 'grad_norm': 0.24593499302864075, 'learning_rate': 4.060895733758188e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7501, 'grad_norm': 0.2518158257007599, 'learning_rate': 4.057355284121084e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6009, 'grad_norm': 0.2378339171409607, 'learning_rate': 4.05381483448398e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7012, 'grad_norm': 0.1868796944618225, 'learning_rate': 4.0502743848468755e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6541, 'grad_norm': 0.29861024022102356, 'learning_rate': 4.0467339352097716e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0311, 'grad_norm': 0.2598767578601837, 'learning_rate': 4.043193485572668e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7235, 'grad_norm': 0.3001400828361511, 'learning_rate': 4.039653035935564e-05, 'epoch': 0.8}\n",
      "{'loss': 0.637, 'grad_norm': 0.26197850704193115, 'learning_rate': 4.03611258629846e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9649, 'grad_norm': 0.3419772684574127, 'learning_rate': 4.032572136661356e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7201, 'grad_norm': 0.21423085033893585, 'learning_rate': 4.0290316870242524e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8136, 'grad_norm': 0.2345428466796875, 'learning_rate': 4.0254912373871486e-05, 'epoch': 0.8}\n",
      "{'loss': 1.1184, 'grad_norm': 0.2943616807460785, 'learning_rate': 4.021950787750045e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9212, 'grad_norm': 0.24107304215431213, 'learning_rate': 4.018410338112941e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7081, 'grad_norm': 0.226378932595253, 'learning_rate': 4.014869888475837e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7378, 'grad_norm': 0.23003970086574554, 'learning_rate': 4.011329438838733e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0472, 'grad_norm': 0.27792733907699585, 'learning_rate': 4.007788989201629e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5383, 'grad_norm': 0.2073444277048111, 'learning_rate': 4.004248539564525e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0945, 'grad_norm': 0.20781981945037842, 'learning_rate': 4.000708089927421e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9963, 'grad_norm': 0.23251038789749146, 'learning_rate': 3.997167640290317e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6115, 'grad_norm': 0.19648994505405426, 'learning_rate': 3.993627190653213e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9337, 'grad_norm': 0.24841877818107605, 'learning_rate': 3.9900867410161094e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5932, 'grad_norm': 0.225864976644516, 'learning_rate': 3.9865462913790056e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7963, 'grad_norm': 0.22387588024139404, 'learning_rate': 3.983005841741901e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7416, 'grad_norm': 0.20096664130687714, 'learning_rate': 3.979465392104797e-05, 'epoch': 0.8}\n",
      "{'loss': 0.564, 'grad_norm': 0.21971575915813446, 'learning_rate': 3.9759249424676934e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9714, 'grad_norm': 0.20827095210552216, 'learning_rate': 3.9723844928305895e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6594, 'grad_norm': 0.2213633954524994, 'learning_rate': 3.968844043193486e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5862, 'grad_norm': 0.1913779228925705, 'learning_rate': 3.965303593556382e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5308, 'grad_norm': 0.17662742733955383, 'learning_rate': 3.961763143919278e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7668, 'grad_norm': 0.2286231815814972, 'learning_rate': 3.958222694282174e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7935, 'grad_norm': 0.23953188955783844, 'learning_rate': 3.9546822446450696e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8243, 'grad_norm': 0.2573235034942627, 'learning_rate': 3.951141795007966e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4787, 'grad_norm': 0.2324981391429901, 'learning_rate': 3.947601345370862e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4596, 'grad_norm': 0.20087818801403046, 'learning_rate': 3.944060895733758e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7776, 'grad_norm': 0.2393084317445755, 'learning_rate': 3.940520446096654e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4896, 'grad_norm': 0.1983567774295807, 'learning_rate': 3.9369799964595504e-05, 'epoch': 0.8}\n",
      "{'loss': 0.8642, 'grad_norm': 0.35353198647499084, 'learning_rate': 3.9334395468224465e-05, 'epoch': 0.8}\n",
      "{'loss': 0.544, 'grad_norm': 0.2188432365655899, 'learning_rate': 3.929899097185343e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7468, 'grad_norm': 0.2162323147058487, 'learning_rate': 3.926358647548239e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6658, 'grad_norm': 0.17642082273960114, 'learning_rate': 3.922818197911135e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6481, 'grad_norm': 0.23251034319400787, 'learning_rate': 3.919277748274031e-05, 'epoch': 0.8}\n",
      "{'loss': 0.7315, 'grad_norm': 0.22871075570583344, 'learning_rate': 3.915737298636927e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4064, 'grad_norm': 0.20424595475196838, 'learning_rate': 3.9121968489998235e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0594, 'grad_norm': 0.278766006231308, 'learning_rate': 3.908656399362719e-05, 'epoch': 0.8}\n",
      "{'loss': 0.9569, 'grad_norm': 0.25689855217933655, 'learning_rate': 3.905115949725615e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6538, 'grad_norm': 0.1720564365386963, 'learning_rate': 3.901575500088511e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7296, 'grad_norm': 0.22727461159229279, 'learning_rate': 3.8980350504514074e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6822, 'grad_norm': 0.22027148306369781, 'learning_rate': 3.8944946008143036e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8675, 'grad_norm': 0.2277553379535675, 'learning_rate': 3.8909541511772e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6592, 'grad_norm': 0.20702815055847168, 'learning_rate': 3.887413701540096e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6629, 'grad_norm': 0.23112715780735016, 'learning_rate': 3.883873251902992e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9105, 'grad_norm': 0.3301194906234741, 'learning_rate': 3.880332802265888e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8932, 'grad_norm': 0.19433680176734924, 'learning_rate': 3.876792352628784e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7119, 'grad_norm': 0.21529440581798553, 'learning_rate': 3.8732519029916805e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5454, 'grad_norm': 0.19745557010173798, 'learning_rate': 3.8697114533545766e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6784, 'grad_norm': 0.2137017846107483, 'learning_rate': 3.866171003717473e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7063, 'grad_norm': 0.288076251745224, 'learning_rate': 3.862630554080369e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8656, 'grad_norm': 0.24927257001399994, 'learning_rate': 3.8590901044432644e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5644, 'grad_norm': 0.22295521199703217, 'learning_rate': 3.8555496548061606e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7415, 'grad_norm': 0.23844780027866364, 'learning_rate': 3.852009205169057e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9494, 'grad_norm': 0.250731498003006, 'learning_rate': 3.848468755531953e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8812, 'grad_norm': 0.26156458258628845, 'learning_rate': 3.844928305894849e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5286, 'grad_norm': 0.20029161870479584, 'learning_rate': 3.841387856257745e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6112, 'grad_norm': 0.21006615459918976, 'learning_rate': 3.8378474066206413e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7946, 'grad_norm': 0.24767020344734192, 'learning_rate': 3.834306956983537e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7222, 'grad_norm': 0.2543688416481018, 'learning_rate': 3.830766507346433e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5412, 'grad_norm': 0.19608980417251587, 'learning_rate': 3.827226057709329e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4852, 'grad_norm': 0.4044053554534912, 'learning_rate': 3.823685608072225e-05, 'epoch': 0.81}\n",
      "{'loss': 0.739, 'grad_norm': 0.17228148877620697, 'learning_rate': 3.8201451584351214e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8063, 'grad_norm': 0.25309473276138306, 'learning_rate': 3.8166047087980176e-05, 'epoch': 0.81}\n",
      "{'loss': 0.433, 'grad_norm': 0.19136111438274384, 'learning_rate': 3.813064259160913e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6793, 'grad_norm': 0.23778413236141205, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5363, 'grad_norm': 0.18324938416481018, 'learning_rate': 3.8059833598867054e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6938, 'grad_norm': 0.1837867945432663, 'learning_rate': 3.8024429102496015e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7303, 'grad_norm': 0.2190672904253006, 'learning_rate': 3.798902460612498e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0182, 'grad_norm': 0.3017999529838562, 'learning_rate': 3.795362010975394e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6902, 'grad_norm': 0.2484358847141266, 'learning_rate': 3.79182156133829e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6916, 'grad_norm': 0.20609398186206818, 'learning_rate': 3.788281111701186e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8987, 'grad_norm': 0.26201528310775757, 'learning_rate': 3.784740662064082e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8371, 'grad_norm': 0.2299971729516983, 'learning_rate': 3.7812002124269785e-05, 'epoch': 0.81}\n",
      "{'loss': 0.56, 'grad_norm': 0.18851540982723236, 'learning_rate': 3.7776597627898746e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7071, 'grad_norm': 0.2762933075428009, 'learning_rate': 3.774119313152771e-05, 'epoch': 0.81}\n",
      "{'loss': 0.8033, 'grad_norm': 0.2793751358985901, 'learning_rate': 3.770578863515667e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0866, 'grad_norm': 0.27348604798316956, 'learning_rate': 3.7670384138785624e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6546, 'grad_norm': 0.23820768296718597, 'learning_rate': 3.7634979642414585e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7442, 'grad_norm': 0.2322039157152176, 'learning_rate': 3.759957514604355e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7886, 'grad_norm': 0.18726275861263275, 'learning_rate': 3.756417064967251e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7145, 'grad_norm': 0.2464430183172226, 'learning_rate': 3.752876615330147e-05, 'epoch': 0.81}\n",
      "{'loss': 1.0179, 'grad_norm': 0.30858129262924194, 'learning_rate': 3.749336165693043e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7633, 'grad_norm': 0.21299763023853302, 'learning_rate': 3.745795716055939e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3897, 'grad_norm': 0.2507016062736511, 'learning_rate': 3.7422552664188355e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4761, 'grad_norm': 0.19813847541809082, 'learning_rate': 3.7387148167817316e-05, 'epoch': 0.81}\n",
      "{'loss': 0.9434, 'grad_norm': 0.284162312746048, 'learning_rate': 3.735174367144628e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5693, 'grad_norm': 0.2228691726922989, 'learning_rate': 3.731633917507524e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5962, 'grad_norm': 0.2325279265642166, 'learning_rate': 3.72809346787042e-05, 'epoch': 0.81}\n",
      "{'loss': 0.6268, 'grad_norm': 0.19394788146018982, 'learning_rate': 3.724553018233316e-05, 'epoch': 0.81}\n",
      "{'loss': 0.7925, 'grad_norm': 0.24755051732063293, 'learning_rate': 3.7210125685962124e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5952, 'grad_norm': 0.22254954278469086, 'learning_rate': 3.717472118959108e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5226, 'grad_norm': 0.18631009757518768, 'learning_rate': 3.713931669322004e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4932, 'grad_norm': 0.20020349323749542, 'learning_rate': 3.7103912196849e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5697, 'grad_norm': 0.19830147922039032, 'learning_rate': 3.7068507700477963e-05, 'epoch': 0.81}\n",
      "{'loss': 0.5432, 'grad_norm': 0.22685204446315765, 'learning_rate': 3.7033103204106925e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8422, 'grad_norm': 0.3185458481311798, 'learning_rate': 3.6997698707735886e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6935, 'grad_norm': 0.24443206191062927, 'learning_rate': 3.696229421136485e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8659, 'grad_norm': 0.22707751393318176, 'learning_rate': 3.692688971499381e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8422, 'grad_norm': 0.2949652373790741, 'learning_rate': 3.689148521862277e-05, 'epoch': 0.82}\n",
      "{'loss': 1.2347, 'grad_norm': 0.24504221975803375, 'learning_rate': 3.6856080722251726e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9012, 'grad_norm': 0.20811480283737183, 'learning_rate': 3.682067622588069e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6672, 'grad_norm': 0.255799800157547, 'learning_rate': 3.678527172950965e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5144, 'grad_norm': 0.1787392646074295, 'learning_rate': 3.674986723313861e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6325, 'grad_norm': 0.20395945012569427, 'learning_rate': 3.6714462736767565e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9026, 'grad_norm': 0.2705729603767395, 'learning_rate': 3.667905824039653e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6102, 'grad_norm': 0.26367804408073425, 'learning_rate': 3.664365374402549e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9872, 'grad_norm': 0.24675950407981873, 'learning_rate': 3.660824924765445e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8702, 'grad_norm': 0.34041541814804077, 'learning_rate': 3.657284475128341e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8639, 'grad_norm': 0.3001708984375, 'learning_rate': 3.653744025491237e-05, 'epoch': 0.82}\n",
      "{'loss': 0.972, 'grad_norm': 0.25955674052238464, 'learning_rate': 3.6502035758541335e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7199, 'grad_norm': 0.20363342761993408, 'learning_rate': 3.6466631262170296e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6916, 'grad_norm': 0.24833738803863525, 'learning_rate': 3.643122676579926e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5754, 'grad_norm': 0.21396799385547638, 'learning_rate': 3.639582226942822e-05, 'epoch': 0.82}\n",
      "{'loss': 0.892, 'grad_norm': 0.2640702426433563, 'learning_rate': 3.636041777305718e-05, 'epoch': 0.82}\n",
      "{'loss': 0.78, 'grad_norm': 0.28767260909080505, 'learning_rate': 3.632501327668614e-05, 'epoch': 0.82}\n",
      "{'loss': 0.694, 'grad_norm': 0.22917966544628143, 'learning_rate': 3.6289608780315104e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7068, 'grad_norm': 0.22336232662200928, 'learning_rate': 3.625420428394406e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8422, 'grad_norm': 0.30600136518478394, 'learning_rate': 3.621879978757302e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6508, 'grad_norm': 0.2089163362979889, 'learning_rate': 3.618339529120198e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4547, 'grad_norm': 0.22798393666744232, 'learning_rate': 3.614799079483094e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9205, 'grad_norm': 0.28618043661117554, 'learning_rate': 3.6112586298459905e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7468, 'grad_norm': 0.25378409028053284, 'learning_rate': 3.6077181802088866e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9078, 'grad_norm': 0.2138020545244217, 'learning_rate': 3.604177730571783e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8692, 'grad_norm': 0.23709343373775482, 'learning_rate': 3.600637280934679e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7547, 'grad_norm': 0.21126164495944977, 'learning_rate': 3.597096831297575e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5862, 'grad_norm': 0.20693586766719818, 'learning_rate': 3.593556381660471e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7261, 'grad_norm': 0.25686705112457275, 'learning_rate': 3.5900159320233674e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7922, 'grad_norm': 0.28187718987464905, 'learning_rate': 3.5864754823862635e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9431, 'grad_norm': 0.31665757298469543, 'learning_rate': 3.58293503274916e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4734, 'grad_norm': 0.19582606852054596, 'learning_rate': 3.579394583112056e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6475, 'grad_norm': 0.18845656514167786, 'learning_rate': 3.575854133474951e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7027, 'grad_norm': 0.22226640582084656, 'learning_rate': 3.5723136838378475e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9418, 'grad_norm': 0.32289397716522217, 'learning_rate': 3.5687732342007436e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9702, 'grad_norm': 0.30913352966308594, 'learning_rate': 3.56523278456364e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8628, 'grad_norm': 0.2739059329032898, 'learning_rate': 3.561692334926536e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6348, 'grad_norm': 0.23310396075248718, 'learning_rate': 3.558151885289432e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7782, 'grad_norm': 0.22397838532924652, 'learning_rate': 3.554611435652328e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8311, 'grad_norm': 0.2456921637058258, 'learning_rate': 3.5510709860152244e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6684, 'grad_norm': 0.21887150406837463, 'learning_rate': 3.5475305363781206e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4097, 'grad_norm': 0.22246886789798737, 'learning_rate': 3.543990086741017e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6648, 'grad_norm': 0.2337958812713623, 'learning_rate': 3.540449637103912e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8492, 'grad_norm': 0.3026132583618164, 'learning_rate': 3.5369091874668084e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7339, 'grad_norm': 0.22984829545021057, 'learning_rate': 3.5333687378297045e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6465, 'grad_norm': 0.2525665760040283, 'learning_rate': 3.529828288192601e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6343, 'grad_norm': 0.256010502576828, 'learning_rate': 3.526287838555497e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4422, 'grad_norm': 0.20425106585025787, 'learning_rate': 3.522747388918392e-05, 'epoch': 0.82}\n",
      "{'loss': 0.759, 'grad_norm': 0.2542295455932617, 'learning_rate': 3.5192069392812884e-05, 'epoch': 0.82}\n",
      "{'loss': 0.961, 'grad_norm': 0.31719452142715454, 'learning_rate': 3.5156664896441846e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4743, 'grad_norm': 0.1791120320558548, 'learning_rate': 3.512126040007081e-05, 'epoch': 0.82}\n",
      "{'loss': 0.8856, 'grad_norm': 0.2378941774368286, 'learning_rate': 3.508585590369977e-05, 'epoch': 0.82}\n",
      "{'loss': 0.7279, 'grad_norm': 0.20538827776908875, 'learning_rate': 3.505045140732873e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8613, 'grad_norm': 0.32720068097114563, 'learning_rate': 3.501504691095769e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0563, 'grad_norm': 0.29104843735694885, 'learning_rate': 3.4979642414586654e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7249, 'grad_norm': 0.27506908774375916, 'learning_rate': 3.4944237918215615e-05, 'epoch': 0.83}\n",
      "{'loss': 0.535, 'grad_norm': 0.2646445631980896, 'learning_rate': 3.490883342184458e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8212, 'grad_norm': 0.30523401498794556, 'learning_rate': 3.487342892547354e-05, 'epoch': 0.83}\n",
      "{'loss': 0.771, 'grad_norm': 0.22439415752887726, 'learning_rate': 3.48380244291025e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7015, 'grad_norm': 0.2742137312889099, 'learning_rate': 3.4802619932731455e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0707, 'grad_norm': 0.29694125056266785, 'learning_rate': 3.4767215436360416e-05, 'epoch': 0.83}\n",
      "{'loss': 0.75, 'grad_norm': 0.28075823187828064, 'learning_rate': 3.473181093998938e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9229, 'grad_norm': 0.31079140305519104, 'learning_rate': 3.469640644361834e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5586, 'grad_norm': 0.2389904409646988, 'learning_rate': 3.46610019472473e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8214, 'grad_norm': 0.2502938508987427, 'learning_rate': 3.462559745087626e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6503, 'grad_norm': 0.2576409876346588, 'learning_rate': 3.4590192954505224e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7944, 'grad_norm': 0.28605470061302185, 'learning_rate': 3.4554788458134185e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8887, 'grad_norm': 0.21773661673069, 'learning_rate': 3.451938396176315e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9516, 'grad_norm': 0.29402396082878113, 'learning_rate': 3.448397946539211e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8008, 'grad_norm': 0.24061277508735657, 'learning_rate': 3.444857496902107e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6493, 'grad_norm': 0.22946351766586304, 'learning_rate': 3.441317047265003e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7368, 'grad_norm': 0.29373276233673096, 'learning_rate': 3.437776597627899e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7061, 'grad_norm': 0.26135095953941345, 'learning_rate': 3.434236147990795e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9523, 'grad_norm': 0.30670562386512756, 'learning_rate': 3.430695698353691e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7692, 'grad_norm': 0.2754665017127991, 'learning_rate': 3.427155248716587e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6495, 'grad_norm': 0.20501387119293213, 'learning_rate': 3.423614799079483e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6294, 'grad_norm': 0.25384992361068726, 'learning_rate': 3.4200743494423794e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8108, 'grad_norm': 0.20160017907619476, 'learning_rate': 3.4165338998052756e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8002, 'grad_norm': 0.2472088485956192, 'learning_rate': 3.412993450168172e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6378, 'grad_norm': 0.2080705612897873, 'learning_rate': 3.409453000531068e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7868, 'grad_norm': 0.2524486780166626, 'learning_rate': 3.405912550893964e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6677, 'grad_norm': 0.23293037712574005, 'learning_rate': 3.40237210125686e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6394, 'grad_norm': 0.21582521498203278, 'learning_rate': 3.398831651619756e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6527, 'grad_norm': 0.1921648532152176, 'learning_rate': 3.3952912019826525e-05, 'epoch': 0.83}\n",
      "{'loss': 0.727, 'grad_norm': 0.2037535011768341, 'learning_rate': 3.391750752345548e-05, 'epoch': 0.83}\n",
      "{'loss': 1.0009, 'grad_norm': 0.2910856008529663, 'learning_rate': 3.388210302708444e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5458, 'grad_norm': 0.16516675055027008, 'learning_rate': 3.38466985307134e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6663, 'grad_norm': 0.25958162546157837, 'learning_rate': 3.3811294034342364e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8799, 'grad_norm': 0.2240632027387619, 'learning_rate': 3.3775889537971326e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6044, 'grad_norm': 0.20054613053798676, 'learning_rate': 3.374048504160028e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8502, 'grad_norm': 0.25251254439353943, 'learning_rate': 3.370508054522924e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8519, 'grad_norm': 0.21892552077770233, 'learning_rate': 3.3669676048858204e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7647, 'grad_norm': 0.2720358073711395, 'learning_rate': 3.3634271552487165e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8247, 'grad_norm': 0.22906242311000824, 'learning_rate': 3.359886705611613e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7504, 'grad_norm': 0.31499889492988586, 'learning_rate': 3.356346255974509e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7922, 'grad_norm': 0.27027320861816406, 'learning_rate': 3.352805806337405e-05, 'epoch': 0.83}\n",
      "{'loss': 0.636, 'grad_norm': 0.2259334772825241, 'learning_rate': 3.349265356700301e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8456, 'grad_norm': 0.1934468299150467, 'learning_rate': 3.345724907063197e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6208, 'grad_norm': 0.2322131097316742, 'learning_rate': 3.3421844574260934e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4312, 'grad_norm': 0.24087201058864594, 'learning_rate': 3.338644007788989e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8626, 'grad_norm': 0.30388373136520386, 'learning_rate': 3.335103558151885e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7692, 'grad_norm': 0.1938004195690155, 'learning_rate': 3.331563108514781e-05, 'epoch': 0.83}\n",
      "{'loss': 0.9205, 'grad_norm': 0.31918442249298096, 'learning_rate': 3.3280226588776774e-05, 'epoch': 0.83}\n",
      "{'loss': 0.592, 'grad_norm': 0.259463906288147, 'learning_rate': 3.3244822092405735e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8531, 'grad_norm': 0.2591591477394104, 'learning_rate': 3.32094175960347e-05, 'epoch': 0.83}\n",
      "{'loss': 0.5846, 'grad_norm': 0.22250007092952728, 'learning_rate': 3.317401309966366e-05, 'epoch': 0.83}\n",
      "{'loss': 0.907, 'grad_norm': 0.21768420934677124, 'learning_rate': 3.313860860329262e-05, 'epoch': 0.83}\n",
      "{'loss': 0.7525, 'grad_norm': 0.21967090666294098, 'learning_rate': 3.310320410692158e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8026, 'grad_norm': 0.25182199478149414, 'learning_rate': 3.306779961055054e-05, 'epoch': 0.83}\n",
      "{'loss': 0.8032, 'grad_norm': 0.25012460350990295, 'learning_rate': 3.3032395114179505e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6673, 'grad_norm': 0.21728578209877014, 'learning_rate': 3.2996990617808466e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6523, 'grad_norm': 0.18520794808864594, 'learning_rate': 3.296158612143743e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5827, 'grad_norm': 0.2689748704433441, 'learning_rate': 3.292618162506638e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8282, 'grad_norm': 0.23261705040931702, 'learning_rate': 3.2890777128695344e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9269, 'grad_norm': 0.32875457406044006, 'learning_rate': 3.2855372632324306e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5158, 'grad_norm': 0.19309605658054352, 'learning_rate': 3.281996813595327e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7249, 'grad_norm': 0.20784203708171844, 'learning_rate': 3.278456363958223e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6198, 'grad_norm': 0.17644377052783966, 'learning_rate': 3.274915914321119e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7431, 'grad_norm': 0.27943500876426697, 'learning_rate': 3.271375464684015e-05, 'epoch': 0.84}\n",
      "{'loss': 0.675, 'grad_norm': 0.2864689826965332, 'learning_rate': 3.267835015046911e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0815, 'grad_norm': 0.2809075117111206, 'learning_rate': 3.2642945654098075e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6679, 'grad_norm': 0.22046469151973724, 'learning_rate': 3.2607541157727036e-05, 'epoch': 0.84}\n",
      "{'loss': 0.658, 'grad_norm': 0.247152179479599, 'learning_rate': 3.2572136661356e-05, 'epoch': 0.84}\n",
      "{'loss': 0.702, 'grad_norm': 0.18967515230178833, 'learning_rate': 3.253673216498496e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5572, 'grad_norm': 0.19570502638816833, 'learning_rate': 3.250132766861392e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7023, 'grad_norm': 0.200077623128891, 'learning_rate': 3.2465923172242876e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8985, 'grad_norm': 0.18384690582752228, 'learning_rate': 3.243051867587184e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5266, 'grad_norm': 0.18398956954479218, 'learning_rate': 3.23951141795008e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6377, 'grad_norm': 0.1967465877532959, 'learning_rate': 3.235970968312976e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5002, 'grad_norm': 0.17044435441493988, 'learning_rate': 3.232430518675872e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5407, 'grad_norm': 0.2469593584537506, 'learning_rate': 3.228890069038768e-05, 'epoch': 0.84}\n",
      "{'loss': 0.4958, 'grad_norm': 0.2187177985906601, 'learning_rate': 3.225349619401664e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7576, 'grad_norm': 0.23749566078186035, 'learning_rate': 3.22180916976456e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6809, 'grad_norm': 0.2609337866306305, 'learning_rate': 3.218268720127456e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7659, 'grad_norm': 0.250314325094223, 'learning_rate': 3.214728270490352e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9274, 'grad_norm': 0.2877906858921051, 'learning_rate': 3.2111878208532484e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6022, 'grad_norm': 0.2296210676431656, 'learning_rate': 3.2076473712161446e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7209, 'grad_norm': 0.24525390565395355, 'learning_rate': 3.204106921579041e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8489, 'grad_norm': 0.2513670027256012, 'learning_rate': 3.200566471941937e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9183, 'grad_norm': 0.2932175397872925, 'learning_rate': 3.1970260223048324e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7708, 'grad_norm': 0.269178181886673, 'learning_rate': 3.1934855726677285e-05, 'epoch': 0.84}\n",
      "{'loss': 0.4806, 'grad_norm': 0.23902766406536102, 'learning_rate': 3.189945123030625e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5498, 'grad_norm': 0.19313374161720276, 'learning_rate': 3.186404673393521e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9043, 'grad_norm': 0.2826688885688782, 'learning_rate': 3.182864223756417e-05, 'epoch': 0.84}\n",
      "{'loss': 1.1801, 'grad_norm': 0.37527400255203247, 'learning_rate': 3.179323774119313e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8348, 'grad_norm': 0.23485830426216125, 'learning_rate': 3.175783324482209e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5737, 'grad_norm': 0.2243962287902832, 'learning_rate': 3.1722428748451055e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7867, 'grad_norm': 0.20904403924942017, 'learning_rate': 3.1687024252080016e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7833, 'grad_norm': 0.21334776282310486, 'learning_rate': 3.165161975570898e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6469, 'grad_norm': 0.2441108077764511, 'learning_rate': 3.161621525933794e-05, 'epoch': 0.84}\n",
      "{'loss': 0.858, 'grad_norm': 0.37247687578201294, 'learning_rate': 3.15808107629669e-05, 'epoch': 0.84}\n",
      "{'loss': 0.9913, 'grad_norm': 0.2816307544708252, 'learning_rate': 3.154540626659586e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8813, 'grad_norm': 0.27511006593704224, 'learning_rate': 3.151000177022482e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8141, 'grad_norm': 0.20585589110851288, 'learning_rate': 3.147459727385378e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6266, 'grad_norm': 0.17719115316867828, 'learning_rate': 3.143919277748274e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7366, 'grad_norm': 0.25568392872810364, 'learning_rate': 3.14037882811117e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8242, 'grad_norm': 0.23240572214126587, 'learning_rate': 3.136838378474066e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7831, 'grad_norm': 0.24937307834625244, 'learning_rate': 3.1332979288369625e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7564, 'grad_norm': 0.25107449293136597, 'learning_rate': 3.1297574791998586e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0204, 'grad_norm': 0.2777014374732971, 'learning_rate': 3.126217029562755e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7789, 'grad_norm': 0.2792326807975769, 'learning_rate': 3.122676579925651e-05, 'epoch': 0.84}\n",
      "{'loss': 0.8881, 'grad_norm': 0.2682085931301117, 'learning_rate': 3.119136130288547e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6163, 'grad_norm': 0.23707616329193115, 'learning_rate': 3.115595680651443e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5638, 'grad_norm': 0.20265968143939972, 'learning_rate': 3.1120552310143394e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6729, 'grad_norm': 0.22462572157382965, 'learning_rate': 3.1085147813772356e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7734, 'grad_norm': 0.2403474897146225, 'learning_rate': 3.104974331740132e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5957, 'grad_norm': 0.185308039188385, 'learning_rate': 3.101433882103027e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5258, 'grad_norm': 0.1848648190498352, 'learning_rate': 3.0978934324659233e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7376, 'grad_norm': 0.25429555773735046, 'learning_rate': 3.0943529828288195e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7727, 'grad_norm': 0.24373149871826172, 'learning_rate': 3.0908125331917157e-05, 'epoch': 0.85}\n",
      "{'loss': 0.768, 'grad_norm': 0.20140640437602997, 'learning_rate': 3.087272083554612e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9312, 'grad_norm': 0.21558338403701782, 'learning_rate': 3.083731633917508e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7782, 'grad_norm': 0.18600994348526, 'learning_rate': 3.0801911842804034e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8061, 'grad_norm': 0.25023120641708374, 'learning_rate': 3.0766507346432996e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0138, 'grad_norm': 0.28932997584342957, 'learning_rate': 3.073110285006196e-05, 'epoch': 0.85}\n",
      "{'loss': 0.772, 'grad_norm': 0.2462993562221527, 'learning_rate': 3.069569835369092e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8263, 'grad_norm': 0.26276740431785583, 'learning_rate': 3.066029385731988e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5995, 'grad_norm': 0.2354067862033844, 'learning_rate': 3.062488936094884e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5399, 'grad_norm': 0.22303159534931183, 'learning_rate': 3.0589484864577804e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6513, 'grad_norm': 0.2948940098285675, 'learning_rate': 3.055408036820676e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5508, 'grad_norm': 0.2769415080547333, 'learning_rate': 3.051867587183572e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8232, 'grad_norm': 0.2764686048030853, 'learning_rate': 3.0483271375464685e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0729, 'grad_norm': 0.3237074017524719, 'learning_rate': 3.0447866879093646e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6923, 'grad_norm': 0.2135007083415985, 'learning_rate': 3.0412462382722608e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8356, 'grad_norm': 0.29800939559936523, 'learning_rate': 3.0377057886351566e-05, 'epoch': 0.85}\n",
      "{'loss': 0.4811, 'grad_norm': 0.19982671737670898, 'learning_rate': 3.0341653389980528e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7524, 'grad_norm': 0.2987000048160553, 'learning_rate': 3.030624889360949e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8175, 'grad_norm': 0.2215956300497055, 'learning_rate': 3.027084439723845e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9857, 'grad_norm': 0.26496338844299316, 'learning_rate': 3.0235439900867412e-05, 'epoch': 0.85}\n",
      "{'loss': 0.842, 'grad_norm': 0.2455112487077713, 'learning_rate': 3.0200035404496374e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7457, 'grad_norm': 0.22710983455181122, 'learning_rate': 3.0164630908125335e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6144, 'grad_norm': 0.24916142225265503, 'learning_rate': 3.0129226411754297e-05, 'epoch': 0.85}\n",
      "{'loss': 0.633, 'grad_norm': 0.17914016544818878, 'learning_rate': 3.009382191538325e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9116, 'grad_norm': 0.2530660033226013, 'learning_rate': 3.0058417419012213e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5993, 'grad_norm': 0.23236501216888428, 'learning_rate': 3.0023012922641175e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7562, 'grad_norm': 0.28828758001327515, 'learning_rate': 2.9987608426270136e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0126, 'grad_norm': 0.3636576235294342, 'learning_rate': 2.9952203929899098e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6448, 'grad_norm': 0.2634378969669342, 'learning_rate': 2.991679943352806e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6464, 'grad_norm': 0.28608062863349915, 'learning_rate': 2.988139493715702e-05, 'epoch': 0.85}\n",
      "{'loss': 1.0486, 'grad_norm': 0.2262841910123825, 'learning_rate': 2.9845990440785982e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5354, 'grad_norm': 0.1757383793592453, 'learning_rate': 2.9810585944414944e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7689, 'grad_norm': 0.20831990242004395, 'learning_rate': 2.9775181448043906e-05, 'epoch': 0.85}\n",
      "{'loss': 1.2828, 'grad_norm': 0.25910189747810364, 'learning_rate': 2.9739776951672864e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8079, 'grad_norm': 0.18142567574977875, 'learning_rate': 2.9704372455301825e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6303, 'grad_norm': 0.1759311407804489, 'learning_rate': 2.9668967958930787e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6903, 'grad_norm': 0.19936338067054749, 'learning_rate': 2.963356346255975e-05, 'epoch': 0.85}\n",
      "{'loss': 1.051, 'grad_norm': 0.3329654633998871, 'learning_rate': 2.9598158966188706e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8665, 'grad_norm': 0.21944673359394073, 'learning_rate': 2.9562754469817665e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6446, 'grad_norm': 0.23704485595226288, 'learning_rate': 2.9527349973446626e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6352, 'grad_norm': 0.22005900740623474, 'learning_rate': 2.9491945477075588e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7166, 'grad_norm': 0.23767127096652985, 'learning_rate': 2.945654098070455e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8357, 'grad_norm': 0.25109797716140747, 'learning_rate': 2.942113648433351e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7696, 'grad_norm': 0.20046524703502655, 'learning_rate': 2.9385731987962472e-05, 'epoch': 0.85}\n",
      "{'loss': 0.8877, 'grad_norm': 0.34305834770202637, 'learning_rate': 2.9350327491591434e-05, 'epoch': 0.85}\n",
      "{'loss': 0.9485, 'grad_norm': 0.2964133620262146, 'learning_rate': 2.9314922995220395e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7206, 'grad_norm': 0.21838970482349396, 'learning_rate': 2.9279518498849357e-05, 'epoch': 0.85}\n",
      "{'loss': 1.1456, 'grad_norm': 0.344691663980484, 'learning_rate': 2.924411400247832e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7945, 'grad_norm': 0.3701314926147461, 'learning_rate': 2.920870950610728e-05, 'epoch': 0.85}\n",
      "{'loss': 0.6915, 'grad_norm': 0.17201922833919525, 'learning_rate': 2.917330500973624e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7685, 'grad_norm': 0.21251732110977173, 'learning_rate': 2.9137900513365196e-05, 'epoch': 0.85}\n",
      "{'loss': 0.7795, 'grad_norm': 0.2174680233001709, 'learning_rate': 2.9102496016994158e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5974, 'grad_norm': 0.1902773082256317, 'learning_rate': 2.906709152062312e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5376, 'grad_norm': 0.2251373827457428, 'learning_rate': 2.903168702425208e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5916, 'grad_norm': 0.2162473350763321, 'learning_rate': 2.8996282527881043e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5334, 'grad_norm': 0.22676871716976166, 'learning_rate': 2.8960878031510004e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7351, 'grad_norm': 0.2658787667751312, 'learning_rate': 2.8925473535138962e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6167, 'grad_norm': 0.22879397869110107, 'learning_rate': 2.8890069038767924e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8932, 'grad_norm': 0.238648921251297, 'learning_rate': 2.8854664542396885e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6586, 'grad_norm': 0.24249504506587982, 'learning_rate': 2.8819260046025847e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7422, 'grad_norm': 0.21334582567214966, 'learning_rate': 2.878385554965481e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7418, 'grad_norm': 0.3163120150566101, 'learning_rate': 2.874845105328377e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6058, 'grad_norm': 0.22654663026332855, 'learning_rate': 2.871304655691273e-05, 'epoch': 0.86}\n",
      "{'loss': 0.74, 'grad_norm': 0.22922824323177338, 'learning_rate': 2.8677642060541693e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7183, 'grad_norm': 0.24165694415569305, 'learning_rate': 2.8642237564170648e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8553, 'grad_norm': 0.2743297219276428, 'learning_rate': 2.860683306779961e-05, 'epoch': 0.86}\n",
      "{'loss': 0.9127, 'grad_norm': 0.2549601197242737, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7655, 'grad_norm': 0.23767146468162537, 'learning_rate': 2.8536024075057532e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6702, 'grad_norm': 0.22008608281612396, 'learning_rate': 2.8500619578686494e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5949, 'grad_norm': 0.29256805777549744, 'learning_rate': 2.8465215082315455e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8832, 'grad_norm': 0.2660021483898163, 'learning_rate': 2.8429810585944417e-05, 'epoch': 0.86}\n",
      "{'loss': 0.759, 'grad_norm': 0.23988355696201324, 'learning_rate': 2.839440608957338e-05, 'epoch': 0.86}\n",
      "{'loss': 1.0592, 'grad_norm': 0.3201653063297272, 'learning_rate': 2.835900159320234e-05, 'epoch': 0.86}\n",
      "{'loss': 0.721, 'grad_norm': 0.21280808746814728, 'learning_rate': 2.83235970968313e-05, 'epoch': 0.86}\n",
      "{'loss': 0.9454, 'grad_norm': 0.23261626064777374, 'learning_rate': 2.8288192600460263e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6301, 'grad_norm': 0.31122922897338867, 'learning_rate': 2.825278810408922e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6507, 'grad_norm': 0.24564994871616364, 'learning_rate': 2.8217383607718183e-05, 'epoch': 0.86}\n",
      "{'loss': 0.4289, 'grad_norm': 0.17374619841575623, 'learning_rate': 2.818197911134714e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6628, 'grad_norm': 0.25945213437080383, 'learning_rate': 2.8146574614976103e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5395, 'grad_norm': 0.2178637534379959, 'learning_rate': 2.8111170118605064e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6343, 'grad_norm': 0.1853133887052536, 'learning_rate': 2.8075765622234022e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7445, 'grad_norm': 0.25042855739593506, 'learning_rate': 2.8040361125862984e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5678, 'grad_norm': 0.25953149795532227, 'learning_rate': 2.8004956629491945e-05, 'epoch': 0.86}\n",
      "{'loss': 0.562, 'grad_norm': 0.22760695219039917, 'learning_rate': 2.7969552133120907e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5505, 'grad_norm': 0.20445537567138672, 'learning_rate': 2.793414763674987e-05, 'epoch': 0.86}\n",
      "{'loss': 1.1969, 'grad_norm': 0.2769680321216583, 'learning_rate': 2.789874314037883e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8299, 'grad_norm': 0.2117132544517517, 'learning_rate': 2.786333864400779e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7009, 'grad_norm': 0.3054102659225464, 'learning_rate': 2.7827934147636753e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6311, 'grad_norm': 0.31882333755493164, 'learning_rate': 2.7792529651265715e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7422, 'grad_norm': 0.20196208357810974, 'learning_rate': 2.7757125154894676e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6751, 'grad_norm': 0.24581776559352875, 'learning_rate': 2.772172065852363e-05, 'epoch': 0.86}\n",
      "{'loss': 0.9145, 'grad_norm': 0.2443878948688507, 'learning_rate': 2.7686316162152592e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8087, 'grad_norm': 0.20929083228111267, 'learning_rate': 2.7650911665781554e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6917, 'grad_norm': 0.18385636806488037, 'learning_rate': 2.7615507169410516e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5001, 'grad_norm': 0.1935553401708603, 'learning_rate': 2.7580102673039477e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8347, 'grad_norm': 0.20156444609165192, 'learning_rate': 2.754469817666844e-05, 'epoch': 0.86}\n",
      "{'loss': 0.9073, 'grad_norm': 0.23250308632850647, 'learning_rate': 2.75092936802974e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7138, 'grad_norm': 0.228214830160141, 'learning_rate': 2.7473889183926362e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6449, 'grad_norm': 0.2247001677751541, 'learning_rate': 2.743848468755532e-05, 'epoch': 0.86}\n",
      "{'loss': 0.919, 'grad_norm': 0.2963235378265381, 'learning_rate': 2.740308019118428e-05, 'epoch': 0.86}\n",
      "{'loss': 0.9567, 'grad_norm': 0.2947613298892975, 'learning_rate': 2.7367675694813243e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7804, 'grad_norm': 0.2250918745994568, 'learning_rate': 2.7332271198442205e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5811, 'grad_norm': 0.19243775308132172, 'learning_rate': 2.7296866702071166e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5625, 'grad_norm': 0.19613075256347656, 'learning_rate': 2.7261462205700128e-05, 'epoch': 0.86}\n",
      "{'loss': 0.6443, 'grad_norm': 0.18706360459327698, 'learning_rate': 2.7226057709329082e-05, 'epoch': 0.86}\n",
      "{'loss': 0.679, 'grad_norm': 0.21044155955314636, 'learning_rate': 2.7190653212958044e-05, 'epoch': 0.86}\n",
      "{'loss': 0.5646, 'grad_norm': 0.19265729188919067, 'learning_rate': 2.7155248716587005e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7504, 'grad_norm': 0.2550821900367737, 'learning_rate': 2.7119844220215967e-05, 'epoch': 0.86}\n",
      "{'loss': 0.8709, 'grad_norm': 0.22442451119422913, 'learning_rate': 2.708443972384493e-05, 'epoch': 0.86}\n",
      "{'loss': 0.7373, 'grad_norm': 0.24889357388019562, 'learning_rate': 2.704903522747389e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5487, 'grad_norm': 0.19319172203540802, 'learning_rate': 2.701363073110285e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7217, 'grad_norm': 0.23023799061775208, 'learning_rate': 2.6978226234731813e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7353, 'grad_norm': 0.2128540277481079, 'learning_rate': 2.6942821738360775e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5481, 'grad_norm': 0.21344897150993347, 'learning_rate': 2.6907417241989736e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8615, 'grad_norm': 0.28822243213653564, 'learning_rate': 2.6872012745618698e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4866, 'grad_norm': 0.2033819556236267, 'learning_rate': 2.683660824924766e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6414, 'grad_norm': 0.25251007080078125, 'learning_rate': 2.6801203752876617e-05, 'epoch': 0.87}\n",
      "{'loss': 1.0739, 'grad_norm': 0.23561234772205353, 'learning_rate': 2.6765799256505576e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7641, 'grad_norm': 0.20116844773292542, 'learning_rate': 2.6730394760134537e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7145, 'grad_norm': 0.22033055126667023, 'learning_rate': 2.66949902637635e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8053, 'grad_norm': 0.27893391251564026, 'learning_rate': 2.665958576739246e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8541, 'grad_norm': 0.29874131083488464, 'learning_rate': 2.662418127102142e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6979, 'grad_norm': 0.28539595007896423, 'learning_rate': 2.658877677465038e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5796, 'grad_norm': 0.2559426724910736, 'learning_rate': 2.655337227827934e-05, 'epoch': 0.87}\n",
      "{'loss': 1.0896, 'grad_norm': 0.2956000566482544, 'learning_rate': 2.6517967781908303e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5823, 'grad_norm': 0.20589183270931244, 'learning_rate': 2.6482563285537265e-05, 'epoch': 0.87}\n",
      "{'loss': 0.893, 'grad_norm': 0.23148159682750702, 'learning_rate': 2.6447158789166226e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7935, 'grad_norm': 0.38424885272979736, 'learning_rate': 2.6411754292795188e-05, 'epoch': 0.87}\n",
      "{'loss': 0.992, 'grad_norm': 0.19520872831344604, 'learning_rate': 2.637634979642415e-05, 'epoch': 0.87}\n",
      "{'loss': 0.9349, 'grad_norm': 0.22193050384521484, 'learning_rate': 2.634094530005311e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7852, 'grad_norm': 0.25931316614151, 'learning_rate': 2.6305540803682072e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5933, 'grad_norm': 0.24393755197525024, 'learning_rate': 2.6270136307311027e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8538, 'grad_norm': 0.24012590944766998, 'learning_rate': 2.623473181093999e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7708, 'grad_norm': 0.22902874648571014, 'learning_rate': 2.619932731456895e-05, 'epoch': 0.87}\n",
      "{'loss': 1.2246, 'grad_norm': 0.2862570285797119, 'learning_rate': 2.616392281819791e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5249, 'grad_norm': 0.20125222206115723, 'learning_rate': 2.6128518321826873e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6274, 'grad_norm': 0.24834854900836945, 'learning_rate': 2.6093113825455835e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4924, 'grad_norm': 0.20779189467430115, 'learning_rate': 2.6057709329084796e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5239, 'grad_norm': 0.21358151733875275, 'learning_rate': 2.6022304832713758e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6314, 'grad_norm': 0.18795889616012573, 'learning_rate': 2.598690033634272e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6444, 'grad_norm': 0.24304330348968506, 'learning_rate': 2.5951495839971678e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4878, 'grad_norm': 0.1954452097415924, 'learning_rate': 2.591609134360064e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6928, 'grad_norm': 0.2389272153377533, 'learning_rate': 2.58806868472296e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6774, 'grad_norm': 0.25357380509376526, 'learning_rate': 2.5845282350858562e-05, 'epoch': 0.87}\n",
      "{'loss': 0.496, 'grad_norm': 0.19195044040679932, 'learning_rate': 2.5809877854487517e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8434, 'grad_norm': 0.2945731282234192, 'learning_rate': 2.577447335811648e-05, 'epoch': 0.87}\n",
      "{'loss': 0.9002, 'grad_norm': 0.2524096369743347, 'learning_rate': 2.573906886174544e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6877, 'grad_norm': 0.20698855817317963, 'learning_rate': 2.57036643653744e-05, 'epoch': 0.87}\n",
      "{'loss': 0.579, 'grad_norm': 0.2183811515569687, 'learning_rate': 2.5668259869003363e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7131, 'grad_norm': 0.2129034847021103, 'learning_rate': 2.5632855372632325e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7117, 'grad_norm': 0.2269516885280609, 'learning_rate': 2.5597450876261286e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6301, 'grad_norm': 0.22048711776733398, 'learning_rate': 2.5562046379890248e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6507, 'grad_norm': 0.22566621005535126, 'learning_rate': 2.552664188351921e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5282, 'grad_norm': 0.2032332569360733, 'learning_rate': 2.549123738714817e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7229, 'grad_norm': 0.2406223863363266, 'learning_rate': 2.5455832890777132e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6966, 'grad_norm': 0.2595142722129822, 'learning_rate': 2.5420428394406094e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6459, 'grad_norm': 0.20350489020347595, 'learning_rate': 2.5385023898035055e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7597, 'grad_norm': 0.30649229884147644, 'learning_rate': 2.534961940166401e-05, 'epoch': 0.87}\n",
      "{'loss': 0.868, 'grad_norm': 0.24904479086399078, 'learning_rate': 2.5314214905292972e-05, 'epoch': 0.87}\n",
      "{'loss': 0.8991, 'grad_norm': 0.2442973405122757, 'learning_rate': 2.5278810408921933e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7987, 'grad_norm': 0.2695614695549011, 'learning_rate': 2.5243405912550895e-05, 'epoch': 0.87}\n",
      "{'loss': 1.3429, 'grad_norm': 0.28316280245780945, 'learning_rate': 2.5208001416179856e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7398, 'grad_norm': 0.20111200213432312, 'learning_rate': 2.5172596919808818e-05, 'epoch': 0.87}\n",
      "{'loss': 0.7561, 'grad_norm': 0.22015057504177094, 'learning_rate': 2.5137192423437776e-05, 'epoch': 0.87}\n",
      "{'loss': 0.6039, 'grad_norm': 0.20640622079372406, 'learning_rate': 2.5101787927066738e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5232, 'grad_norm': 0.20777401328086853, 'learning_rate': 2.50663834306957e-05, 'epoch': 0.87}\n",
      "{'loss': 0.5843, 'grad_norm': 0.21532393991947174, 'learning_rate': 2.503097893432466e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4711, 'grad_norm': 0.18197201192378998, 'learning_rate': 2.499557443795362e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7624, 'grad_norm': 0.23444969952106476, 'learning_rate': 2.496016994158258e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5452, 'grad_norm': 0.20173941552639008, 'learning_rate': 2.4924765445211542e-05, 'epoch': 0.88}\n",
      "{'loss': 0.864, 'grad_norm': 0.2668364942073822, 'learning_rate': 2.4889360948840503e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7779, 'grad_norm': 0.21051308512687683, 'learning_rate': 2.4853956452469465e-05, 'epoch': 0.88}\n",
      "{'loss': 0.526, 'grad_norm': 0.2170899361371994, 'learning_rate': 2.4818551956098427e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5382, 'grad_norm': 0.1815849393606186, 'learning_rate': 2.4783147459727388e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8999, 'grad_norm': 0.26017820835113525, 'learning_rate': 2.4747742963356346e-05, 'epoch': 0.88}\n",
      "{'loss': 0.639, 'grad_norm': 0.21961501240730286, 'learning_rate': 2.4712338466985308e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7756, 'grad_norm': 0.28044670820236206, 'learning_rate': 2.467693397061427e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9637, 'grad_norm': 0.24395537376403809, 'learning_rate': 2.464152947424323e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6346, 'grad_norm': 0.29417040944099426, 'learning_rate': 2.4606124977872192e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8619, 'grad_norm': 0.2624300420284271, 'learning_rate': 2.4570720481501154e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8926, 'grad_norm': 0.2701343894004822, 'learning_rate': 2.4535315985130116e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6774, 'grad_norm': 0.18576832115650177, 'learning_rate': 2.4499911488759074e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6836, 'grad_norm': 0.2896147668361664, 'learning_rate': 2.4464506992388035e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5504, 'grad_norm': 0.19319215416908264, 'learning_rate': 2.4429102496016997e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8764, 'grad_norm': 0.24815712869167328, 'learning_rate': 2.4393697999645955e-05, 'epoch': 0.88}\n",
      "{'loss': 0.832, 'grad_norm': 0.23630918562412262, 'learning_rate': 2.4358293503274916e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6892, 'grad_norm': 0.20154835283756256, 'learning_rate': 2.4322889006903878e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5584, 'grad_norm': 0.21450680494308472, 'learning_rate': 2.4287484510532836e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6156, 'grad_norm': 0.2068040817975998, 'learning_rate': 2.4252080014161798e-05, 'epoch': 0.88}\n",
      "{'loss': 1.0297, 'grad_norm': 0.20415985584259033, 'learning_rate': 2.421667551779076e-05, 'epoch': 0.88}\n",
      "{'loss': 1.0236, 'grad_norm': 0.27884432673454285, 'learning_rate': 2.418127102141972e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9323, 'grad_norm': 0.27882108092308044, 'learning_rate': 2.4145866525048682e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5181, 'grad_norm': 0.2510177195072174, 'learning_rate': 2.4110462028677644e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7938, 'grad_norm': 0.24461787939071655, 'learning_rate': 2.4075057532306605e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6038, 'grad_norm': 0.208765909075737, 'learning_rate': 2.4039653035935564e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5555, 'grad_norm': 0.22785618901252747, 'learning_rate': 2.4004248539564525e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7222, 'grad_norm': 0.2354670614004135, 'learning_rate': 2.3968844043193487e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7859, 'grad_norm': 0.23945312201976776, 'learning_rate': 2.3933439546822448e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6676, 'grad_norm': 0.1835392266511917, 'learning_rate': 2.389803505045141e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7772, 'grad_norm': 0.23654061555862427, 'learning_rate': 2.386263055408037e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7473, 'grad_norm': 0.21038532257080078, 'learning_rate': 2.3827226057709333e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6061, 'grad_norm': 0.2583601474761963, 'learning_rate': 2.379182156133829e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9002, 'grad_norm': 0.25049901008605957, 'learning_rate': 2.3756417064967252e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5991, 'grad_norm': 0.2020367532968521, 'learning_rate': 2.3721012568596214e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5457, 'grad_norm': 0.2032245695590973, 'learning_rate': 2.3685608072225172e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8201, 'grad_norm': 0.2822118401527405, 'learning_rate': 2.3650203575854134e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6658, 'grad_norm': 0.23099863529205322, 'learning_rate': 2.3614799079483095e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8426, 'grad_norm': 0.2953827381134033, 'learning_rate': 2.3579394583112057e-05, 'epoch': 0.88}\n",
      "{'loss': 0.789, 'grad_norm': 0.2043224722146988, 'learning_rate': 2.3543990086741015e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5481, 'grad_norm': 0.2257676124572754, 'learning_rate': 2.3508585590369977e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8759, 'grad_norm': 0.29270392656326294, 'learning_rate': 2.3473181093998938e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8201, 'grad_norm': 0.34539055824279785, 'learning_rate': 2.34377765976279e-05, 'epoch': 0.88}\n",
      "{'loss': 0.581, 'grad_norm': 0.22611857950687408, 'learning_rate': 2.340237210125686e-05, 'epoch': 0.88}\n",
      "{'loss': 0.7078, 'grad_norm': 0.2602770924568176, 'learning_rate': 2.3366967604885823e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5643, 'grad_norm': 0.2709607481956482, 'learning_rate': 2.333156310851478e-05, 'epoch': 0.88}\n",
      "{'loss': 1.1147, 'grad_norm': 0.2676142454147339, 'learning_rate': 2.3296158612143742e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6858, 'grad_norm': 0.22577229142189026, 'learning_rate': 2.3260754115772704e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4308, 'grad_norm': 0.20863506197929382, 'learning_rate': 2.3225349619401665e-05, 'epoch': 0.88}\n",
      "{'loss': 1.1093, 'grad_norm': 0.3723347783088684, 'learning_rate': 2.3189945123030627e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee384f5300>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 7f6d498f-0068-4621-8bd9-d0d12db61dbf)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5359, 'grad_norm': 0.1733028143644333, 'learning_rate': 2.315454062665959e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8219, 'grad_norm': 0.23643222451210022, 'learning_rate': 2.311913613028855e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9079, 'grad_norm': 0.19090883433818817, 'learning_rate': 2.3083731633917508e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5926, 'grad_norm': 0.20389729738235474, 'learning_rate': 2.304832713754647e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6139, 'grad_norm': 0.22022655606269836, 'learning_rate': 2.301292264117543e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5927, 'grad_norm': 0.16876593232154846, 'learning_rate': 2.2977518144804393e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5636, 'grad_norm': 0.18037112057209015, 'learning_rate': 2.294211364843335e-05, 'epoch': 0.89}\n",
      "{'loss': 0.9375, 'grad_norm': 0.29497581720352173, 'learning_rate': 2.2906709152062313e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8498, 'grad_norm': 0.2735775411128998, 'learning_rate': 2.2871304655691274e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5981, 'grad_norm': 0.22439004480838776, 'learning_rate': 2.2835900159320232e-05, 'epoch': 0.89}\n",
      "{'loss': 0.707, 'grad_norm': 0.19434906542301178, 'learning_rate': 2.2800495662949194e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6828, 'grad_norm': 0.21804571151733398, 'learning_rate': 2.2765091166578155e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6212, 'grad_norm': 0.19596707820892334, 'learning_rate': 2.2729686670207117e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7029, 'grad_norm': 0.2364136129617691, 'learning_rate': 2.269428217383608e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6526, 'grad_norm': 0.22798337042331696, 'learning_rate': 2.265887767746504e-05, 'epoch': 0.89}\n",
      "{'loss': 0.713, 'grad_norm': 0.18738900125026703, 'learning_rate': 2.2623473181093998e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5777, 'grad_norm': 0.1942683607339859, 'learning_rate': 2.258806868472296e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6257, 'grad_norm': 0.20598869025707245, 'learning_rate': 2.255266418835192e-05, 'epoch': 0.89}\n",
      "{'loss': 0.689, 'grad_norm': 0.23553401231765747, 'learning_rate': 2.2517259691980883e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6558, 'grad_norm': 0.24022747576236725, 'learning_rate': 2.2481855195609844e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8568, 'grad_norm': 0.23055800795555115, 'learning_rate': 2.2446450699238806e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7693, 'grad_norm': 0.22362026572227478, 'learning_rate': 2.2411046202867767e-05, 'epoch': 0.89}\n",
      "{'loss': 0.633, 'grad_norm': 0.1900142878293991, 'learning_rate': 2.2375641706496726e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6319, 'grad_norm': 0.2050207108259201, 'learning_rate': 2.2340237210125687e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7637, 'grad_norm': 0.28973034024238586, 'learning_rate': 2.230483271375465e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4886, 'grad_norm': 0.18820618093013763, 'learning_rate': 2.226942821738361e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6469, 'grad_norm': 0.23905283212661743, 'learning_rate': 2.2234023721012572e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7091, 'grad_norm': 0.20353162288665771, 'learning_rate': 2.219861922464153e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6949, 'grad_norm': 0.2650611996650696, 'learning_rate': 2.216321472827049e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6344, 'grad_norm': 0.22237280011177063, 'learning_rate': 2.212781023189945e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6055, 'grad_norm': 0.23982858657836914, 'learning_rate': 2.209240573552841e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6322, 'grad_norm': 0.2351880818605423, 'learning_rate': 2.2057001239157373e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6295, 'grad_norm': 0.25376683473587036, 'learning_rate': 2.2021596742786334e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5234, 'grad_norm': 0.19900646805763245, 'learning_rate': 2.1986192246415296e-05, 'epoch': 0.89}\n",
      "{'loss': 0.819, 'grad_norm': 0.2859700322151184, 'learning_rate': 2.1950787750044257e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6617, 'grad_norm': 0.19523616135120392, 'learning_rate': 2.1915383253673215e-05, 'epoch': 0.89}\n",
      "{'loss': 0.9731, 'grad_norm': 0.3435693383216858, 'learning_rate': 2.1879978757302177e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6907, 'grad_norm': 0.21131767332553864, 'learning_rate': 2.184457426093114e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6266, 'grad_norm': 0.21188180148601532, 'learning_rate': 2.18091697645601e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6161, 'grad_norm': 0.23479828238487244, 'learning_rate': 2.177376526818906e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5841, 'grad_norm': 0.24935881793498993, 'learning_rate': 2.1738360771818023e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6126, 'grad_norm': 0.24202175438404083, 'learning_rate': 2.1702956275446985e-05, 'epoch': 0.89}\n",
      "{'loss': 0.9145, 'grad_norm': 0.25247061252593994, 'learning_rate': 2.1667551779075943e-05, 'epoch': 0.89}\n",
      "{'loss': 0.9978, 'grad_norm': 0.31456759572029114, 'learning_rate': 2.1632147282704904e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7293, 'grad_norm': 0.1899077147245407, 'learning_rate': 2.1596742786333866e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5291, 'grad_norm': 0.19761213660240173, 'learning_rate': 2.1561338289962827e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8273, 'grad_norm': 0.2391456663608551, 'learning_rate': 2.152593379359179e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7771, 'grad_norm': 0.2720174789428711, 'learning_rate': 2.149052929722075e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8085, 'grad_norm': 0.25594910979270935, 'learning_rate': 2.145512480084971e-05, 'epoch': 0.89}\n",
      "{'loss': 0.59, 'grad_norm': 0.2497674524784088, 'learning_rate': 2.141972030447867e-05, 'epoch': 0.89}\n",
      "{'loss': 0.543, 'grad_norm': 0.30227208137512207, 'learning_rate': 2.138431580810763e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6543, 'grad_norm': 0.19432373344898224, 'learning_rate': 2.134891131173659e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8028, 'grad_norm': 0.24685688316822052, 'learning_rate': 2.131350681536555e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5423, 'grad_norm': 0.22954364120960236, 'learning_rate': 2.1278102318994513e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7035, 'grad_norm': 0.2240051031112671, 'learning_rate': 2.1242697822623475e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5711, 'grad_norm': 0.2812923789024353, 'learning_rate': 2.1207293326252433e-05, 'epoch': 0.89}\n",
      "{'loss': 0.7759, 'grad_norm': 0.2316494882106781, 'learning_rate': 2.1171888829881394e-05, 'epoch': 0.89}\n",
      "{'loss': 0.632, 'grad_norm': 0.23531682789325714, 'learning_rate': 2.1136484333510356e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5313, 'grad_norm': 0.21822890639305115, 'learning_rate': 2.1101079837139317e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6704, 'grad_norm': 0.22390128672122955, 'learning_rate': 2.106567534076828e-05, 'epoch': 0.89}\n",
      "{'loss': 0.6542, 'grad_norm': 0.19428646564483643, 'learning_rate': 2.103027084439724e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7327, 'grad_norm': 0.2303016483783722, 'learning_rate': 2.0994866348026202e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8498, 'grad_norm': 0.21882948279380798, 'learning_rate': 2.095946185165516e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8804, 'grad_norm': 0.2541690468788147, 'learning_rate': 2.092405735528412e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5591, 'grad_norm': 0.21003450453281403, 'learning_rate': 2.0888652858913083e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8167, 'grad_norm': 0.2127465307712555, 'learning_rate': 2.0853248362542045e-05, 'epoch': 0.9}\n",
      "{'loss': 0.9713, 'grad_norm': 0.33584561944007874, 'learning_rate': 2.0817843866171006e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4086, 'grad_norm': 0.20604492723941803, 'learning_rate': 2.0782439369799968e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8495, 'grad_norm': 0.2323761284351349, 'learning_rate': 2.074703487342893e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8875, 'grad_norm': 0.274533748626709, 'learning_rate': 2.0711630377057888e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8527, 'grad_norm': 0.21707479655742645, 'learning_rate': 2.067622588068685e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6728, 'grad_norm': 0.20698188245296478, 'learning_rate': 2.0640821384315807e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8103, 'grad_norm': 0.22137831151485443, 'learning_rate': 2.060541688794477e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6741, 'grad_norm': 0.20980927348136902, 'learning_rate': 2.057001239157373e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5918, 'grad_norm': 0.21438242495059967, 'learning_rate': 2.0534607895202692e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6781, 'grad_norm': 0.2382151335477829, 'learning_rate': 2.0499203398831653e-05, 'epoch': 0.9}\n",
      "{'loss': 1.0263, 'grad_norm': 0.26254981756210327, 'learning_rate': 2.046379890246061e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6604, 'grad_norm': 0.19119854271411896, 'learning_rate': 2.0428394406089573e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5114, 'grad_norm': 0.20349454879760742, 'learning_rate': 2.0392989909718535e-05, 'epoch': 0.9}\n",
      "{'loss': 1.0114, 'grad_norm': 0.2446223944425583, 'learning_rate': 2.0357585413347496e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7875, 'grad_norm': 0.2885097563266754, 'learning_rate': 2.0322180916976458e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6346, 'grad_norm': 0.21037401258945465, 'learning_rate': 2.028677642060542e-05, 'epoch': 0.9}\n",
      "{'loss': 0.9059, 'grad_norm': 0.3330877721309662, 'learning_rate': 2.0251371924234377e-05, 'epoch': 0.9}\n",
      "{'loss': 1.0338, 'grad_norm': 0.30731001496315, 'learning_rate': 2.021596742786334e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6026, 'grad_norm': 0.20392684638500214, 'learning_rate': 2.01805629314923e-05, 'epoch': 0.9}\n",
      "{'loss': 0.638, 'grad_norm': 0.26140639185905457, 'learning_rate': 2.0145158435121262e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6111, 'grad_norm': 0.3004094660282135, 'learning_rate': 2.0109753938750224e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4914, 'grad_norm': 0.18497563898563385, 'learning_rate': 2.0074349442379185e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8008, 'grad_norm': 0.22740241885185242, 'learning_rate': 2.0038944946008147e-05, 'epoch': 0.9}\n",
      "{'loss': 1.5107, 'grad_norm': 0.23279479146003723, 'learning_rate': 2.0003540449637105e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7987, 'grad_norm': 0.23672127723693848, 'learning_rate': 1.9968135953266066e-05, 'epoch': 0.9}\n",
      "{'loss': 1.0486, 'grad_norm': 0.23802882432937622, 'learning_rate': 1.9932731456895028e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5101, 'grad_norm': 0.1815708428621292, 'learning_rate': 1.9897326960523986e-05, 'epoch': 0.9}\n",
      "{'loss': 1.003, 'grad_norm': 0.2974267601966858, 'learning_rate': 1.9861922464152948e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8809, 'grad_norm': 0.19620218873023987, 'learning_rate': 1.982651796778191e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8557, 'grad_norm': 0.29952916502952576, 'learning_rate': 1.979111347141087e-05, 'epoch': 0.9}\n",
      "{'loss': 0.627, 'grad_norm': 0.2270192801952362, 'learning_rate': 1.975570897503983e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7876, 'grad_norm': 0.2634686827659607, 'learning_rate': 1.972030447866879e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7301, 'grad_norm': 0.4341779947280884, 'learning_rate': 1.9684899982297752e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6775, 'grad_norm': 0.21604157984256744, 'learning_rate': 1.9649495485926713e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7355, 'grad_norm': 0.31325215101242065, 'learning_rate': 1.9614090989555675e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6137, 'grad_norm': 0.2018475979566574, 'learning_rate': 1.9578686493184637e-05, 'epoch': 0.9}\n",
      "{'loss': 0.9658, 'grad_norm': 0.2823428809642792, 'learning_rate': 1.9543281996813595e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8552, 'grad_norm': 0.24169041216373444, 'learning_rate': 1.9507877500442556e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6424, 'grad_norm': 0.23327623307704926, 'learning_rate': 1.9472473004071518e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4873, 'grad_norm': 0.20432044565677643, 'learning_rate': 1.943706850770048e-05, 'epoch': 0.9}\n",
      "{'loss': 0.9978, 'grad_norm': 0.2805512547492981, 'learning_rate': 1.940166401132944e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6784, 'grad_norm': 0.23991559445858002, 'learning_rate': 1.9366259514958402e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7991, 'grad_norm': 0.19122281670570374, 'learning_rate': 1.9330855018587364e-05, 'epoch': 0.9}\n",
      "{'loss': 0.8212, 'grad_norm': 0.2688392102718353, 'learning_rate': 1.9295450522216322e-05, 'epoch': 0.9}\n",
      "{'loss': 0.7426, 'grad_norm': 0.20758606493473053, 'learning_rate': 1.9260046025845284e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6214, 'grad_norm': 0.17675858736038208, 'learning_rate': 1.9224641529474245e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5652, 'grad_norm': 0.21301056444644928, 'learning_rate': 1.9189237033103207e-05, 'epoch': 0.9}\n",
      "{'loss': 0.4595, 'grad_norm': 0.20532990992069244, 'learning_rate': 1.9153832536732165e-05, 'epoch': 0.9}\n",
      "{'loss': 0.6044, 'grad_norm': 0.17124336957931519, 'learning_rate': 1.9118428040361126e-05, 'epoch': 0.9}\n",
      "{'loss': 0.613, 'grad_norm': 0.24558481574058533, 'learning_rate': 1.9083023543990088e-05, 'epoch': 0.9}\n",
      "{'loss': 0.5868, 'grad_norm': 0.27277007699012756, 'learning_rate': 1.9047619047619046e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5719, 'grad_norm': 0.2300710380077362, 'learning_rate': 1.9012214551248008e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6424, 'grad_norm': 0.25459790229797363, 'learning_rate': 1.897681005487697e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7441, 'grad_norm': 0.2849428355693817, 'learning_rate': 1.894140555850593e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6778, 'grad_norm': 0.23581141233444214, 'learning_rate': 1.8906001062134892e-05, 'epoch': 0.91}\n",
      "{'loss': 1.0213, 'grad_norm': 0.23469455540180206, 'learning_rate': 1.8870596565763854e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5208, 'grad_norm': 0.2017444670200348, 'learning_rate': 1.8835192069392812e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8913, 'grad_norm': 0.23202580213546753, 'learning_rate': 1.8799787573021774e-05, 'epoch': 0.91}\n",
      "{'loss': 0.673, 'grad_norm': 0.2330305129289627, 'learning_rate': 1.8764383076650735e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6602, 'grad_norm': 0.1949651837348938, 'learning_rate': 1.8728978580279697e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5932, 'grad_norm': 0.2774745523929596, 'learning_rate': 1.8693574083908658e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8057, 'grad_norm': 0.27669742703437805, 'learning_rate': 1.865816958753762e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7438, 'grad_norm': 0.19555936753749847, 'learning_rate': 1.862276509116658e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7253, 'grad_norm': 0.23582489788532257, 'learning_rate': 1.858736059479554e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6276, 'grad_norm': 0.26887208223342896, 'learning_rate': 1.85519560984245e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7263, 'grad_norm': 0.2582901120185852, 'learning_rate': 1.8516551602053462e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6664, 'grad_norm': 0.20621781051158905, 'learning_rate': 1.8481147105682424e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4921, 'grad_norm': 0.24675790965557098, 'learning_rate': 1.8445742609311386e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7448, 'grad_norm': 0.20463065803050995, 'learning_rate': 1.8410338112940344e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7159, 'grad_norm': 0.2357073277235031, 'learning_rate': 1.8374933616569305e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8627, 'grad_norm': 0.20859979093074799, 'learning_rate': 1.8339529120198263e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7517, 'grad_norm': 0.22100023925304413, 'learning_rate': 1.8304124623827225e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9591, 'grad_norm': 0.30052998661994934, 'learning_rate': 1.8268720127456186e-05, 'epoch': 0.91}\n",
      "{'loss': 0.528, 'grad_norm': 0.19604328274726868, 'learning_rate': 1.8233315631085148e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5814, 'grad_norm': 0.22641780972480774, 'learning_rate': 1.819791113471411e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7767, 'grad_norm': 0.22769145667552948, 'learning_rate': 1.816250663834307e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8962, 'grad_norm': 0.26399514079093933, 'learning_rate': 1.812710214197203e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8129, 'grad_norm': 0.2944096028804779, 'learning_rate': 1.809169764560099e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5678, 'grad_norm': 0.24115316569805145, 'learning_rate': 1.8056293149229952e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7055, 'grad_norm': 0.2945714592933655, 'learning_rate': 1.8020888652858914e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6355, 'grad_norm': 0.1957179605960846, 'learning_rate': 1.7985484156487875e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7716, 'grad_norm': 0.21307373046875, 'learning_rate': 1.7950079660116837e-05, 'epoch': 0.91}\n",
      "{'loss': 0.744, 'grad_norm': 0.2537693977355957, 'learning_rate': 1.79146751637458e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8981, 'grad_norm': 0.2887837290763855, 'learning_rate': 1.7879270667374757e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6576, 'grad_norm': 0.19581595063209534, 'learning_rate': 1.7843866171003718e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6894, 'grad_norm': 0.2809220254421234, 'learning_rate': 1.780846167463268e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7813, 'grad_norm': 0.3133709728717804, 'learning_rate': 1.777305717826164e-05, 'epoch': 0.91}\n",
      "{'loss': 0.404, 'grad_norm': 0.15531539916992188, 'learning_rate': 1.7737652681890603e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9491, 'grad_norm': 0.25936031341552734, 'learning_rate': 1.770224818551956e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4545, 'grad_norm': 0.18833740055561066, 'learning_rate': 1.7666843689148523e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7641, 'grad_norm': 0.1993030458688736, 'learning_rate': 1.7631439192777484e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8072, 'grad_norm': 0.22388029098510742, 'learning_rate': 1.7596034696406442e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9291, 'grad_norm': 0.20953308045864105, 'learning_rate': 1.7560630200035404e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7758, 'grad_norm': 0.3283420503139496, 'learning_rate': 1.7525225703664365e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9212, 'grad_norm': 0.22299674153327942, 'learning_rate': 1.7489821207293327e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5921, 'grad_norm': 0.20748752355575562, 'learning_rate': 1.745441671092229e-05, 'epoch': 0.91}\n",
      "{'loss': 0.5556, 'grad_norm': 0.20025570690631866, 'learning_rate': 1.741901221455125e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6691, 'grad_norm': 0.22319862246513367, 'learning_rate': 1.7383607718180208e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8083, 'grad_norm': 0.26940274238586426, 'learning_rate': 1.734820322180917e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6262, 'grad_norm': 0.16974438726902008, 'learning_rate': 1.731279872543813e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9284, 'grad_norm': 0.28883442282676697, 'learning_rate': 1.7277394229067093e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7397, 'grad_norm': 0.24935834109783173, 'learning_rate': 1.7241989732696054e-05, 'epoch': 0.91}\n",
      "{'loss': 0.7504, 'grad_norm': 0.23160657286643982, 'learning_rate': 1.7206585236325016e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8076, 'grad_norm': 0.2947135269641876, 'learning_rate': 1.7171180739953974e-05, 'epoch': 0.91}\n",
      "{'loss': 0.8253, 'grad_norm': 0.24486079812049866, 'learning_rate': 1.7135776243582936e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6019, 'grad_norm': 0.2128099799156189, 'learning_rate': 1.7100371747211897e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4222, 'grad_norm': 0.2292240858078003, 'learning_rate': 1.706496725084086e-05, 'epoch': 0.91}\n",
      "{'loss': 0.9358, 'grad_norm': 0.28277385234832764, 'learning_rate': 1.702956275446982e-05, 'epoch': 0.92}\n",
      "{'loss': 0.754, 'grad_norm': 0.2035641074180603, 'learning_rate': 1.699415825809878e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6281, 'grad_norm': 0.2522812485694885, 'learning_rate': 1.695875376172774e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7579, 'grad_norm': 0.2719719707965851, 'learning_rate': 1.69233492653567e-05, 'epoch': 0.92}\n",
      "{'loss': 1.0556, 'grad_norm': 0.2610705494880676, 'learning_rate': 1.6887944768985663e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8405, 'grad_norm': 0.2916020452976227, 'learning_rate': 1.685254027261462e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6757, 'grad_norm': 0.18460862338542938, 'learning_rate': 1.6817135776243583e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4159, 'grad_norm': 0.17850132286548615, 'learning_rate': 1.6781731279872544e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6748, 'grad_norm': 0.22901400923728943, 'learning_rate': 1.6746326783501506e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6815, 'grad_norm': 0.1817447394132614, 'learning_rate': 1.6710922287130467e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8037, 'grad_norm': 0.30086448788642883, 'learning_rate': 1.6675517790759425e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7471, 'grad_norm': 0.19366060197353363, 'learning_rate': 1.6640113294388387e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6379, 'grad_norm': 0.20293650031089783, 'learning_rate': 1.660470879801735e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5454, 'grad_norm': 0.20945875346660614, 'learning_rate': 1.656930430164631e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5468, 'grad_norm': 0.27032962441444397, 'learning_rate': 1.653389980527527e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7815, 'grad_norm': 0.22524544596672058, 'learning_rate': 1.6498495308904233e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6134, 'grad_norm': 0.2482805997133255, 'learning_rate': 1.646309081253319e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8367, 'grad_norm': 0.23854318261146545, 'learning_rate': 1.6427686316162153e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4864, 'grad_norm': 0.1651894599199295, 'learning_rate': 1.6392281819791114e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6668, 'grad_norm': 0.2667367458343506, 'learning_rate': 1.6356877323420076e-05, 'epoch': 0.92}\n",
      "{'loss': 0.733, 'grad_norm': 0.26754701137542725, 'learning_rate': 1.6321472827049037e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6727, 'grad_norm': 0.16799262166023254, 'learning_rate': 1.6286068330678e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6198, 'grad_norm': 0.21464142203330994, 'learning_rate': 1.625066383430696e-05, 'epoch': 0.92}\n",
      "{'loss': 0.836, 'grad_norm': 0.19196131825447083, 'learning_rate': 1.621525933793592e-05, 'epoch': 0.92}\n",
      "{'loss': 0.9639, 'grad_norm': 0.28143826127052307, 'learning_rate': 1.617985484156488e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5799, 'grad_norm': 0.2686339020729065, 'learning_rate': 1.614445034519384e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5893, 'grad_norm': 0.2708118259906769, 'learning_rate': 1.61090458488228e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8904, 'grad_norm': 0.26843443512916565, 'learning_rate': 1.607364135245176e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5932, 'grad_norm': 0.2311806082725525, 'learning_rate': 1.6038236856080723e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6984, 'grad_norm': 0.21147307753562927, 'learning_rate': 1.6002832359709685e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5403, 'grad_norm': 0.19679686427116394, 'learning_rate': 1.5967427863338643e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5644, 'grad_norm': 0.186863973736763, 'learning_rate': 1.5932023366967604e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7316, 'grad_norm': 0.30837002396583557, 'learning_rate': 1.5896618870596566e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6197, 'grad_norm': 0.23388218879699707, 'learning_rate': 1.5861214374225527e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6566, 'grad_norm': 0.23135648667812347, 'learning_rate': 1.582580987785449e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6309, 'grad_norm': 0.19421632587909698, 'learning_rate': 1.579040538148345e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6744, 'grad_norm': 0.24449650943279266, 'learning_rate': 1.575500088511241e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6735, 'grad_norm': 0.27319592237472534, 'learning_rate': 1.571959638874137e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5149, 'grad_norm': 0.27789920568466187, 'learning_rate': 1.568419189237033e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7016, 'grad_norm': 0.26131829619407654, 'learning_rate': 1.5648787395999293e-05, 'epoch': 0.92}\n",
      "{'loss': 1.0312, 'grad_norm': 0.2728540897369385, 'learning_rate': 1.5613382899628255e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8596, 'grad_norm': 0.21080631017684937, 'learning_rate': 1.5577978403257216e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6335, 'grad_norm': 0.2215251922607422, 'learning_rate': 1.5542573906886178e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6243, 'grad_norm': 0.24831213057041168, 'learning_rate': 1.5507169410515136e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5469, 'grad_norm': 0.2525768280029297, 'learning_rate': 1.5471764914144097e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7088, 'grad_norm': 0.30701518058776855, 'learning_rate': 1.543636041777306e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6265, 'grad_norm': 0.26986706256866455, 'learning_rate': 1.5400955921402017e-05, 'epoch': 0.92}\n",
      "{'loss': 0.724, 'grad_norm': 0.2621811628341675, 'learning_rate': 1.536555142503098e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5306, 'grad_norm': 0.1803145706653595, 'learning_rate': 1.533014692865994e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8878, 'grad_norm': 0.23517747223377228, 'learning_rate': 1.5294742432288902e-05, 'epoch': 0.92}\n",
      "{'loss': 0.9245, 'grad_norm': 0.25873851776123047, 'learning_rate': 1.525933793591786e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4798, 'grad_norm': 0.19644998013973236, 'learning_rate': 1.5223933439546823e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8683, 'grad_norm': 0.23448173701763153, 'learning_rate': 1.5188528943175783e-05, 'epoch': 0.92}\n",
      "{'loss': 0.9585, 'grad_norm': 0.2256285548210144, 'learning_rate': 1.5153124446804745e-05, 'epoch': 0.92}\n",
      "{'loss': 0.8531, 'grad_norm': 0.20744188129901886, 'learning_rate': 1.5117719950433706e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5284, 'grad_norm': 0.19582052528858185, 'learning_rate': 1.5082315454062668e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7554, 'grad_norm': 0.2680678367614746, 'learning_rate': 1.5046910957691626e-05, 'epoch': 0.93}\n",
      "{'loss': 0.9133, 'grad_norm': 0.23186132311820984, 'learning_rate': 1.5011506461320587e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5496, 'grad_norm': 0.2486400455236435, 'learning_rate': 1.4976101964949549e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6778, 'grad_norm': 0.2437426596879959, 'learning_rate': 1.494069746857851e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4965, 'grad_norm': 0.19167345762252808, 'learning_rate': 1.4905292972207472e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6691, 'grad_norm': 0.25622183084487915, 'learning_rate': 1.4869888475836432e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5596, 'grad_norm': 0.20246735215187073, 'learning_rate': 1.4834483979465393e-05, 'epoch': 0.93}\n",
      "{'loss': 0.9315, 'grad_norm': 0.21119892597198486, 'learning_rate': 1.4799079483094353e-05, 'epoch': 0.93}\n",
      "{'loss': 1.0208, 'grad_norm': 0.23339445888996124, 'learning_rate': 1.4763674986723313e-05, 'epoch': 0.93}\n",
      "{'loss': 1.1311, 'grad_norm': 0.2832421064376831, 'learning_rate': 1.4728270490352275e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5677, 'grad_norm': 0.21691642701625824, 'learning_rate': 1.4692865993981236e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7278, 'grad_norm': 0.26573070883750916, 'learning_rate': 1.4657461497610198e-05, 'epoch': 0.93}\n",
      "{'loss': 0.859, 'grad_norm': 0.2545597553253174, 'learning_rate': 1.462205700123916e-05, 'epoch': 0.93}\n",
      "{'loss': 0.9705, 'grad_norm': 0.33206722140312195, 'learning_rate': 1.458665250486812e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6801, 'grad_norm': 0.2891370356082916, 'learning_rate': 1.4551248008497079e-05, 'epoch': 0.93}\n",
      "{'loss': 0.9202, 'grad_norm': 0.2540942132472992, 'learning_rate': 1.451584351212604e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6713, 'grad_norm': 0.25152087211608887, 'learning_rate': 1.4480439015755002e-05, 'epoch': 0.93}\n",
      "{'loss': 0.8142, 'grad_norm': 0.23183946311473846, 'learning_rate': 1.4445034519383962e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5342, 'grad_norm': 0.22899365425109863, 'learning_rate': 1.4409630023012923e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7006, 'grad_norm': 0.26406776905059814, 'learning_rate': 1.4374225526641885e-05, 'epoch': 0.93}\n",
      "{'loss': 0.523, 'grad_norm': 0.25798115134239197, 'learning_rate': 1.4338821030270847e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6184, 'grad_norm': 0.19026732444763184, 'learning_rate': 1.4303416533899805e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7531, 'grad_norm': 0.2311057597398758, 'learning_rate': 1.4268012037528766e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5868, 'grad_norm': 0.2232431024312973, 'learning_rate': 1.4232607541157728e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6476, 'grad_norm': 0.2534189522266388, 'learning_rate': 1.419720304478669e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6184, 'grad_norm': 0.22476258873939514, 'learning_rate': 1.416179854841565e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7822, 'grad_norm': 0.2467982918024063, 'learning_rate': 1.412639405204461e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5537, 'grad_norm': 0.22236190736293793, 'learning_rate': 1.409098955567357e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4711, 'grad_norm': 0.21890480816364288, 'learning_rate': 1.4055585059302532e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5855, 'grad_norm': 0.21256092190742493, 'learning_rate': 1.4020180562931492e-05, 'epoch': 0.93}\n",
      "{'loss': 1.0579, 'grad_norm': 0.24850310385227203, 'learning_rate': 1.3984776066560453e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6668, 'grad_norm': 0.2650527060031891, 'learning_rate': 1.3949371570189415e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7435, 'grad_norm': 0.2186208963394165, 'learning_rate': 1.3913967073818377e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5621, 'grad_norm': 0.24528323113918304, 'learning_rate': 1.3878562577447338e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6394, 'grad_norm': 0.23983493447303772, 'learning_rate': 1.3843158081076296e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7993, 'grad_norm': 0.1827143281698227, 'learning_rate': 1.3807753584705258e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6459, 'grad_norm': 0.25637006759643555, 'learning_rate': 1.377234908833422e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5745, 'grad_norm': 0.2713753283023834, 'learning_rate': 1.3736944591963181e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7752, 'grad_norm': 0.3051273226737976, 'learning_rate': 1.370154009559214e-05, 'epoch': 0.93}\n",
      "{'loss': 0.945, 'grad_norm': 0.20252053439617157, 'learning_rate': 1.3666135599221102e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5076, 'grad_norm': 0.30382922291755676, 'learning_rate': 1.3630731102850064e-05, 'epoch': 0.93}\n",
      "{'loss': 0.661, 'grad_norm': 0.23697152733802795, 'learning_rate': 1.3595326606479022e-05, 'epoch': 0.93}\n",
      "{'loss': 0.9072, 'grad_norm': 0.3045092225074768, 'learning_rate': 1.3559922110107983e-05, 'epoch': 0.93}\n",
      "{'loss': 0.9954, 'grad_norm': 0.2019047737121582, 'learning_rate': 1.3524517613736945e-05, 'epoch': 0.93}\n",
      "{'loss': 0.4307, 'grad_norm': 0.1895301192998886, 'learning_rate': 1.3489113117365907e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6644, 'grad_norm': 0.21226385235786438, 'learning_rate': 1.3453708620994868e-05, 'epoch': 0.93}\n",
      "{'loss': 0.8668, 'grad_norm': 0.22854262590408325, 'learning_rate': 1.341830412462383e-05, 'epoch': 0.93}\n",
      "{'loss': 0.8871, 'grad_norm': 0.20945985615253448, 'learning_rate': 1.3382899628252788e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6289, 'grad_norm': 0.26734116673469543, 'learning_rate': 1.334749513188175e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6732, 'grad_norm': 0.2309282422065735, 'learning_rate': 1.331209063551071e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7511, 'grad_norm': 0.22566154599189758, 'learning_rate': 1.327668613913967e-05, 'epoch': 0.93}\n",
      "{'loss': 0.738, 'grad_norm': 0.21843264997005463, 'learning_rate': 1.3241281642768632e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6822, 'grad_norm': 0.20209068059921265, 'learning_rate': 1.3205877146397594e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6412, 'grad_norm': 0.21177880465984344, 'learning_rate': 1.3170472650026555e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6159, 'grad_norm': 0.19987517595291138, 'learning_rate': 1.3135068153655514e-05, 'epoch': 0.93}\n",
      "{'loss': 0.8028, 'grad_norm': 0.2190072238445282, 'learning_rate': 1.3099663657284475e-05, 'epoch': 0.93}\n",
      "{'loss': 1.0729, 'grad_norm': 0.3497622609138489, 'learning_rate': 1.3064259160913437e-05, 'epoch': 0.93}\n",
      "{'loss': 0.5972, 'grad_norm': 0.19884158670902252, 'learning_rate': 1.3028854664542398e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7627, 'grad_norm': 0.21666580438613892, 'learning_rate': 1.299345016817136e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8769, 'grad_norm': 0.2594141364097595, 'learning_rate': 1.295804567180032e-05, 'epoch': 0.94}\n",
      "{'loss': 0.963, 'grad_norm': 0.22876282036304474, 'learning_rate': 1.2922641175429281e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8579, 'grad_norm': 0.24954481422901154, 'learning_rate': 1.288723667905824e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7087, 'grad_norm': 0.23040978610515594, 'learning_rate': 1.28518321826872e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6694, 'grad_norm': 0.17723976075649261, 'learning_rate': 1.2816427686316162e-05, 'epoch': 0.94}\n",
      "{'loss': 1.0536, 'grad_norm': 0.30037933588027954, 'learning_rate': 1.2781023189945124e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6447, 'grad_norm': 0.18714198470115662, 'learning_rate': 1.2745618693574085e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7221, 'grad_norm': 0.2655615508556366, 'learning_rate': 1.2710214197203047e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8232, 'grad_norm': 0.2136455923318863, 'learning_rate': 1.2674809700832005e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6848, 'grad_norm': 0.2396671324968338, 'learning_rate': 1.2639405204460967e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7934, 'grad_norm': 0.3021796643733978, 'learning_rate': 1.2604000708089928e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8622, 'grad_norm': 0.22423698008060455, 'learning_rate': 1.2568596211718888e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5798, 'grad_norm': 0.20980578660964966, 'learning_rate': 1.253319171534785e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9915, 'grad_norm': 0.2649257481098175, 'learning_rate': 1.249778721897681e-05, 'epoch': 0.94}\n",
      "{'loss': 1.324, 'grad_norm': 0.20299501717090607, 'learning_rate': 1.2462382722605771e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6964, 'grad_norm': 0.21344129741191864, 'learning_rate': 1.2426978226234733e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5424, 'grad_norm': 0.19346190989017487, 'learning_rate': 1.2391573729863694e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7916, 'grad_norm': 0.2904641628265381, 'learning_rate': 1.2356169233492654e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9419, 'grad_norm': 0.23456935584545135, 'learning_rate': 1.2320764737121615e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9886, 'grad_norm': 0.26538583636283875, 'learning_rate': 1.2285360240750577e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6053, 'grad_norm': 0.2480284869670868, 'learning_rate': 1.2249955744379537e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8627, 'grad_norm': 0.28243768215179443, 'learning_rate': 1.2214551248008498e-05, 'epoch': 0.94}\n",
      "{'loss': 0.741, 'grad_norm': 0.21336793899536133, 'learning_rate': 1.2179146751637458e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7113, 'grad_norm': 0.24464468657970428, 'learning_rate': 1.2143742255266418e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7268, 'grad_norm': 0.23908565938472748, 'learning_rate': 1.210833775889538e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5385, 'grad_norm': 0.20393797755241394, 'learning_rate': 1.2072933262524341e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9851, 'grad_norm': 0.3090968430042267, 'learning_rate': 1.2037528766153303e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9909, 'grad_norm': 0.3483903408050537, 'learning_rate': 1.2002124269782263e-05, 'epoch': 0.94}\n",
      "{'loss': 0.54, 'grad_norm': 0.20873452723026276, 'learning_rate': 1.1966719773411224e-05, 'epoch': 0.94}\n",
      "{'loss': 0.875, 'grad_norm': 0.26501965522766113, 'learning_rate': 1.1931315277040186e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8228, 'grad_norm': 0.24725498259067535, 'learning_rate': 1.1895910780669145e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7415, 'grad_norm': 0.2121807336807251, 'learning_rate': 1.1860506284298107e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6513, 'grad_norm': 0.18725767731666565, 'learning_rate': 1.1825101787927067e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7606, 'grad_norm': 0.23135167360305786, 'learning_rate': 1.1789697291556028e-05, 'epoch': 0.94}\n",
      "{'loss': 1.0944, 'grad_norm': 0.3046342730522156, 'learning_rate': 1.1754292795184988e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8722, 'grad_norm': 0.24875110387802124, 'learning_rate': 1.171888829881395e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7634, 'grad_norm': 0.2794053554534912, 'learning_rate': 1.1683483802442911e-05, 'epoch': 0.94}\n",
      "{'loss': 0.951, 'grad_norm': 0.21651051938533783, 'learning_rate': 1.1648079306071871e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6917, 'grad_norm': 0.1957111805677414, 'learning_rate': 1.1612674809700833e-05, 'epoch': 0.94}\n",
      "{'loss': 0.593, 'grad_norm': 0.2510700523853302, 'learning_rate': 1.1577270313329794e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7216, 'grad_norm': 0.20003297924995422, 'learning_rate': 1.1541865816958754e-05, 'epoch': 0.94}\n",
      "{'loss': 0.651, 'grad_norm': 0.2103726714849472, 'learning_rate': 1.1506461320587716e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7363, 'grad_norm': 0.25327008962631226, 'learning_rate': 1.1471056824216676e-05, 'epoch': 0.94}\n",
      "{'loss': 0.5542, 'grad_norm': 0.17472538352012634, 'learning_rate': 1.1435652327845637e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8449, 'grad_norm': 0.23821042478084564, 'learning_rate': 1.1400247831474597e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6501, 'grad_norm': 0.21461792290210724, 'learning_rate': 1.1364843335103558e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6345, 'grad_norm': 0.20122048258781433, 'learning_rate': 1.132943883873252e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7281, 'grad_norm': 0.2920414209365845, 'learning_rate': 1.129403434236148e-05, 'epoch': 0.94}\n",
      "{'loss': 0.9194, 'grad_norm': 0.23956504464149475, 'learning_rate': 1.1258629845990441e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6556, 'grad_norm': 0.2537996470928192, 'learning_rate': 1.1223225349619403e-05, 'epoch': 0.94}\n",
      "{'loss': 1.0131, 'grad_norm': 0.2547033727169037, 'learning_rate': 1.1187820853248363e-05, 'epoch': 0.94}\n",
      "{'loss': 0.8904, 'grad_norm': 0.2873116731643677, 'learning_rate': 1.1152416356877324e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7202, 'grad_norm': 0.21541397273540497, 'learning_rate': 1.1117011860506286e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6426, 'grad_norm': 0.22703753411769867, 'learning_rate': 1.1081607364135246e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7538, 'grad_norm': 0.236183300614357, 'learning_rate': 1.1046202867764206e-05, 'epoch': 0.94}\n",
      "{'loss': 0.7976, 'grad_norm': 0.22985483705997467, 'learning_rate': 1.1010798371393167e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6719, 'grad_norm': 0.2128414809703827, 'learning_rate': 1.0975393875022129e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7759, 'grad_norm': 0.21440955996513367, 'learning_rate': 1.0939989378651088e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6062, 'grad_norm': 0.20695346593856812, 'learning_rate': 1.090458488228005e-05, 'epoch': 0.95}\n",
      "{'loss': 0.756, 'grad_norm': 0.24571731686592102, 'learning_rate': 1.0869180385909012e-05, 'epoch': 0.95}\n",
      "{'loss': 0.4555, 'grad_norm': 0.17310650646686554, 'learning_rate': 1.0833775889537971e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6495, 'grad_norm': 0.22777631878852844, 'learning_rate': 1.0798371393166933e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6699, 'grad_norm': 0.19556085765361786, 'learning_rate': 1.0762966896795894e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6004, 'grad_norm': 0.16348764300346375, 'learning_rate': 1.0727562400424854e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7243, 'grad_norm': 0.19995354115962982, 'learning_rate': 1.0692157904053814e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8619, 'grad_norm': 0.26828354597091675, 'learning_rate': 1.0656753407682776e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8747, 'grad_norm': 0.23273655772209167, 'learning_rate': 1.0621348911311737e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5881, 'grad_norm': 0.17650055885314941, 'learning_rate': 1.0585944414940697e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7603, 'grad_norm': 0.24772141873836517, 'learning_rate': 1.0550539918569659e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6668, 'grad_norm': 0.28350648283958435, 'learning_rate': 1.051513542219862e-05, 'epoch': 0.95}\n",
      "{'loss': 0.936, 'grad_norm': 0.2811836004257202, 'learning_rate': 1.047973092582758e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7657, 'grad_norm': 0.24639898538589478, 'learning_rate': 1.0444326429456542e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7435, 'grad_norm': 0.23447488248348236, 'learning_rate': 1.0408921933085503e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7763, 'grad_norm': 0.23439431190490723, 'learning_rate': 1.0373517436714465e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5616, 'grad_norm': 0.19736464321613312, 'learning_rate': 1.0338112940343425e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5999, 'grad_norm': 0.26108986139297485, 'learning_rate': 1.0302708443972384e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5809, 'grad_norm': 0.2219991236925125, 'learning_rate': 1.0267303947601346e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7147, 'grad_norm': 0.19595585763454437, 'learning_rate': 1.0231899451230306e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8812, 'grad_norm': 0.2847551107406616, 'learning_rate': 1.0196494954859267e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8498, 'grad_norm': 0.2727198004722595, 'learning_rate': 1.0161090458488229e-05, 'epoch': 0.95}\n",
      "{'loss': 0.7175, 'grad_norm': 0.21924710273742676, 'learning_rate': 1.0125685962117189e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5866, 'grad_norm': 0.21451207995414734, 'learning_rate': 1.009028146574615e-05, 'epoch': 0.95}\n",
      "{'loss': 0.6786, 'grad_norm': 0.255744069814682, 'learning_rate': 1.0054876969375112e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5693, 'grad_norm': 0.34959009289741516, 'learning_rate': 1.0019472473004073e-05, 'epoch': 0.95}\n",
      "{'loss': 0.5798, 'grad_norm': 0.19540433585643768, 'learning_rate': 9.984067976633033e-06, 'epoch': 0.95}\n",
      "{'loss': 0.7318, 'grad_norm': 0.19409365952014923, 'learning_rate': 9.948663480261993e-06, 'epoch': 0.95}\n",
      "{'loss': 0.9801, 'grad_norm': 0.26766902208328247, 'learning_rate': 9.913258983890955e-06, 'epoch': 0.95}\n",
      "{'loss': 0.7165, 'grad_norm': 0.2803910970687866, 'learning_rate': 9.877854487519914e-06, 'epoch': 0.95}\n",
      "{'loss': 0.6822, 'grad_norm': 0.2322421371936798, 'learning_rate': 9.842449991148876e-06, 'epoch': 0.95}\n",
      "{'loss': 0.733, 'grad_norm': 0.25339651107788086, 'learning_rate': 9.807045494777837e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5732, 'grad_norm': 0.19090363383293152, 'learning_rate': 9.771640998406797e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5015, 'grad_norm': 0.1936732828617096, 'learning_rate': 9.736236502035759e-06, 'epoch': 0.95}\n",
      "{'loss': 0.4668, 'grad_norm': 0.17441530525684357, 'learning_rate': 9.70083200566472e-06, 'epoch': 0.95}\n",
      "{'loss': 0.7412, 'grad_norm': 0.21972979605197906, 'learning_rate': 9.665427509293682e-06, 'epoch': 0.95}\n",
      "{'loss': 0.6725, 'grad_norm': 0.2475898712873459, 'learning_rate': 9.630023012922642e-06, 'epoch': 0.95}\n",
      "{'loss': 0.6604, 'grad_norm': 0.18190664052963257, 'learning_rate': 9.594618516551603e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5416, 'grad_norm': 0.17280453443527222, 'learning_rate': 9.559214020180563e-06, 'epoch': 0.95}\n",
      "{'loss': 0.6011, 'grad_norm': 0.17795689404010773, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.95}\n",
      "{'loss': 0.9856, 'grad_norm': 0.2981184720993042, 'learning_rate': 9.488405027438485e-06, 'epoch': 0.95}\n",
      "{'loss': 0.7825, 'grad_norm': 0.21218998730182648, 'learning_rate': 9.453000531067446e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5652, 'grad_norm': 0.19673067331314087, 'learning_rate': 9.417596034696406e-06, 'epoch': 0.95}\n",
      "{'loss': 0.8425, 'grad_norm': 0.21068163216114044, 'learning_rate': 9.382191538325368e-06, 'epoch': 0.95}\n",
      "{'loss': 0.8179, 'grad_norm': 0.28782516717910767, 'learning_rate': 9.346787041954329e-06, 'epoch': 0.95}\n",
      "{'loss': 0.6977, 'grad_norm': 0.22190630435943604, 'learning_rate': 9.31138254558329e-06, 'epoch': 0.95}\n",
      "{'loss': 0.9821, 'grad_norm': 0.24138899147510529, 'learning_rate': 9.27597804921225e-06, 'epoch': 0.95}\n",
      "{'loss': 0.4529, 'grad_norm': 0.2254476547241211, 'learning_rate': 9.240573552841212e-06, 'epoch': 0.95}\n",
      "{'loss': 0.6759, 'grad_norm': 0.1805209517478943, 'learning_rate': 9.205169056470172e-06, 'epoch': 0.95}\n",
      "{'loss': 0.9103, 'grad_norm': 0.24743294715881348, 'learning_rate': 9.169764560099132e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5967, 'grad_norm': 0.24097685515880585, 'learning_rate': 9.134360063728093e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5587, 'grad_norm': 0.17859090864658356, 'learning_rate': 9.098955567357055e-06, 'epoch': 0.95}\n",
      "{'loss': 0.9244, 'grad_norm': 0.2147354781627655, 'learning_rate': 9.063551070986015e-06, 'epoch': 0.95}\n",
      "{'loss': 0.5896, 'grad_norm': 0.36984455585479736, 'learning_rate': 9.028146574614976e-06, 'epoch': 0.96}\n",
      "{'loss': 0.9446, 'grad_norm': 0.24079665541648865, 'learning_rate': 8.992742078243938e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6399, 'grad_norm': 0.1829749494791031, 'learning_rate': 8.9573375818729e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5391, 'grad_norm': 0.1994277536869049, 'learning_rate': 8.921933085501859e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7661, 'grad_norm': 0.25628790259361267, 'learning_rate': 8.88652858913082e-06, 'epoch': 0.96}\n",
      "{'loss': 0.545, 'grad_norm': 0.23621682822704315, 'learning_rate': 8.85112409275978e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7135, 'grad_norm': 0.24340468645095825, 'learning_rate': 8.815719596388742e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5079, 'grad_norm': 0.1772611141204834, 'learning_rate': 8.780315100017702e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7724, 'grad_norm': 0.214988112449646, 'learning_rate': 8.744910603646663e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8254, 'grad_norm': 0.21436871588230133, 'learning_rate': 8.709506107275625e-06, 'epoch': 0.96}\n",
      "{'loss': 1.1973, 'grad_norm': 0.25730326771736145, 'learning_rate': 8.674101610904585e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8035, 'grad_norm': 0.2259296476840973, 'learning_rate': 8.638697114533546e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6494, 'grad_norm': 0.23976732790470123, 'learning_rate': 8.603292618162508e-06, 'epoch': 0.96}\n",
      "{'loss': 0.4997, 'grad_norm': 0.19480422139167786, 'learning_rate': 8.567888121791468e-06, 'epoch': 0.96}\n",
      "{'loss': 1.0433, 'grad_norm': 0.32553038001060486, 'learning_rate': 8.53248362542043e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6429, 'grad_norm': 0.28604021668434143, 'learning_rate': 8.49707912904939e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8707, 'grad_norm': 0.2565664052963257, 'learning_rate': 8.46167463267835e-06, 'epoch': 0.96}\n",
      "{'loss': 0.4635, 'grad_norm': 0.18128494918346405, 'learning_rate': 8.42627013630731e-06, 'epoch': 0.96}\n",
      "{'loss': 0.834, 'grad_norm': 0.22030490636825562, 'learning_rate': 8.390865639936272e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7587, 'grad_norm': 0.2738131284713745, 'learning_rate': 8.355461143565234e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8814, 'grad_norm': 0.23755845427513123, 'learning_rate': 8.320056647194193e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7869, 'grad_norm': 0.2463727593421936, 'learning_rate': 8.284652150823155e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6423, 'grad_norm': 0.2424290031194687, 'learning_rate': 8.249247654452117e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5262, 'grad_norm': 0.183901846408844, 'learning_rate': 8.213843158081076e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5527, 'grad_norm': 0.18539386987686157, 'learning_rate': 8.178438661710038e-06, 'epoch': 0.96}\n",
      "{'loss': 1.053, 'grad_norm': 0.3073858916759491, 'learning_rate': 8.143034165339e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5383, 'grad_norm': 0.2321292608976364, 'learning_rate': 8.10762966896796e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5954, 'grad_norm': 0.21003752946853638, 'learning_rate': 8.07222517259692e-06, 'epoch': 0.96}\n",
      "{'loss': 0.722, 'grad_norm': 0.3610053062438965, 'learning_rate': 8.03682067622588e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6028, 'grad_norm': 0.2029181718826294, 'learning_rate': 8.001416179854842e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7038, 'grad_norm': 0.23719176650047302, 'learning_rate': 7.966011683483802e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6751, 'grad_norm': 0.26046860218048096, 'learning_rate': 7.930607187112764e-06, 'epoch': 0.96}\n",
      "{'loss': 0.863, 'grad_norm': 0.2388858199119568, 'learning_rate': 7.895202690741725e-06, 'epoch': 0.96}\n",
      "{'loss': 0.77, 'grad_norm': 0.19961442053318024, 'learning_rate': 7.859798194370685e-06, 'epoch': 0.96}\n",
      "{'loss': 0.747, 'grad_norm': 0.2363455444574356, 'learning_rate': 7.824393697999647e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5153, 'grad_norm': 0.2242518812417984, 'learning_rate': 7.788989201628608e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7218, 'grad_norm': 0.24877743422985077, 'learning_rate': 7.753584705257568e-06, 'epoch': 0.96}\n",
      "{'loss': 1.2375, 'grad_norm': 0.25437843799591064, 'learning_rate': 7.71818020888653e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6278, 'grad_norm': 0.2202267199754715, 'learning_rate': 7.68277571251549e-06, 'epoch': 0.96}\n",
      "{'loss': 0.703, 'grad_norm': 0.21875031292438507, 'learning_rate': 7.647371216144451e-06, 'epoch': 0.96}\n",
      "{'loss': 0.4755, 'grad_norm': 0.24146130681037903, 'learning_rate': 7.611966719773412e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8378, 'grad_norm': 0.30235838890075684, 'learning_rate': 7.576562223402372e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7656, 'grad_norm': 0.24650627374649048, 'learning_rate': 7.541157727031334e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7128, 'grad_norm': 0.23028865456581116, 'learning_rate': 7.505753230660294e-06, 'epoch': 0.96}\n",
      "{'loss': 0.6818, 'grad_norm': 0.21904930472373962, 'learning_rate': 7.470348734289255e-06, 'epoch': 0.96}\n",
      "{'loss': 1.1435, 'grad_norm': 0.18724234402179718, 'learning_rate': 7.434944237918216e-06, 'epoch': 0.96}\n",
      "{'loss': 1.1531, 'grad_norm': 0.27353784441947937, 'learning_rate': 7.399539741547177e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7495, 'grad_norm': 0.2844245731830597, 'learning_rate': 7.364135245176137e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8868, 'grad_norm': 0.2418474704027176, 'learning_rate': 7.328730748805099e-06, 'epoch': 0.96}\n",
      "{'loss': 0.754, 'grad_norm': 0.22512845695018768, 'learning_rate': 7.29332625243406e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7251, 'grad_norm': 0.2018047422170639, 'learning_rate': 7.25792175606302e-06, 'epoch': 0.96}\n",
      "{'loss': 0.7987, 'grad_norm': 0.2952929139137268, 'learning_rate': 7.222517259691981e-06, 'epoch': 0.96}\n",
      "{'loss': 0.61, 'grad_norm': 0.1982201784849167, 'learning_rate': 7.1871127633209425e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8148, 'grad_norm': 0.2570239305496216, 'learning_rate': 7.151708266949902e-06, 'epoch': 0.96}\n",
      "{'loss': 0.9334, 'grad_norm': 0.2761622965335846, 'learning_rate': 7.116303770578864e-06, 'epoch': 0.96}\n",
      "{'loss': 0.8545, 'grad_norm': 0.2548232972621918, 'learning_rate': 7.080899274207825e-06, 'epoch': 0.96}\n",
      "{'loss': 0.4961, 'grad_norm': 0.21329064667224884, 'learning_rate': 7.045494777836785e-06, 'epoch': 0.96}\n",
      "{'loss': 0.5378, 'grad_norm': 0.19447258114814758, 'learning_rate': 7.010090281465746e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5737, 'grad_norm': 0.21781454980373383, 'learning_rate': 6.9746857850947075e-06, 'epoch': 0.97}\n",
      "{'loss': 0.705, 'grad_norm': 0.21587690711021423, 'learning_rate': 6.939281288723669e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7528, 'grad_norm': 0.25169265270233154, 'learning_rate': 6.903876792352629e-06, 'epoch': 0.97}\n",
      "{'loss': 0.6615, 'grad_norm': 0.2183256298303604, 'learning_rate': 6.8684722959815904e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8011, 'grad_norm': 0.23107817769050598, 'learning_rate': 6.833067799610551e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8323, 'grad_norm': 0.2146432250738144, 'learning_rate': 6.797663303239511e-06, 'epoch': 0.97}\n",
      "{'loss': 0.4165, 'grad_norm': 0.19360600411891937, 'learning_rate': 6.7622588068684725e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7213, 'grad_norm': 0.19426526129245758, 'learning_rate': 6.726854310497434e-06, 'epoch': 0.97}\n",
      "{'loss': 0.611, 'grad_norm': 0.18234875798225403, 'learning_rate': 6.691449814126394e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8738, 'grad_norm': 0.24793578684329987, 'learning_rate': 6.656045317755355e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5716, 'grad_norm': 0.17503632605075836, 'learning_rate': 6.620640821384316e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7311, 'grad_norm': 0.18176484107971191, 'learning_rate': 6.585236325013278e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5493, 'grad_norm': 0.2635505199432373, 'learning_rate': 6.5498318286422375e-06, 'epoch': 0.97}\n",
      "{'loss': 0.6071, 'grad_norm': 0.24068523943424225, 'learning_rate': 6.514427332271199e-06, 'epoch': 0.97}\n",
      "{'loss': 0.9163, 'grad_norm': 0.2880813777446747, 'learning_rate': 6.47902283590016e-06, 'epoch': 0.97}\n",
      "{'loss': 0.6313, 'grad_norm': 0.2263346016407013, 'learning_rate': 6.44361833952912e-06, 'epoch': 0.97}\n",
      "{'loss': 0.75, 'grad_norm': 0.2291336953639984, 'learning_rate': 6.408213843158081e-06, 'epoch': 0.97}\n",
      "{'loss': 0.92, 'grad_norm': 0.280821830034256, 'learning_rate': 6.372809346787043e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7217, 'grad_norm': 0.18364451825618744, 'learning_rate': 6.3374048504160026e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5278, 'grad_norm': 0.20206081867218018, 'learning_rate': 6.302000354044964e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7437, 'grad_norm': 0.303657203912735, 'learning_rate': 6.266595857673925e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8221, 'grad_norm': 0.2586653530597687, 'learning_rate': 6.2311913613028855e-06, 'epoch': 0.97}\n",
      "{'loss': 0.4995, 'grad_norm': 0.17752987146377563, 'learning_rate': 6.195786864931847e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7298, 'grad_norm': 0.19841322302818298, 'learning_rate': 6.160382368560808e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7704, 'grad_norm': 0.25316473841667175, 'learning_rate': 6.124977872189768e-06, 'epoch': 0.97}\n",
      "{'loss': 0.9027, 'grad_norm': 0.2565268576145172, 'learning_rate': 6.089573375818729e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8148, 'grad_norm': 0.25841888785362244, 'learning_rate': 6.05416887944769e-06, 'epoch': 0.97}\n",
      "{'loss': 1.0019, 'grad_norm': 0.2042599320411682, 'learning_rate': 6.018764383076651e-06, 'epoch': 0.97}\n",
      "{'loss': 0.9515, 'grad_norm': 0.26360562443733215, 'learning_rate': 5.983359886705612e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5845, 'grad_norm': 0.2013077437877655, 'learning_rate': 5.947955390334573e-06, 'epoch': 0.97}\n",
      "{'loss': 0.6945, 'grad_norm': 0.21158409118652344, 'learning_rate': 5.9125508939635334e-06, 'epoch': 0.97}\n",
      "{'loss': 0.9527, 'grad_norm': 0.28859925270080566, 'learning_rate': 5.877146397592494e-06, 'epoch': 0.97}\n",
      "{'loss': 1.1056, 'grad_norm': 0.2562732696533203, 'learning_rate': 5.841741901221456e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8954, 'grad_norm': 0.2704976797103882, 'learning_rate': 5.806337404850416e-06, 'epoch': 0.97}\n",
      "{'loss': 0.911, 'grad_norm': 0.24802932143211365, 'learning_rate': 5.770932908479377e-06, 'epoch': 0.97}\n",
      "{'loss': 0.9699, 'grad_norm': 0.32349511981010437, 'learning_rate': 5.735528412108338e-06, 'epoch': 0.97}\n",
      "{'loss': 0.649, 'grad_norm': 0.27997419238090515, 'learning_rate': 5.7001239157372985e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7703, 'grad_norm': 0.21770909428596497, 'learning_rate': 5.66471941936626e-06, 'epoch': 0.97}\n",
      "{'loss': 0.9333, 'grad_norm': 0.25642672181129456, 'learning_rate': 5.629314922995221e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5868, 'grad_norm': 0.16829964518547058, 'learning_rate': 5.593910426624181e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7052, 'grad_norm': 0.23408186435699463, 'learning_rate': 5.558505930253143e-06, 'epoch': 0.97}\n",
      "{'loss': 0.8786, 'grad_norm': 0.30529624223709106, 'learning_rate': 5.523101433882103e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5922, 'grad_norm': 0.2175741046667099, 'learning_rate': 5.487696937511064e-06, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee37f242e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 042b0780-4952-492a-91f9-f9fdce5d13a4)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8214, 'grad_norm': 0.26235687732696533, 'learning_rate': 5.452292441140025e-06, 'epoch': 0.97}\n",
      "{'loss': 0.6556, 'grad_norm': 0.27478864789009094, 'learning_rate': 5.416887944768986e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5252, 'grad_norm': 0.20414888858795166, 'learning_rate': 5.381483448397947e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7811, 'grad_norm': 0.24595856666564941, 'learning_rate': 5.346078952026907e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7354, 'grad_norm': 0.2255927324295044, 'learning_rate': 5.310674455655869e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7331, 'grad_norm': 0.2308746874332428, 'learning_rate': 5.275269959284829e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5281, 'grad_norm': 0.23222166299819946, 'learning_rate': 5.23986546291379e-06, 'epoch': 0.97}\n",
      "{'loss': 0.57, 'grad_norm': 0.22308744490146637, 'learning_rate': 5.2044609665427516e-06, 'epoch': 0.97}\n",
      "{'loss': 0.6595, 'grad_norm': 0.24802899360656738, 'learning_rate': 5.169056470171712e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7384, 'grad_norm': 0.2327650636434555, 'learning_rate': 5.133651973800673e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7755, 'grad_norm': 0.29018545150756836, 'learning_rate': 5.098247477429634e-06, 'epoch': 0.97}\n",
      "{'loss': 0.7301, 'grad_norm': 0.2845490872859955, 'learning_rate': 5.062842981058594e-06, 'epoch': 0.97}\n",
      "{'loss': 0.5729, 'grad_norm': 0.26975035667419434, 'learning_rate': 5.027438484687556e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6895, 'grad_norm': 0.18770088255405426, 'learning_rate': 4.992033988316517e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5332, 'grad_norm': 0.28041866421699524, 'learning_rate': 4.956629491945477e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6315, 'grad_norm': 0.21898168325424194, 'learning_rate': 4.921224995574438e-06, 'epoch': 0.98}\n",
      "{'loss': 0.9642, 'grad_norm': 0.3291766047477722, 'learning_rate': 4.885820499203399e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5725, 'grad_norm': 0.19044983386993408, 'learning_rate': 4.85041600283236e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5867, 'grad_norm': 0.2627193331718445, 'learning_rate': 4.815011506461321e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5637, 'grad_norm': 0.25025078654289246, 'learning_rate': 4.779607010090282e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5318, 'grad_norm': 0.1824445277452469, 'learning_rate': 4.744202513719242e-06, 'epoch': 0.98}\n",
      "{'loss': 0.665, 'grad_norm': 0.2529354989528656, 'learning_rate': 4.708798017348203e-06, 'epoch': 0.98}\n",
      "{'loss': 0.4896, 'grad_norm': 0.15006844699382782, 'learning_rate': 4.6733935209771645e-06, 'epoch': 0.98}\n",
      "{'loss': 0.9866, 'grad_norm': 0.28181880712509155, 'learning_rate': 4.637989024606125e-06, 'epoch': 0.98}\n",
      "{'loss': 0.9827, 'grad_norm': 0.2995416820049286, 'learning_rate': 4.602584528235086e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7916, 'grad_norm': 0.23961535096168518, 'learning_rate': 4.567180031864047e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7958, 'grad_norm': 0.23352453112602234, 'learning_rate': 4.531775535493007e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6367, 'grad_norm': 0.19348657131195068, 'learning_rate': 4.496371039121969e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5499, 'grad_norm': 0.1851608157157898, 'learning_rate': 4.4609665427509296e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5939, 'grad_norm': 0.19410257041454315, 'learning_rate': 4.42556204637989e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6138, 'grad_norm': 0.28085923194885254, 'learning_rate': 4.390157550008851e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6731, 'grad_norm': 0.2616860568523407, 'learning_rate': 4.3547530536378125e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7154, 'grad_norm': 0.29096725583076477, 'learning_rate': 4.319348557266773e-06, 'epoch': 0.98}\n",
      "{'loss': 0.539, 'grad_norm': 0.24101701378822327, 'learning_rate': 4.283944060895734e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5579, 'grad_norm': 0.17450253665447235, 'learning_rate': 4.248539564524695e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7599, 'grad_norm': 0.2424216866493225, 'learning_rate': 4.213135068153655e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6785, 'grad_norm': 0.23803429305553436, 'learning_rate': 4.177730571782617e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8312, 'grad_norm': 0.24915815889835358, 'learning_rate': 4.1423260754115775e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6841, 'grad_norm': 0.18536406755447388, 'learning_rate': 4.106921579040538e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6625, 'grad_norm': 0.22897501289844513, 'learning_rate': 4.0715170826695e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5277, 'grad_norm': 0.2402217835187912, 'learning_rate': 4.03611258629846e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7597, 'grad_norm': 0.23275116086006165, 'learning_rate': 4.000708089927421e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5037, 'grad_norm': 0.17451275885105133, 'learning_rate': 3.965303593556382e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5444, 'grad_norm': 0.19894376397132874, 'learning_rate': 3.9298990971853425e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8482, 'grad_norm': 0.21908767521381378, 'learning_rate': 3.894494600814304e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8296, 'grad_norm': 0.23927871882915497, 'learning_rate': 3.859090104443265e-06, 'epoch': 0.98}\n",
      "{'loss': 0.71, 'grad_norm': 0.18142059445381165, 'learning_rate': 3.8236856080722255e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5124, 'grad_norm': 0.1756359487771988, 'learning_rate': 3.788281111701186e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5618, 'grad_norm': 0.17462852597236633, 'learning_rate': 3.752876615330147e-06, 'epoch': 0.98}\n",
      "{'loss': 0.895, 'grad_norm': 0.21526078879833221, 'learning_rate': 3.717472118959108e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6679, 'grad_norm': 0.2411644607782364, 'learning_rate': 3.6820676225880687e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5672, 'grad_norm': 0.17897166311740875, 'learning_rate': 3.64666312621703e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6567, 'grad_norm': 0.21437403559684753, 'learning_rate': 3.6112586298459905e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8236, 'grad_norm': 0.17519760131835938, 'learning_rate': 3.575854133474951e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7692, 'grad_norm': 0.2739938795566559, 'learning_rate': 3.5404496371039127e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6233, 'grad_norm': 0.24924136698246002, 'learning_rate': 3.505045140732873e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6129, 'grad_norm': 0.2206767350435257, 'learning_rate': 3.4696406443618345e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6735, 'grad_norm': 0.18964309990406036, 'learning_rate': 3.4342361479907952e-06, 'epoch': 0.98}\n",
      "{'loss': 0.9083, 'grad_norm': 0.2714875638484955, 'learning_rate': 3.3988316516197555e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5939, 'grad_norm': 0.22660993039608002, 'learning_rate': 3.363427155248717e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6304, 'grad_norm': 0.1938273161649704, 'learning_rate': 3.3280226588776773e-06, 'epoch': 0.98}\n",
      "{'loss': 0.9205, 'grad_norm': 0.27135956287384033, 'learning_rate': 3.292618162506639e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7692, 'grad_norm': 0.25383666157722473, 'learning_rate': 3.2572136661355995e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6214, 'grad_norm': 0.2090669572353363, 'learning_rate': 3.22180916976456e-06, 'epoch': 0.98}\n",
      "{'loss': 0.6559, 'grad_norm': 0.19606703519821167, 'learning_rate': 3.1864046733935214e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8705, 'grad_norm': 0.2963719367980957, 'learning_rate': 3.151000177022482e-06, 'epoch': 0.98}\n",
      "{'loss': 0.5657, 'grad_norm': 0.16749857366085052, 'learning_rate': 3.1155956806514427e-06, 'epoch': 0.98}\n",
      "{'loss': 0.8045, 'grad_norm': 0.30047646164894104, 'learning_rate': 3.080191184280404e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7473, 'grad_norm': 0.23916985094547272, 'learning_rate': 3.0447866879093646e-06, 'epoch': 0.98}\n",
      "{'loss': 0.7552, 'grad_norm': 0.26580920815467834, 'learning_rate': 3.0093821915383257e-06, 'epoch': 0.99}\n",
      "{'loss': 0.775, 'grad_norm': 0.20092561841011047, 'learning_rate': 2.9739776951672864e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7844, 'grad_norm': 0.3533150255680084, 'learning_rate': 2.938573198796247e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6717, 'grad_norm': 0.2515542507171631, 'learning_rate': 2.903168702425208e-06, 'epoch': 0.99}\n",
      "{'loss': 1.0198, 'grad_norm': 0.25692620873451233, 'learning_rate': 2.867764206054169e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6716, 'grad_norm': 0.26094159483909607, 'learning_rate': 2.83235970968313e-06, 'epoch': 0.99}\n",
      "{'loss': 0.8368, 'grad_norm': 0.22916704416275024, 'learning_rate': 2.7969552133120907e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7004, 'grad_norm': 0.22229772806167603, 'learning_rate': 2.7615507169410514e-06, 'epoch': 0.99}\n",
      "{'loss': 1.1759, 'grad_norm': 0.23545560240745544, 'learning_rate': 2.7261462205700125e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5848, 'grad_norm': 0.24391809105873108, 'learning_rate': 2.6907417241989736e-06, 'epoch': 0.99}\n",
      "{'loss': 0.9934, 'grad_norm': 0.2342907190322876, 'learning_rate': 2.6553372278279343e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5673, 'grad_norm': 0.19014857709407806, 'learning_rate': 2.619932731456895e-06, 'epoch': 0.99}\n",
      "{'loss': 0.8326, 'grad_norm': 0.3177576959133148, 'learning_rate': 2.584528235085856e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5626, 'grad_norm': 0.21641387045383453, 'learning_rate': 2.549123738714817e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7221, 'grad_norm': 0.2867845892906189, 'learning_rate': 2.513719242343778e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7846, 'grad_norm': 0.26250413060188293, 'learning_rate': 2.4783147459727386e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6143, 'grad_norm': 0.18982627987861633, 'learning_rate': 2.4429102496016993e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7434, 'grad_norm': 0.3936961591243744, 'learning_rate': 2.4075057532306605e-06, 'epoch': 0.99}\n",
      "{'loss': 1.0976, 'grad_norm': 0.24889051914215088, 'learning_rate': 2.372101256859621e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6288, 'grad_norm': 0.2679308354854584, 'learning_rate': 2.3366967604885823e-06, 'epoch': 0.99}\n",
      "{'loss': 0.9216, 'grad_norm': 0.3428897261619568, 'learning_rate': 2.301292264117543e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6339, 'grad_norm': 0.23212337493896484, 'learning_rate': 2.2658877677465037e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5531, 'grad_norm': 0.209598571062088, 'learning_rate': 2.2304832713754648e-06, 'epoch': 0.99}\n",
      "{'loss': 0.9538, 'grad_norm': 0.25904160737991333, 'learning_rate': 2.1950787750044255e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6772, 'grad_norm': 0.2337401956319809, 'learning_rate': 2.1596742786333866e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6847, 'grad_norm': 0.20992496609687805, 'learning_rate': 2.1242697822623477e-06, 'epoch': 0.99}\n",
      "{'loss': 0.9041, 'grad_norm': 0.2199762612581253, 'learning_rate': 2.0888652858913084e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5913, 'grad_norm': 0.21203456819057465, 'learning_rate': 2.053460789520269e-06, 'epoch': 0.99}\n",
      "{'loss': 1.1615, 'grad_norm': 0.2792053818702698, 'learning_rate': 2.01805629314923e-06, 'epoch': 0.99}\n",
      "{'loss': 0.8394, 'grad_norm': 0.23933018743991852, 'learning_rate': 1.982651796778191e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7711, 'grad_norm': 0.2612985372543335, 'learning_rate': 1.947247300407152e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5503, 'grad_norm': 0.22101092338562012, 'learning_rate': 1.9118428040361127e-06, 'epoch': 0.99}\n",
      "{'loss': 1.2113, 'grad_norm': 0.24626997113227844, 'learning_rate': 1.8764383076650734e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6842, 'grad_norm': 0.24693971872329712, 'learning_rate': 1.8410338112940343e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5967, 'grad_norm': 0.21840915083885193, 'learning_rate': 1.8056293149229952e-06, 'epoch': 0.99}\n",
      "{'loss': 0.9203, 'grad_norm': 0.26663780212402344, 'learning_rate': 1.7702248185519564e-06, 'epoch': 0.99}\n",
      "{'loss': 0.8643, 'grad_norm': 0.2405792474746704, 'learning_rate': 1.7348203221809173e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5939, 'grad_norm': 0.23411214351654053, 'learning_rate': 1.6994158258098777e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5231, 'grad_norm': 0.27254849672317505, 'learning_rate': 1.6640113294388387e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6702, 'grad_norm': 0.23288901150226593, 'learning_rate': 1.6286068330677998e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6765, 'grad_norm': 0.24141328036785126, 'learning_rate': 1.5932023366967607e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5865, 'grad_norm': 0.19720818102359772, 'learning_rate': 1.5577978403257214e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6546, 'grad_norm': 0.1838551014661789, 'learning_rate': 1.5223933439546823e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5225, 'grad_norm': 0.19387491047382355, 'learning_rate': 1.4869888475836432e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7938, 'grad_norm': 0.22786979377269745, 'learning_rate': 1.451584351212604e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6784, 'grad_norm': 0.2756824791431427, 'learning_rate': 1.416179854841565e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7024, 'grad_norm': 0.21700069308280945, 'learning_rate': 1.3807753584705257e-06, 'epoch': 0.99}\n",
      "{'loss': 0.4921, 'grad_norm': 0.15952768921852112, 'learning_rate': 1.3453708620994868e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7365, 'grad_norm': 0.17979800701141357, 'learning_rate': 1.3099663657284475e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6789, 'grad_norm': 0.2551611363887787, 'learning_rate': 1.2745618693574084e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6419, 'grad_norm': 0.21801841259002686, 'learning_rate': 1.2391573729863693e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7343, 'grad_norm': 0.29151681065559387, 'learning_rate': 1.2037528766153302e-06, 'epoch': 0.99}\n",
      "{'loss': 0.6656, 'grad_norm': 0.24257215857505798, 'learning_rate': 1.1683483802442911e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7633, 'grad_norm': 0.24557721614837646, 'learning_rate': 1.1329438838732518e-06, 'epoch': 0.99}\n",
      "{'loss': 0.5911, 'grad_norm': 0.2228706032037735, 'learning_rate': 1.0975393875022127e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7874, 'grad_norm': 0.22377881407737732, 'learning_rate': 1.0621348911311739e-06, 'epoch': 0.99}\n",
      "{'loss': 0.7087, 'grad_norm': 0.2238227128982544, 'learning_rate': 1.0267303947601345e-06, 'epoch': 1.0}\n",
      "{'loss': 0.6544, 'grad_norm': 0.2179141640663147, 'learning_rate': 9.913258983890955e-07, 'epoch': 1.0}\n",
      "{'loss': 0.8007, 'grad_norm': 0.25111234188079834, 'learning_rate': 9.559214020180564e-07, 'epoch': 1.0}\n",
      "{'loss': 0.8185, 'grad_norm': 0.3243864178657532, 'learning_rate': 9.205169056470172e-07, 'epoch': 1.0}\n",
      "{'loss': 0.935, 'grad_norm': 0.33865946531295776, 'learning_rate': 8.851124092759782e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6804, 'grad_norm': 0.2062588930130005, 'learning_rate': 8.497079129049389e-07, 'epoch': 1.0}\n",
      "{'loss': 0.9906, 'grad_norm': 0.23157410323619843, 'learning_rate': 8.143034165338999e-07, 'epoch': 1.0}\n",
      "{'loss': 0.9095, 'grad_norm': 0.31837260723114014, 'learning_rate': 7.788989201628607e-07, 'epoch': 1.0}\n",
      "{'loss': 0.9263, 'grad_norm': 0.2597801089286804, 'learning_rate': 7.434944237918216e-07, 'epoch': 1.0}\n",
      "{'loss': 0.8148, 'grad_norm': 0.2295599728822708, 'learning_rate': 7.080899274207825e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6777, 'grad_norm': 0.21493688225746155, 'learning_rate': 6.726854310497434e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6077, 'grad_norm': 0.18092474341392517, 'learning_rate': 6.372809346787042e-07, 'epoch': 1.0}\n",
      "{'loss': 0.4777, 'grad_norm': 0.1962403804063797, 'learning_rate': 6.018764383076651e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6006, 'grad_norm': 0.23974105715751648, 'learning_rate': 5.664719419366259e-07, 'epoch': 1.0}\n",
      "{'loss': 0.7406, 'grad_norm': 0.1812230348587036, 'learning_rate': 5.310674455655869e-07, 'epoch': 1.0}\n",
      "{'loss': 0.665, 'grad_norm': 0.3258650004863739, 'learning_rate': 4.956629491945477e-07, 'epoch': 1.0}\n",
      "{'loss': 0.7787, 'grad_norm': 0.2136358916759491, 'learning_rate': 4.602584528235086e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6298, 'grad_norm': 0.19074836373329163, 'learning_rate': 4.2485395645246944e-07, 'epoch': 1.0}\n",
      "{'loss': 1.1686, 'grad_norm': 0.2771185040473938, 'learning_rate': 3.8944946008143034e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6442, 'grad_norm': 0.23380419611930847, 'learning_rate': 3.5404496371039125e-07, 'epoch': 1.0}\n",
      "{'loss': 0.7498, 'grad_norm': 0.23484528064727783, 'learning_rate': 3.186404673393521e-07, 'epoch': 1.0}\n",
      "{'loss': 0.6164, 'grad_norm': 0.21832238137722015, 'learning_rate': 2.8323597096831296e-07, 'epoch': 1.0}\n",
      "{'loss': 0.8673, 'grad_norm': 0.2072088122367859, 'learning_rate': 2.4783147459727386e-07, 'epoch': 1.0}\n",
      "{'loss': 0.4269, 'grad_norm': 0.17147138714790344, 'learning_rate': 2.1242697822623472e-07, 'epoch': 1.0}\n",
      "{'loss': 0.9045, 'grad_norm': 0.33079811930656433, 'learning_rate': 1.7702248185519562e-07, 'epoch': 1.0}\n",
      "{'loss': 0.8596, 'grad_norm': 0.2012263834476471, 'learning_rate': 1.4161798548415648e-07, 'epoch': 1.0}\n",
      "{'loss': 0.8057, 'grad_norm': 0.22766828536987305, 'learning_rate': 1.0621348911311736e-07, 'epoch': 1.0}\n",
      "{'loss': 0.7499, 'grad_norm': 0.2478722184896469, 'learning_rate': 7.080899274207824e-08, 'epoch': 1.0}\n",
      "{'loss': 0.7324, 'grad_norm': 0.2432030290365219, 'learning_rate': 3.540449637103912e-08, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee37547850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 41913d06-1ee6-4fed-a581-7d7f499b2869)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 22434.1296, 'train_samples_per_second': 2.016, 'train_steps_per_second': 0.252, 'train_loss': 0.7496720589341568, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finetuned model is then saved to disk. Since we only modifed the LoRA adapters, the export fits on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/Qwen2.5-1.5B-bnb-4bit/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x76ee378601f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\"), '(Request ID: 5daa0049-cdae-4bb5-8634-1fd6480553dd)') - silently ignoring the lookup for the file config.json in unsloth/Qwen2.5-1.5B-bnb-4bit.\n",
      "  warnings.warn(\n",
      "/home/mia/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in unsloth/Qwen2.5-1.5B-bnb-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('qwen-brilliant-1.5B/tokenizer_config.json',\n",
       " 'qwen-brilliant-1.5B/special_tokens_map.json',\n",
       " 'qwen-brilliant-1.5B/vocab.json',\n",
       " 'qwen-brilliant-1.5B/merges.txt',\n",
       " 'qwen-brilliant-1.5B/added_tokens.json',\n",
       " 'qwen-brilliant-1.5B/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"qwen-brilliant-1.5B\")\n",
    "tokenizer.save_pretrained(\"qwen-brilliant-1.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want to repeat the finetuning process, we can then simply load the model from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2 patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2080 Super with Max-Q Design. Max memory: 7.781 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536, padding_idx=151665)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"qwen-brilliant-1.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we benchmark the new model. Note that since we excluded `df_sample` from the training data, the model can't just memorize the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab31c6e88af14f7ebc590dfe3bfadb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Questions:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "If $(1,x,y)$ is a geometric sequence and $(x,y,3)$ is an arithmetic sequence then find the maximum value of $x+y$ .\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{12}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Adam, Bob, Caleb, Dylan, Elaine, Francis, Gillian, and Hamilton all decided they wanted to play Mario Kart together. They played three races, and the winner was determined by a points system:\n",
      "‚Äã For getting first place in a single race, a player was awarded 8 points.‚Äã For getting second place in a single race, a player was awarded 7 points.‚Äã For getting third place in a single race, a player was awarded 6 points.‚Äã For getting fourth place in a single race, a player was awarded 5 points.‚Äã For getting fifth place in a single race, a player was awarded 4 points.‚Äã For getting sixth place in a single race, a player was awarded 3 points.‚Äã For getting seventh place in a single race, a player was awarded 2 points.‚Äã For getting eighth place in a single race, a player was awarded 1 point.‚Äã The First Race‚Äã Francis finished directly in front of Elaine, but finished directly behind Dylan.‚Äã Bob was neither in first nor last, and did better than Adam.‚Äã Gillian came in sixth place.‚Äã Elaine did better than both Bob and Caleb, but did not do better than Hamilton. ‚ÄãDylan came in second. Adam came in last. The Second Race Hamilton did better than Dylan (Dylan was not directly behind Hamilton), but was behind Bob (not directly). Francis did better than both Bob and Elaine (in that order respectively). Gillian did better than Dylan (Dylan was directly behind Gillian), but Gillian was directly behind Caleb. Adam came in last. The Third Race Dylan and Francis finished fourth and fifth (but not necessarily in that order). Caleb did better than both Elaine and Adam. Gillian did better than Bob, but did worse than Caleb. Elaine did better than Adam. Dylan did worse than Caleb, but did better than Hamilton and Gillian. Bob came in last. Hamilton came directly behind Francis, and came directly in front of Gillian. Based on the following information, figure out where everyone placed after three races.Each racer's first initial has a number assigned to it, and that is how the question is answered A=1 B=2 C=3 D=4 E=5 F=6 G=7 H=8, so someone's answer might look like, if they thought that Adam won: 1/////// (the / just represents the numbers that would go there). If there was a tie, put the initial that came first. I.e., if both Adam and Bob had 12 points at the end, then Adam's number would go first because the letter A comes before the letter B.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{12345678}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "tan 45 = ....\n",
      "\n",
      "### Response:\n",
      "tan45=1 Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A palindromic number is one which can be written the same forward as backward. Ex: 19391, 88388, 11, etc. How many 3-digit palindromic numbers exist?\n",
      "\n",
      "### Response:\n",
      "The first digit can be any number from 1 to 9. The second digit can be any number from 0 to 9. The third digit can be any number from 0 to 9. Therefore, there are 9 x 10 x 10 = 900 palindromic numbers. Thus, the final answer is \\\\boxed{900}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\Large \\\\left( \\\\frac{81^{\\\\frac{1}{\\\\log_{5}9}}+3^{\\\\frac{3}{\\\\log_{\\\\sqrt{6}}3}}}{409}\\\\right) \\\\left((\\\\sqrt{7})^{\\\\frac{2}{\\\\log_{25}7}}-(125)^{\\\\log_{25}6}\\\\right)= \\\\, ?$\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the units digit of 2011! + 2012! + 2013! + 2014! + 2015!.\n",
      "\n",
      "### Response:\n",
      "2011! + 2012! + 2013! + 2014! + 2015! = 2011! + 2012! + 2013! + 2014! + 2015! = 2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! + 2012! + 2013! + 2014! + 2015! =2011! +\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\frac{\\\\left(1^4 + \\\\frac14\\\\right)\\\\left(3^4 + \\\\frac14\\\\right)\\\\cdots \\\\left((2n-1)^4 + \\\\frac14\\\\right)}{\\\\left(2^4 + \\\\frac14\\\\right)\\\\left(4^4 + \\\\frac14\\\\right)\\\\cdots \\\\left((2n)^4 + \\\\frac14\\\\right)}$  Given that the expression above can be simplified to $(an^2+ bn + c)^{-1}$ for constants $a,b,c$ , find $21(a-b+c)$ .\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{10}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\sin^2(1^\\\\circ)+\\\\sin^2(2^\\\\circ)+\\\\ldots+\\\\sin^2(359^\\\\circ)+\\\\sin^2(360^\\\\circ) = \\\\ ?$\n",
      "\n",
      "### Response:\n",
      "sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò)+sin‚Å°2(360‚àò)=sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò)+sin‚Å°2(0‚àò)+sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò)=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(359‚àò))=2(sin‚Å°2(1‚àò)+sin‚Å°2(2‚àò)+‚ãØ+sin‚Å°2(35\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "In $\\\\Delta ABC$ , $s$ represents the semiperimeter and $r_{1}$ represents the radius of the excircle opposite to $A$ . If $3s^{2}=r_{1}^{2}$ , then enter the degree measure of $\\\\angle A$ . This problem is part of the setTrigonometry.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{90}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the least number whose last digit is 7 and which becomes 5 times larger when this last digit is carried to the beginning  of the number.\n",
      "\n",
      "### Response:\n",
      "Let the number be 10a+7. Then, 10a+7=5(10a+7-10a) +7. Solving this, we get a=1. Thus, the final answer is \\\\boxed{17}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\int_0^1 x^6 \\\\sqrt{1-x^2} \\\\, dx$  If the integral above can be expressed as $\\\\frac {a\\\\pi}b$ for coprime positive integers $a$ and $b$ , what is the value of $a+b$ ?\n",
      "\n",
      "### Response:\n",
      "I=‚à´01x61‚àíx2dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01x6(1‚àíx2)‚àí12dx=‚à´01\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "(2+6)/4^100+(2+2√ó6)/4^99+(2+3√ó6)/4^98................+(2+99√ó6)/4^2+(2+100√ó6)/4 is equal to\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\sum_{n=0}^{\\\\infty} \\\\frac{(-1)^n}{(n+1)(2n+1)}= \\\\ ?$  Give your answer to 2 decimal places.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{0.5}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "I have 2016 identical marbles. I plan to distribute them equally into one or more identical containers. How many ways can this be done if I have an unlimited number of containers?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{2017}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\int_{-2}^5 \\\\big(|x+1|+|x+2|-|2x+3| \\\\big)\\\\ dx = \\\\ ?$   $$  Notation: $|\\\\cdot|$ denotes theabsolute value function.\n",
      "\n",
      "### Response:\n",
      "I=‚à´‚àí25‚à£x+1‚à£+‚à£x+2‚à£‚àí‚à£2x+3‚à£dx=‚à´‚àí2‚àí1(x+1)‚àí(x+2)+(2x+3)dx+‚à´‚àí10(‚àíx‚àí1)‚àí(‚àíx‚àí2)+(2x+3)dx+‚à´01(x+1)+(x+2)‚àí(2x+3)dx+‚à´12(‚àíx‚àí1)+(‚àíx‚àí2)+(2x+3)dx+‚à´25(x+1)‚àí(x+2)‚àí(2x+3)dx=‚à´‚àí2‚àí1(x+1)‚àí(x+2)+(2x+3)dx+‚à´‚àí10(‚àíx‚àí1)‚àí(‚àíx‚àí2)+(2x+3)dx+‚à´01(x+1)+(x+2)‚àí(2x+3)dx+‚à´12(‚àíx‚àí1)+(‚àíx‚àí2)+(2x+3)dx+‚à´25(x+1)‚àí(x+2)‚àí(2x+3)dx=‚à´‚àí2‚àí1(2x+3)dx+‚à´‚àí10(2x+3)dx+‚à´01(2x+3)dx+‚à´12(2x+3)dx+‚à´25(2x+3)dx=2x2+3x‚à£‚àí2‚àí1+2x2+3x‚à£‚àí10+2x2+3x‚à£01+2x2+3x‚à£12+2x2+3x‚à£25=2(‚àí2)2+3(‚àí2)+2(‚àí1)2+3(‚àí1)+2(0)2+3(0)+2(1)2+3(1)+2(2)2+3(2)+2(5)2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+3(1)+2+3(2)+2+3(5)=20+3(‚àí2)+2+3(‚àí1)+0+2+\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "What is the sum of the factors of $50! + 50$ between $1$ and $50$ inclusive? (Eg. The factors of 15 between 1 and 4 are 1 and 3)\n",
      "\n",
      "### Response:\n",
      "50! + 50 = 50! + 50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(50! + 1)50! + 1 = 2(5\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Let $f:\\\\mathbb{R} \\\\to \\\\mathbb{R}$ be a function defined by $f(x)=\\\\begin{cases} \\\\lfloor x \\\\rfloor \\\\ \\\\ , \\\\quad x \\\\leq 2 \\\\\\\\ 0 \\\\ \\\\ , \\\\quad x>2 \\\\end{cases}$ IfI=‚à´‚àí12xf(x2)2+f(x+1)dxI=\\\\int_{-1}^2 \\\\frac{xf(x^2)}{2+f(x+1)}\\\\, dxI=‚à´‚àí12‚Äã2+f(x+1)xf(x2)‚Äãdx, then find the value of $4I-1$ .\n",
      "\n",
      "### Response:\n",
      "I=‚à´‚àí12xf(x2)2+f(x+1)dx=‚à´‚àí12xf(x2)2+0dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12xf(x2)2dx=‚à´‚àí12\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the number of different ways in which the 10 letters of the word  \" P R O P O R T I O N \" can be arranged if exactly two of the O s are next to each other. You may use a calculator.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{120}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "In the diagram, $P$ is a point inside a regular hexagon $ABCDEF$ . The areas of triangles $ABP$ , $CDP$ , and $EFP$ are $\\\\SI{3}{\\\\centi\\\\meter\\\\squared}, \\\\SI{5}{\\\\centi\\\\meter\\\\squared},$ and $\\\\SI{8}{\\\\centi\\\\meter\\\\squared},$ respectively. Find the area of triangle $BCP$ in $\\\\si{\\\\centi\\\\meter\\\\squared}$ rounded off to the nearest thousandth.\n",
      "\n",
      "### Response:\n",
      "Let the area of the hexagon beAAA. Then, the area of the hexagon is the sum of the areas of the six triangles, soA=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A+8A=24A=3A+5A+8A+3A+5A\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Calculate the normalization constant $A$ if the wavefunction is  $\\\\Psi(x,t) = A\\\\text{exp}(-2\\\\pi |x|-i\\\\pi t).$\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$729x^{6}-1458x^{5}-1215x^{4}-540x^{3}-135x^{2}-18x-1=0$  The above equation has a positive real root in the form  $\\\\frac13(a^{1/6}+b^{1/6}+c^{1/6}+d^{1/6}+e^{1/6}+f^{1/6}).$  with $a, b, c, d, e, f$ are all integers. Find $a+b+c+d+e+f+3$ .\n",
      "\n",
      "### Response:\n",
      "729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí540x3‚àí135x2‚àí18x‚àí1=0729x6‚àí1458x5‚àí1215x4‚àí\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A parallel plate capacitor with plate seperation 5mm is charged by a battery.it is found that on introducing a mica aheet 2 mm thick, while keeping the connections intact, capacitor draws 25 percnt. more charge than before.Find dielectric constant of mica.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1.5}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "There were 8 actors who were very close to getting the role of Peeta in the Hunger Games and about 12 actresses who almost got the coveted role of Katniss. How many possible pairings of Katniss and Peeta there could have been if we don't take the main actors into account?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{96}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$L = \\\\lim_{n\\\\to \\\\infty} \\\\sum_{r=1}^{n} \\\\sum_{t=0}^{r-1} \\\\frac 1{5^n} \\\\binom nr \\\\binom rt 3^t$  Given the above, find the value of $L^{99} + 99^L$ . Notation: $\\\\dbinom NM = \\\\dfrac {N!}{M!(N-M)!}$ denotes thebinomial coefficient.\n",
      "\n",
      "### Response:\n",
      "L=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí115nNrCr3t=lim‚Å°n‚Üí‚àû‚àër=1n‚àët=0r‚àí1\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "The mid-points of side of a square of side 12 m are joined to form another square inside it.The mid-points of the inner square are further joined to form another square within it.This process goes on infinitely.Find the sum of areas of all such squares formed.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{144}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A ray of light is incident at an angle of $60^\\\\circ$ on one face of a prism which has an apex angle of $30^\\\\circ.$ The ray emerging out of the prism makes an angle of $30^{\\\\circ}$ with the incident ray. Calculate the refractive index of the material of the prism.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1.5}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "One day, reading One Piece and eating chips, you notice a circle that has a diameter of 5 inches. You notice a smaller circle of 4.951 inches inside the circle. If the next circle is 4.542, what is 5 circles after that? Hint: Use a Pattern\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{5}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$x$ , $y$ and $z$ are rational numbers such that  $\\\\sqrt[3]{\\\\sqrt[3]{2}-1} = \\\\sqrt[3]{x}+\\\\sqrt[3]{y}+\\\\sqrt[3]{z}$  If $x+y+z$ can be expressed as $\\\\dfrac{m}{n}$ , where $m$ and $n$ are relatively prime positive integers, find $m+n$ .\n",
      "\n",
      "### Response:\n",
      "Letx=a3,y=b3,z=c3x=a^3, y=b^3, z=c^3x=a3,y=b3,z=c3. Then we have2‚àí13=3a+3b+3c3\\\\sqrt[3]{2-1}=\\\\sqrt[3]{3a+3b+3c}32‚àí1‚Äã=33a+3b+3c‚Äã. Cubing both sides, we get2‚àí1=3a+3b+3c2-1=3a+3b+3c2‚àí1=3a+3b+3c. Hencea+b+c=1a+b+c=1a+b+c=1. Thus, the final answer is \\\\boxed{2}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Be quick!\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "How many permutations can be made out of the letters in the word ABLUTION taking four letters at a time?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{120}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Four card players A,B,C, & D are playing cards. The total amount of their money at the start of the game is 3900, with a ratio of 1:3:4:5 respectively. At the middle of the game,player D quits after losing 500. But players A,B,&C still continue playing with the new  total amount of their money having a ratio of 5:11:13 respectively, after player D quits. After the end of the game the ratio of their money with respect to the total amount becomes 9:9:11 (9 for A, 9 for B &11 for C). How much is the total winning amount did the winner gets?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1000}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A Shop Keeper wants to buy \"weighing balances\" (Weights) for his weighing scale, From his past experience he knows that he requires to measure items up to 85Kgs (in multiples of 1Kg). Find Minimum numbers of weighing balances required with which he can measure all possible weight up to 85kgs?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{4}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "What is the measure of the acute angle(in degrees) between the hour and minute hands of a correctly working clock at 4:18 ?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{12.9}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\int_0^3\\\\sqrt{\\\\dfrac{3-x}{3+x}}\\\\, dx = A\\\\left[\\\\dfrac{\\\\pi}{B}-C\\\\right]$  If the equation above holds true forpositive integers $A,B$ and $C$ , find $A+B+C$ .\n",
      "\n",
      "### Response:\n",
      "I=‚à´033‚àíx3+xdx=‚à´033‚àíx3+3‚àíx3+3dx=‚à´033‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àíx3+3‚àí\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\dfrac{1}{1}+\\\\dfrac{1}{2}-\\\\dfrac{1}{3}-\\\\dfrac{1}{4}+\\\\dfrac{1}{5}+\\\\dfrac{1}{6}-\\\\dfrac{1}{7}-\\\\dfrac{1}{8}+\\\\cdots$  If the closed form of the sum above can be expressed as $\\\\dfrac{\\\\pi+\\\\ln (a)}{a}$ , where $a$ is a positive integer, find $a$ .\n",
      "\n",
      "### Response:\n",
      "The sum can be written as‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën=1‚àû(‚àí1)n+1(1n‚àí1n+1)=‚àën\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "The terms of a sequence of positive integers satisfy $a_{n+3} = a_{n+2}(a_{n+1}+a_{n})$ , for $n = 1,2,3,\\\\dots$  If $a_6 = 8820$ , what's the sum of the digits of $a_7$ ?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{18}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Let us call a $6$ -digit numbercoolif each of its digits is no less than the preceding digit. \n",
      "How manycool $6$ -digit numbers are there? Details And Assumptions: For example, $112446$ iscool.  $233043$ isn'tcool.\n",
      "\n",
      "### Response:\n",
      "We can use the stars and bars method to solve this problem. We have 6 digits, and we want to find the number of ways to arrange them such that each digit is no less than the preceding digit. This is equivalent to finding the number of ways to arrange 6 stars and 5 bars (to separate the digits). We have a total of 11 objects to arrange, and we want to choose 6 of them to be stars. This can be done in(116)=330\\\\binom{11}{6} = 330(611‚Äã)=330ways. Thus, the final answer is \\\\boxed{330}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "If $\\\\frac{ 5x+4y} { x + 3y } = \\\\frac{3 } {2 }$ , what is the value of $\\\\frac{  3y } { x  }$ ?\n",
      "\n",
      "### Response:\n",
      "5x+4yx+3y=32\\\\frac{5x+4y}{x+3y} = \\\\frac{3}{2}x+3y5x+4y‚Äã=23‚Äã5x+4y=32(x+3y)\\\\frac{5x+4y}{3} = \\\\frac{3}{2}(x+3y)25x+4y‚Äã=23‚Äã5x+4y=92(x+3y)\\\\frac{5x+4y}{9} = \\\\frac{3}{2}(x+3y)25x+4y‚Äã=23‚Äã5x+4y=272(x+3y)\\\\frac{5x+4y}{27} = \\\\frac{3}{2}(x+3y)275x+4y‚Äã=23‚Äã10x+8y=27(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{(x+3y)}2710x+8y‚Äã=(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3y)27‚Äã10x+8y=272(x+3y)\\\\frac{10x+8y}{27} = \\\\frac{27}{2(x+3y)}2710x+8y‚Äã=2(x+3\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\sum_{x = 0} ^{100} \\\\lfloor \\\\sin (x ^{\\\\circ}) \\\\rfloor = \\\\, ?$  Give your answer to 2 decimal places.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{100.00}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the smallest positive integer n such that(i) $n$ has exactly 144 distinct positive divisors, and(ii) there are ten consecutive integers among the positive divisors of $n$ . (IMO)\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{144}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Point $I (979,2379,2500)$ is an incenter of a triangle. Vertices of the triangle are on the axes of the coordinate system. Find perimeter of the triangle. Also try these: Triangle defined by one point Triangle defined by one point II Triangle defined by one point III\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1000}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "In the $xyz$ -coordinate system, a massive particle is initially at rest on a plane whose equation is given below: $x + 2y + 3z = 0.$ The particle is launched with initial velocity $(v_x,v_y,v_z) = (-3,7,12)$ .  It travels under the influence of a gravitational acceleration of $(a_x,a_y,a_z) = (0,0,-10)$ until it intersects the plane once more. When the particle intersects the plane for the second time, how far away is it from the launch point (to 3 decimal places)?\n",
      "\n",
      "### Response:\n",
      "The particle's trajectory is given by the equationx+2y+3z=0x+2y+3z=0x+2y+3z=0.  The particle's velocity is given by the equationv‚Éó=‚àí3i^+7j^+12k^v=\\\\begin{pmatrix} -3 \\\\\\\\ 7 \\\\\\\\ 12 \\\\end{pmatrix}v=‚àí3712‚Äã.  The particle's acceleration is given by the equationa‚Éó=0i^+0j^‚àí10k^a=\\\\begin{pmatrix} 0 \\\\\\\\ 0 \\\\\\\\ -10 \\\\end{pmatrix}a=00‚àí10‚Äã.  The particle's position at timetttis given by the equationr‚Éó=r‚Éó0+‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=‚à´0tr‚ÉóÀôdt=\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find sum of all natural numbers $x$ such that the product of their digits (in decimal presentation) is equal to $x^2 - 10x - 22$ .\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{11}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Given that $x^2+x+1=0$ , find the value of $(x+\\\\dfrac{1}{x})^2+(x^2+\\\\dfrac{1}{x^2})^2+\\\\dots +(x^6+\\\\dfrac{1}{x^6})^2$ .\n",
      "\n",
      "### Response:\n",
      "x2+x+1=0x2+x=‚àí1x2+x+1=0x^2+x+1=0\\\\\\\\\n",
      "x^2+x=-1\\\\\\\\\n",
      "x^2+x+1=0x2+x+1=0x2+x=‚àí1x2+x+1=0x2+x=‚àí1x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x+1=0x2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=2‚àí1‚Äãx2+x=‚àí12x2+x+1=0x^2+x=\\\\frac{-1}{2}\\\\\\\\\n",
      "x^2+x+1=0x2+x=\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A particle in a one-dimensional quantum well is governed by a variant of the time-independent Schrodinger equation, expressed in terms of wave function $\\\\Psi (x)$ . The quantities $V$ and $E$ are the potential energy and total energy, respectively. $E$ is a positive constant.  $-\\\\frac{d^2}{d x^2} \\\\, \\\\Psi (x) + V(x) \\\\, \\\\Psi (x) = E \\\\, \\\\Psi (x)$  The potential energy varies as follows:  $V(x)=\n",
      " \\\\begin{cases}\n",
      "\\\\infty, &\\\\,\\\\, x < 0 \\\\\\\\\n",
      "0, &\\\\,\\\\, 0 \\\\leq x \\\\leq \\\\pi \\\\\\\\\n",
      "\\\\infty, &\\\\,\\\\, x > \\\\pi \\\\\\\\\n",
      "\\\\end{cases}$  The boundary conditions on $\\\\Psi (x)$ are:  $\\\\Psi(x) = \n",
      "\\\\begin{cases}\n",
      "0, \\\\,\\\\, x \\\\leq 0 \\\\\\\\ \n",
      "0, \\\\,\\\\, x \\\\geq \\\\pi \\\\\\\\ \n",
      "\\\\end{cases}$  Determine the sum of the five smallest non-zero allowable values of $E$ . Note:This problem is easily solvable by hand Solution Strategy:1)Solve the differential equation within the well $(V = 0)$ to derive a general solution for $\\\\Psi(x)$ within the well2)Apply the boundary conditions $\\\\Psi(0) = 0$ and $\\\\Psi(\\\\pi) = 0$ to the solution3)Energy quantization arises naturally as a result of the prior two steps\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1.5708}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "$\\\\large \\\\begin{cases} {a+b=8} \\\\\\\\ {ab+c+d=23} \\\\\\\\ {ad+bc=28} \\\\\\\\ {cd =12} \\\\end{cases}$  Let $a,b,c$ an $d$ dbe four real numbers satisfying the system of equations above. What is the sum of all possible different values of $a+2b+3c+4d$ ?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{10}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "ABCD is a rectangle, and E is a point on AB.in triangle DEC ,<DEC=90,DE=3,EC=4.the area of triangle ADE is  a/b(where,a and b are both co prime.). b+a=?\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{10}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Let $T_n$ denote the $n^{th}$ tetrahdonal number. $T_n$ is a tetrahedral number that is also a perfect square. $n$ is greater than $2!$ . $\\\\sqrt{T_n} + n = m$ and $\\\\phi(m)=z$ where $\\\\phi$ is Euler's Totient function. What is the decimal value of : $GCD(|T_n - m^2 - z^3|,n)$ where $\\\\text{GCD ( , )}$ is the greatest common divisor function and $| |$ is the absolute value function.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "A person looking into an empty container is able to see the far edge of the container‚Äôs bottom. The height of the container is $h$ , and its width is $d$ . When the container is completely filled with a fluid of index of refractio $n$ nand viewed from the same angle, the person can see the center of a coin at the middle of the container‚Äôs bottom. Question is that if the the container has a width of $8 \\\\,\\\\text{cm}$ and is filled with water, \n",
      "then find the height of the container in $\\\\text{cm}$ to 2 decimal places. Details and assumptions\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{10.67}.<|endoftext|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please reason step by step, and put your final answer within \\boxed{}.\n",
      "\n",
      "### Input:\n",
      "Find the number of ordered integral solutions (x,y) of the diophantine equation: y(y+1) = x(x+1)(x+2) Note: The problem is not original. Taken from \"Introductory Algebraic Number Theory\" by Alaca and Williams.\n",
      "\n",
      "### Response:\n",
      " Thus, the final answer is \\\\boxed{1}.<|endoftext|>\n",
      "\n",
      "Final Accuracy: 6.00%\n"
     ]
    }
   ],
   "source": [
    "#df_sample = df.sample(n=50, random_state=43)\n",
    "benchmark(df_sample, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the finetuning did not lead to an improvement. This could be caused by the model being unsuited for the task, since the problems are simply too hard for such a small model. Another reason could be that the mined dataset is of quite low quality, since as reasoning we automatically choose the highest upvoted comment without any human moderation that it contains a sensible and correct proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Mia M√º√üig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
